{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_stock_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM+HJagn9uz07/1ANDpynC7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cansibi/Deep-learning/blob/master/RNN_stock_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9abbGi-lWpbq"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udCMlMIqWtlp",
        "outputId": "91e7d1e9-3509-49bd-cdb5-a78f640826c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "df = pd.read_csv('/content/Google_Stock_Price_Train.csv', index_col=0)\n",
        "df.index = list(map(lambda x:datetime.datetime.strptime(x, '%m/%d/%Y'), df.index))\n",
        "for i in range(len(df['Volume'])):\n",
        "  df['Volume'][i]=df['Volume'][i].replace(',','')\n",
        "df['Volume']=pd.to_numeric(df['Volume'])\n",
        "for i in range(len(df['Close'])):\n",
        "  df['Close'][i]=df['Close'][i].replace(',','')\n",
        "df['Close']=pd.to_numeric(df['Close'])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2012-01-03</th>\n",
              "      <td>325.25</td>\n",
              "      <td>332.83</td>\n",
              "      <td>324.97</td>\n",
              "      <td>663.59</td>\n",
              "      <td>7380500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-01-04</th>\n",
              "      <td>331.27</td>\n",
              "      <td>333.87</td>\n",
              "      <td>329.08</td>\n",
              "      <td>666.45</td>\n",
              "      <td>5749400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-01-05</th>\n",
              "      <td>329.83</td>\n",
              "      <td>330.75</td>\n",
              "      <td>326.89</td>\n",
              "      <td>657.21</td>\n",
              "      <td>6590300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-01-06</th>\n",
              "      <td>328.34</td>\n",
              "      <td>328.77</td>\n",
              "      <td>323.68</td>\n",
              "      <td>648.24</td>\n",
              "      <td>5405900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-01-09</th>\n",
              "      <td>322.04</td>\n",
              "      <td>322.29</td>\n",
              "      <td>309.46</td>\n",
              "      <td>620.76</td>\n",
              "      <td>11688800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Open    High     Low   Close    Volume\n",
              "2012-01-03  325.25  332.83  324.97  663.59   7380500\n",
              "2012-01-04  331.27  333.87  329.08  666.45   5749400\n",
              "2012-01-05  329.83  330.75  326.89  657.21   6590300\n",
              "2012-01-06  328.34  328.77  323.68  648.24   5405900\n",
              "2012-01-09  322.04  322.29  309.46  620.76  11688800"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw1TTwd9O2lv",
        "outputId": "f1603795-330c-4c22-b0e1-cb96b2ed0c7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "testset= pd.read_csv('/content/Google_Stock_Price_Test.csv', index_col=0)\n",
        "testset.index = list(map(lambda x:datetime.datetime.strptime(x, '%m/%d/%Y'), testset.index))\n",
        "for i in range(len(testset['Volume'])):\n",
        "  testset['Volume'][i]=testset['Volume'][i].replace(',','')\n",
        "testset['Volume']=pd.to_numeric(testset['Volume'])\n",
        "testset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-01-03</th>\n",
              "      <td>778.81</td>\n",
              "      <td>789.63</td>\n",
              "      <td>775.80</td>\n",
              "      <td>786.14</td>\n",
              "      <td>1657300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-04</th>\n",
              "      <td>788.36</td>\n",
              "      <td>791.34</td>\n",
              "      <td>783.16</td>\n",
              "      <td>786.90</td>\n",
              "      <td>1073000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-05</th>\n",
              "      <td>786.08</td>\n",
              "      <td>794.48</td>\n",
              "      <td>785.02</td>\n",
              "      <td>794.02</td>\n",
              "      <td>1335200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-06</th>\n",
              "      <td>795.26</td>\n",
              "      <td>807.90</td>\n",
              "      <td>792.20</td>\n",
              "      <td>806.15</td>\n",
              "      <td>1640200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-09</th>\n",
              "      <td>806.40</td>\n",
              "      <td>809.97</td>\n",
              "      <td>802.83</td>\n",
              "      <td>806.65</td>\n",
              "      <td>1272400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Open    High     Low   Close   Volume\n",
              "2017-01-03  778.81  789.63  775.80  786.14  1657300\n",
              "2017-01-04  788.36  791.34  783.16  786.90  1073000\n",
              "2017-01-05  786.08  794.48  785.02  794.02  1335200\n",
              "2017-01-06  795.26  807.90  792.20  806.15  1640200\n",
              "2017-01-09  806.40  809.97  802.83  806.65  1272400"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Es4lIPPPrYAv"
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTM, self).__init__()\n",
        "        \n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=5,   \n",
        "            hidden_size=128,\n",
        "            num_layers=1, \n",
        "            batch_first=True)\n",
        "        \n",
        "        self.out = nn.Sequential(\n",
        "            nn.Linear(128, 5))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        r_out, (h_n, h_c) = self.lstm(x, None)   \n",
        "        out = self.out(r_out)          \n",
        "        \n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG1C9G7srp5M"
      },
      "source": [
        "LR = 0.0001\n",
        "EPOCH = 2000\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAWpmiWsrW_c"
      },
      "source": [
        "\n",
        "def normalize(df):\n",
        "  df1=df.copy()\n",
        "  for i in df1.columns:\n",
        "    df1[i]=(df1[i]-(np.mean(np.array(df1[i]))))/np.std(np.array(df1[i]))\n",
        "  return df1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGDoEE5esSqf",
        "outputId": "7d85aa98-f1da-4c39-9f92-633ea0de5ecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df1=normalize(df)\n",
        "df1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2012-01-03</th>\n",
              "      <td>-1.372855</td>\n",
              "      <td>-1.340653</td>\n",
              "      <td>-1.355794</td>\n",
              "      <td>-0.298018</td>\n",
              "      <td>1.857776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-01-04</th>\n",
              "      <td>-1.333209</td>\n",
              "      <td>-1.333854</td>\n",
              "      <td>-1.328483</td>\n",
              "      <td>-0.280651</td>\n",
              "      <td>1.140122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-01-05</th>\n",
              "      <td>-1.342692</td>\n",
              "      <td>-1.354253</td>\n",
              "      <td>-1.343036</td>\n",
              "      <td>-0.336758</td>\n",
              "      <td>1.510102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-01-06</th>\n",
              "      <td>-1.352505</td>\n",
              "      <td>-1.367198</td>\n",
              "      <td>-1.364366</td>\n",
              "      <td>-0.391225</td>\n",
              "      <td>0.988988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2012-01-09</th>\n",
              "      <td>-1.393995</td>\n",
              "      <td>-1.409566</td>\n",
              "      <td>-1.458855</td>\n",
              "      <td>-0.558087</td>\n",
              "      <td>3.753349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-23</th>\n",
              "      <td>1.693778</td>\n",
              "      <td>1.666317</td>\n",
              "      <td>1.716177</td>\n",
              "      <td>0.469013</td>\n",
              "      <td>-1.115225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-27</th>\n",
              "      <td>1.692329</td>\n",
              "      <td>1.699792</td>\n",
              "      <td>1.718702</td>\n",
              "      <td>0.478971</td>\n",
              "      <td>-1.042320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-28</th>\n",
              "      <td>1.712218</td>\n",
              "      <td>1.676059</td>\n",
              "      <td>1.689066</td>\n",
              "      <td>0.439502</td>\n",
              "      <td>-0.881858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-29</th>\n",
              "      <td>1.643925</td>\n",
              "      <td>1.621792</td>\n",
              "      <td>1.660626</td>\n",
              "      <td>0.425779</td>\n",
              "      <td>-1.062031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-30</th>\n",
              "      <td>1.640105</td>\n",
              "      <td>1.601197</td>\n",
              "      <td>1.604079</td>\n",
              "      <td>0.359168</td>\n",
              "      <td>-0.610742</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1258 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                Open      High       Low     Close    Volume\n",
              "2012-01-03 -1.372855 -1.340653 -1.355794 -0.298018  1.857776\n",
              "2012-01-04 -1.333209 -1.333854 -1.328483 -0.280651  1.140122\n",
              "2012-01-05 -1.342692 -1.354253 -1.343036 -0.336758  1.510102\n",
              "2012-01-06 -1.352505 -1.367198 -1.364366 -0.391225  0.988988\n",
              "2012-01-09 -1.393995 -1.409566 -1.458855 -0.558087  3.753349\n",
              "...              ...       ...       ...       ...       ...\n",
              "2016-12-23  1.693778  1.666317  1.716177  0.469013 -1.115225\n",
              "2016-12-27  1.692329  1.699792  1.718702  0.478971 -1.042320\n",
              "2016-12-28  1.712218  1.676059  1.689066  0.439502 -0.881858\n",
              "2016-12-29  1.643925  1.621792  1.660626  0.425779 -1.062031\n",
              "2016-12-30  1.640105  1.601197  1.604079  0.359168 -0.610742\n",
              "\n",
              "[1258 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiA0HNteuogS"
      },
      "source": [
        "new_testset=normalize(testset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNsUf-thvRra",
        "outputId": "ad06b9af-4e23-4ba2-a1b8-7dd07b0ff0e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "new_testset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-01-03</th>\n",
              "      <td>-1.947845</td>\n",
              "      <td>-1.590669</td>\n",
              "      <td>-2.020454</td>\n",
              "      <td>-1.690368</td>\n",
              "      <td>-0.003930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-04</th>\n",
              "      <td>-1.300056</td>\n",
              "      <td>-1.468675</td>\n",
              "      <td>-1.451780</td>\n",
              "      <td>-1.631341</td>\n",
              "      <td>-0.888803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-05</th>\n",
              "      <td>-1.454711</td>\n",
              "      <td>-1.244662</td>\n",
              "      <td>-1.308066</td>\n",
              "      <td>-1.078357</td>\n",
              "      <td>-0.491723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-06</th>\n",
              "      <td>-0.832019</td>\n",
              "      <td>-0.287257</td>\n",
              "      <td>-0.753300</td>\n",
              "      <td>-0.136265</td>\n",
              "      <td>-0.029826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-09</th>\n",
              "      <td>-0.076378</td>\n",
              "      <td>-0.139580</td>\n",
              "      <td>0.068032</td>\n",
              "      <td>-0.097432</td>\n",
              "      <td>-0.586828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-10</th>\n",
              "      <td>0.022656</td>\n",
              "      <td>-0.199507</td>\n",
              "      <td>0.120573</td>\n",
              "      <td>-0.241892</td>\n",
              "      <td>-0.731607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-11</th>\n",
              "      <td>-0.171342</td>\n",
              "      <td>-0.269422</td>\n",
              "      <td>-0.044775</td>\n",
              "      <td>0.000427</td>\n",
              "      <td>-0.899555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-12</th>\n",
              "      <td>-0.026183</td>\n",
              "      <td>-0.323641</td>\n",
              "      <td>-0.214759</td>\n",
              "      <td>-0.119956</td>\n",
              "      <td>-0.464615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-13</th>\n",
              "      <td>-0.003120</td>\n",
              "      <td>-0.050403</td>\n",
              "      <td>0.366277</td>\n",
              "      <td>-0.001903</td>\n",
              "      <td>-0.849125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-17</th>\n",
              "      <td>-0.030253</td>\n",
              "      <td>-0.341477</td>\n",
              "      <td>-0.122041</td>\n",
              "      <td>-0.255872</td>\n",
              "      <td>-0.450985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-18</th>\n",
              "      <td>-0.116399</td>\n",
              "      <td>-0.407825</td>\n",
              "      <td>-0.074136</td>\n",
              "      <td>-0.142479</td>\n",
              "      <td>-0.553511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-19</th>\n",
              "      <td>-0.163202</td>\n",
              "      <td>-0.174537</td>\n",
              "      <td>-0.011551</td>\n",
              "      <td>-0.445377</td>\n",
              "      <td>-1.121569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-20</th>\n",
              "      <td>-0.041784</td>\n",
              "      <td>-0.357885</td>\n",
              "      <td>-0.020050</td>\n",
              "      <td>-0.224028</td>\n",
              "      <td>0.015303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-23</th>\n",
              "      <td>-0.018721</td>\n",
              "      <td>0.638044</td>\n",
              "      <td>0.138344</td>\n",
              "      <td>0.885823</td>\n",
              "      <td>0.459936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-24</th>\n",
              "      <td>1.002140</td>\n",
              "      <td>0.996893</td>\n",
              "      <td>1.226242</td>\n",
              "      <td>1.239981</td>\n",
              "      <td>-0.281522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-25</th>\n",
              "      <td>1.498666</td>\n",
              "      <td>1.701035</td>\n",
              "      <td>1.785645</td>\n",
              "      <td>2.156443</td>\n",
              "      <td>-0.250477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-26</th>\n",
              "      <td>2.054205</td>\n",
              "      <td>1.860126</td>\n",
              "      <td>1.936312</td>\n",
              "      <td>1.883058</td>\n",
              "      <td>1.989950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-27</th>\n",
              "      <td>1.843928</td>\n",
              "      <td>2.141926</td>\n",
              "      <td>1.428678</td>\n",
              "      <td>1.196488</td>\n",
              "      <td>1.977683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-30</th>\n",
              "      <td>0.483909</td>\n",
              "      <td>0.279196</td>\n",
              "      <td>-0.166082</td>\n",
              "      <td>-0.433727</td>\n",
              "      <td>2.402931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-31</th>\n",
              "      <td>-0.723489</td>\n",
              "      <td>-0.761679</td>\n",
              "      <td>-0.883106</td>\n",
              "      <td>-0.863222</td>\n",
              "      <td>0.758275</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Open      High       Low     Close    Volume\n",
              "2017-01-03 -1.947845 -1.590669 -2.020454 -1.690368 -0.003930\n",
              "2017-01-04 -1.300056 -1.468675 -1.451780 -1.631341 -0.888803\n",
              "2017-01-05 -1.454711 -1.244662 -1.308066 -1.078357 -0.491723\n",
              "2017-01-06 -0.832019 -0.287257 -0.753300 -0.136265 -0.029826\n",
              "2017-01-09 -0.076378 -0.139580  0.068032 -0.097432 -0.586828\n",
              "2017-01-10  0.022656 -0.199507  0.120573 -0.241892 -0.731607\n",
              "2017-01-11 -0.171342 -0.269422 -0.044775  0.000427 -0.899555\n",
              "2017-01-12 -0.026183 -0.323641 -0.214759 -0.119956 -0.464615\n",
              "2017-01-13 -0.003120 -0.050403  0.366277 -0.001903 -0.849125\n",
              "2017-01-17 -0.030253 -0.341477 -0.122041 -0.255872 -0.450985\n",
              "2017-01-18 -0.116399 -0.407825 -0.074136 -0.142479 -0.553511\n",
              "2017-01-19 -0.163202 -0.174537 -0.011551 -0.445377 -1.121569\n",
              "2017-01-20 -0.041784 -0.357885 -0.020050 -0.224028  0.015303\n",
              "2017-01-23 -0.018721  0.638044  0.138344  0.885823  0.459936\n",
              "2017-01-24  1.002140  0.996893  1.226242  1.239981 -0.281522\n",
              "2017-01-25  1.498666  1.701035  1.785645  2.156443 -0.250477\n",
              "2017-01-26  2.054205  1.860126  1.936312  1.883058  1.989950\n",
              "2017-01-27  1.843928  2.141926  1.428678  1.196488  1.977683\n",
              "2017-01-30  0.483909  0.279196 -0.166082 -0.433727  2.402931\n",
              "2017-01-31 -0.723489 -0.761679 -0.883106 -0.863222  0.758275"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjjZX_eYuTtD"
      },
      "source": [
        "train_data_numpy=np.array(df1)\n",
        "train_data_tensor=torch.Tensor(train_data_numpy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qii4WJkFvBa2"
      },
      "source": [
        "test_data_numpy = np.array(new_testset)\n",
        "test_data_tensor = torch.Tensor(test_data_numpy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kcdwSL3x242"
      },
      "source": [
        "X_train=[]\n",
        "Y_train=[]\n",
        "timesteps = 7\n",
        "for i in range(timesteps, 1258):\n",
        "    X_train.append(train_data_numpy[i-timesteps:i])\n",
        "    Y_train.append(train_data_numpy[i-timesteps:i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0hz7UYFzq7_"
      },
      "source": [
        "X_train=np.array(X_train)\n",
        "Y_train=np.array(Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nyr0syI4-VPO"
      },
      "source": [
        "X_val=torch.Tensor(X_train[900:])\n",
        "Y_val=torch.Tensor(Y_train[900:])\n",
        "X_train=torch.Tensor(X_train[:900])\n",
        "Y_train=torch.Tensor(Y_train[:900])\n",
        "X_train=X_train.cuda()\n",
        "Y_train=Y_train.cuda()\n",
        "X_val=X_val.cuda()\n",
        "Y_val=Y_val.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqY2gFXvDXOw"
      },
      "source": [
        "Y_train=Y_train.reshape(Y_train.shape[0],-1,Y_train.shape[1])\n",
        "Y_val=Y_val.reshape(Y_val.shape[0],-1,Y_val.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5yLUo1Frk5h"
      },
      "source": [
        "class TrainSet(Dataset):\n",
        "    def __init__(self, Xtrain,Ytrain):\n",
        "        self.data, self.label = Xtrain.float(), Ytrain.float()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index], self.label[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8LjylsWMKDB"
      },
      "source": [
        "\n",
        "train_set = TrainSet(X_train,Y_train)\n",
        "val_set=TrainSet(X_val,Y_val)\n",
        "train_loader = DataLoader(train_set, batch_size=256, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=256, shuffle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-0TK7UuRuNx",
        "outputId": "bec98b5f-b46a-46c6-bcac-d76c33cc9874",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "Loss=[]\n",
        "Step=[]\n",
        "import os\n",
        "rnn=LSTM()\n",
        "if torch.cuda.is_available():\n",
        "  rnn = rnn.cuda()\n",
        "\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)  # optimize all cnn parameters\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "best_loss = 1000\n",
        "\n",
        "\n",
        "if not os.path.exists('weights'):\n",
        "  os.mkdir('weights')    \n",
        "\n",
        "for step in range(EPOCH):\n",
        "     for tx, ty in train_loader:\n",
        "         if torch.cuda.is_available():\n",
        "             tx = tx.cuda()\n",
        "             ty = ty.cuda() \n",
        "        \n",
        "         output = rnn(tx)          \n",
        "         loss = loss_func(torch.squeeze(output), ty)\n",
        "         Loss.append(loss.cpu().item())        \n",
        "         optimizer.zero_grad() \n",
        "         loss.backward()  \n",
        "         optimizer.step()\n",
        "         Step.append(step)\n",
        "        \n",
        "         print('epoch : %d  ' % step, 'train_loss : %.4f' % loss.cpu().item())\n",
        "\n",
        "        \n",
        "     with torch.no_grad():\n",
        "           for tx, ty in val_loader:\n",
        "             if torch.cuda.is_available():\n",
        "               tx = tx.cuda()\n",
        "               ty = ty.cuda()\n",
        "             output = rnn((tx))\n",
        "             loss = loss_func(torch.squeeze(output), ty)\n",
        "             print('epoch : %d  ' % step, 'val_loss : %.4f' % loss.cpu().item())\n",
        "             \n",
        "           if loss.cpu().item() < best_loss:\n",
        "            best_loss = loss.cpu().item()\n",
        "            torch.save(rnn, 'weights/rnn.pth'.format(loss.cpu().item()))\n",
        "            print('new model saved at epoch {} with val_loss {}'.format(step, best_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
            "epoch : 1174   train_loss : 0.0010\n",
            "epoch : 1174   train_loss : 0.0007\n",
            "epoch : 1174   val_loss : 0.0311\n",
            "epoch : 1174   val_loss : 0.0308\n",
            "epoch : 1175   train_loss : 0.0008\n",
            "epoch : 1175   train_loss : 0.0008\n",
            "epoch : 1175   train_loss : 0.0008\n",
            "epoch : 1175   train_loss : 0.0010\n",
            "epoch : 1175   val_loss : 0.0309\n",
            "epoch : 1175   val_loss : 0.0311\n",
            "epoch : 1176   train_loss : 0.0008\n",
            "epoch : 1176   train_loss : 0.0007\n",
            "epoch : 1176   train_loss : 0.0010\n",
            "epoch : 1176   train_loss : 0.0007\n",
            "epoch : 1176   val_loss : 0.0297\n",
            "epoch : 1176   val_loss : 0.0343\n",
            "epoch : 1177   train_loss : 0.0009\n",
            "epoch : 1177   train_loss : 0.0007\n",
            "epoch : 1177   train_loss : 0.0007\n",
            "epoch : 1177   train_loss : 0.0010\n",
            "epoch : 1177   val_loss : 0.0305\n",
            "epoch : 1177   val_loss : 0.0321\n",
            "epoch : 1178   train_loss : 0.0009\n",
            "epoch : 1178   train_loss : 0.0008\n",
            "epoch : 1178   train_loss : 0.0009\n",
            "epoch : 1178   train_loss : 0.0006\n",
            "epoch : 1178   val_loss : 0.0307\n",
            "epoch : 1178   val_loss : 0.0312\n",
            "epoch : 1179   train_loss : 0.0008\n",
            "epoch : 1179   train_loss : 0.0007\n",
            "epoch : 1179   train_loss : 0.0011\n",
            "epoch : 1179   train_loss : 0.0006\n",
            "epoch : 1179   val_loss : 0.0315\n",
            "epoch : 1179   val_loss : 0.0287\n",
            "epoch : 1180   train_loss : 0.0008\n",
            "epoch : 1180   train_loss : 0.0009\n",
            "epoch : 1180   train_loss : 0.0007\n",
            "epoch : 1180   train_loss : 0.0008\n",
            "epoch : 1180   val_loss : 0.0307\n",
            "epoch : 1180   val_loss : 0.0309\n",
            "epoch : 1181   train_loss : 0.0006\n",
            "epoch : 1181   train_loss : 0.0007\n",
            "epoch : 1181   train_loss : 0.0009\n",
            "epoch : 1181   train_loss : 0.0011\n",
            "epoch : 1181   val_loss : 0.0308\n",
            "epoch : 1181   val_loss : 0.0303\n",
            "epoch : 1182   train_loss : 0.0009\n",
            "epoch : 1182   train_loss : 0.0009\n",
            "epoch : 1182   train_loss : 0.0006\n",
            "epoch : 1182   train_loss : 0.0010\n",
            "epoch : 1182   val_loss : 0.0303\n",
            "epoch : 1182   val_loss : 0.0317\n",
            "epoch : 1183   train_loss : 0.0008\n",
            "epoch : 1183   train_loss : 0.0008\n",
            "epoch : 1183   train_loss : 0.0008\n",
            "epoch : 1183   train_loss : 0.0008\n",
            "epoch : 1183   val_loss : 0.0306\n",
            "epoch : 1183   val_loss : 0.0307\n",
            "epoch : 1184   train_loss : 0.0008\n",
            "epoch : 1184   train_loss : 0.0008\n",
            "epoch : 1184   train_loss : 0.0008\n",
            "epoch : 1184   train_loss : 0.0009\n",
            "epoch : 1184   val_loss : 0.0309\n",
            "epoch : 1184   val_loss : 0.0297\n",
            "epoch : 1185   train_loss : 0.0007\n",
            "epoch : 1185   train_loss : 0.0006\n",
            "epoch : 1185   train_loss : 0.0008\n",
            "epoch : 1185   train_loss : 0.0013\n",
            "epoch : 1185   val_loss : 0.0314\n",
            "epoch : 1185   val_loss : 0.0281\n",
            "epoch : 1186   train_loss : 0.0006\n",
            "epoch : 1186   train_loss : 0.0010\n",
            "epoch : 1186   train_loss : 0.0009\n",
            "epoch : 1186   train_loss : 0.0007\n",
            "epoch : 1186   val_loss : 0.0302\n",
            "epoch : 1186   val_loss : 0.0310\n",
            "epoch : 1187   train_loss : 0.0007\n",
            "epoch : 1187   train_loss : 0.0008\n",
            "epoch : 1187   train_loss : 0.0009\n",
            "epoch : 1187   train_loss : 0.0007\n",
            "epoch : 1187   val_loss : 0.0304\n",
            "epoch : 1187   val_loss : 0.0305\n",
            "epoch : 1188   train_loss : 0.0006\n",
            "epoch : 1188   train_loss : 0.0007\n",
            "epoch : 1188   train_loss : 0.0008\n",
            "epoch : 1188   train_loss : 0.0013\n",
            "epoch : 1188   val_loss : 0.0298\n",
            "epoch : 1188   val_loss : 0.0317\n",
            "epoch : 1189   train_loss : 0.0007\n",
            "epoch : 1189   train_loss : 0.0010\n",
            "epoch : 1189   train_loss : 0.0009\n",
            "epoch : 1189   train_loss : 0.0006\n",
            "epoch : 1189   val_loss : 0.0298\n",
            "epoch : 1189   val_loss : 0.0314\n",
            "epoch : 1190   train_loss : 0.0010\n",
            "epoch : 1190   train_loss : 0.0006\n",
            "epoch : 1190   train_loss : 0.0008\n",
            "epoch : 1190   train_loss : 0.0009\n",
            "epoch : 1190   val_loss : 0.0299\n",
            "epoch : 1190   val_loss : 0.0311\n",
            "epoch : 1191   train_loss : 0.0011\n",
            "epoch : 1191   train_loss : 0.0006\n",
            "epoch : 1191   train_loss : 0.0008\n",
            "epoch : 1191   train_loss : 0.0006\n",
            "epoch : 1191   val_loss : 0.0303\n",
            "epoch : 1191   val_loss : 0.0300\n",
            "epoch : 1192   train_loss : 0.0006\n",
            "epoch : 1192   train_loss : 0.0008\n",
            "epoch : 1192   train_loss : 0.0008\n",
            "epoch : 1192   train_loss : 0.0013\n",
            "epoch : 1192   val_loss : 0.0301\n",
            "epoch : 1192   val_loss : 0.0302\n",
            "epoch : 1193   train_loss : 0.0008\n",
            "epoch : 1193   train_loss : 0.0007\n",
            "epoch : 1193   train_loss : 0.0009\n",
            "epoch : 1193   train_loss : 0.0007\n",
            "epoch : 1193   val_loss : 0.0296\n",
            "epoch : 1193   val_loss : 0.0315\n",
            "epoch : 1194   train_loss : 0.0007\n",
            "epoch : 1194   train_loss : 0.0009\n",
            "epoch : 1194   train_loss : 0.0008\n",
            "epoch : 1194   train_loss : 0.0008\n",
            "epoch : 1194   val_loss : 0.0302\n",
            "epoch : 1194   val_loss : 0.0298\n",
            "epoch : 1195   train_loss : 0.0006\n",
            "epoch : 1195   train_loss : 0.0007\n",
            "epoch : 1195   train_loss : 0.0011\n",
            "epoch : 1195   train_loss : 0.0007\n",
            "epoch : 1195   val_loss : 0.0304\n",
            "epoch : 1195   val_loss : 0.0289\n",
            "epoch : 1196   train_loss : 0.0008\n",
            "epoch : 1196   train_loss : 0.0008\n",
            "epoch : 1196   train_loss : 0.0007\n",
            "epoch : 1196   train_loss : 0.0009\n",
            "epoch : 1196   val_loss : 0.0291\n",
            "epoch : 1196   val_loss : 0.0324\n",
            "epoch : 1197   train_loss : 0.0007\n",
            "epoch : 1197   train_loss : 0.0007\n",
            "epoch : 1197   train_loss : 0.0009\n",
            "epoch : 1197   train_loss : 0.0009\n",
            "epoch : 1197   val_loss : 0.0302\n",
            "epoch : 1197   val_loss : 0.0291\n",
            "epoch : 1198   train_loss : 0.0007\n",
            "epoch : 1198   train_loss : 0.0008\n",
            "epoch : 1198   train_loss : 0.0008\n",
            "epoch : 1198   train_loss : 0.0010\n",
            "epoch : 1198   val_loss : 0.0311\n",
            "epoch : 1198   val_loss : 0.0266\n",
            "new model saved at epoch 1198 with val_loss 0.02658749930560589\n",
            "epoch : 1199   train_loss : 0.0005\n",
            "epoch : 1199   train_loss : 0.0011\n",
            "epoch : 1199   train_loss : 0.0009\n",
            "epoch : 1199   train_loss : 0.0006\n",
            "epoch : 1199   val_loss : 0.0299\n",
            "epoch : 1199   val_loss : 0.0294\n",
            "epoch : 1200   train_loss : 0.0010\n",
            "epoch : 1200   train_loss : 0.0009\n",
            "epoch : 1200   train_loss : 0.0006\n",
            "epoch : 1200   train_loss : 0.0005\n",
            "epoch : 1200   val_loss : 0.0304\n",
            "epoch : 1200   val_loss : 0.0281\n",
            "epoch : 1201   train_loss : 0.0007\n",
            "epoch : 1201   train_loss : 0.0009\n",
            "epoch : 1201   train_loss : 0.0008\n",
            "epoch : 1201   train_loss : 0.0007\n",
            "epoch : 1201   val_loss : 0.0305\n",
            "epoch : 1201   val_loss : 0.0275\n",
            "epoch : 1202   train_loss : 0.0008\n",
            "epoch : 1202   train_loss : 0.0009\n",
            "epoch : 1202   train_loss : 0.0008\n",
            "epoch : 1202   train_loss : 0.0007\n",
            "epoch : 1202   val_loss : 0.0297\n",
            "epoch : 1202   val_loss : 0.0296\n",
            "epoch : 1203   train_loss : 0.0010\n",
            "epoch : 1203   train_loss : 0.0008\n",
            "epoch : 1203   train_loss : 0.0006\n",
            "epoch : 1203   train_loss : 0.0008\n",
            "epoch : 1203   val_loss : 0.0298\n",
            "epoch : 1203   val_loss : 0.0291\n",
            "epoch : 1204   train_loss : 0.0008\n",
            "epoch : 1204   train_loss : 0.0007\n",
            "epoch : 1204   train_loss : 0.0009\n",
            "epoch : 1204   train_loss : 0.0008\n",
            "epoch : 1204   val_loss : 0.0301\n",
            "epoch : 1204   val_loss : 0.0282\n",
            "epoch : 1205   train_loss : 0.0007\n",
            "epoch : 1205   train_loss : 0.0007\n",
            "epoch : 1205   train_loss : 0.0009\n",
            "epoch : 1205   train_loss : 0.0009\n",
            "epoch : 1205   val_loss : 0.0295\n",
            "epoch : 1205   val_loss : 0.0296\n",
            "epoch : 1206   train_loss : 0.0010\n",
            "epoch : 1206   train_loss : 0.0006\n",
            "epoch : 1206   train_loss : 0.0009\n",
            "epoch : 1206   train_loss : 0.0006\n",
            "epoch : 1206   val_loss : 0.0305\n",
            "epoch : 1206   val_loss : 0.0266\n",
            "epoch : 1207   train_loss : 0.0008\n",
            "epoch : 1207   train_loss : 0.0007\n",
            "epoch : 1207   train_loss : 0.0008\n",
            "epoch : 1207   train_loss : 0.0008\n",
            "epoch : 1207   val_loss : 0.0294\n",
            "epoch : 1207   val_loss : 0.0294\n",
            "epoch : 1208   train_loss : 0.0007\n",
            "epoch : 1208   train_loss : 0.0008\n",
            "epoch : 1208   train_loss : 0.0008\n",
            "epoch : 1208   train_loss : 0.0008\n",
            "epoch : 1208   val_loss : 0.0298\n",
            "epoch : 1208   val_loss : 0.0283\n",
            "epoch : 1209   train_loss : 0.0010\n",
            "epoch : 1209   train_loss : 0.0008\n",
            "epoch : 1209   train_loss : 0.0005\n",
            "epoch : 1209   train_loss : 0.0008\n",
            "epoch : 1209   val_loss : 0.0296\n",
            "epoch : 1209   val_loss : 0.0286\n",
            "epoch : 1210   train_loss : 0.0007\n",
            "epoch : 1210   train_loss : 0.0009\n",
            "epoch : 1210   train_loss : 0.0008\n",
            "epoch : 1210   train_loss : 0.0007\n",
            "epoch : 1210   val_loss : 0.0295\n",
            "epoch : 1210   val_loss : 0.0285\n",
            "epoch : 1211   train_loss : 0.0007\n",
            "epoch : 1211   train_loss : 0.0009\n",
            "epoch : 1211   train_loss : 0.0008\n",
            "epoch : 1211   train_loss : 0.0007\n",
            "epoch : 1211   val_loss : 0.0287\n",
            "epoch : 1211   val_loss : 0.0305\n",
            "epoch : 1212   train_loss : 0.0008\n",
            "epoch : 1212   train_loss : 0.0007\n",
            "epoch : 1212   train_loss : 0.0008\n",
            "epoch : 1212   train_loss : 0.0007\n",
            "epoch : 1212   val_loss : 0.0282\n",
            "epoch : 1212   val_loss : 0.0318\n",
            "epoch : 1213   train_loss : 0.0009\n",
            "epoch : 1213   train_loss : 0.0008\n",
            "epoch : 1213   train_loss : 0.0006\n",
            "epoch : 1213   train_loss : 0.0007\n",
            "epoch : 1213   val_loss : 0.0297\n",
            "epoch : 1213   val_loss : 0.0273\n",
            "epoch : 1214   train_loss : 0.0010\n",
            "epoch : 1214   train_loss : 0.0006\n",
            "epoch : 1214   train_loss : 0.0007\n",
            "epoch : 1214   train_loss : 0.0006\n",
            "epoch : 1214   val_loss : 0.0293\n",
            "epoch : 1214   val_loss : 0.0283\n",
            "epoch : 1215   train_loss : 0.0006\n",
            "epoch : 1215   train_loss : 0.0008\n",
            "epoch : 1215   train_loss : 0.0009\n",
            "epoch : 1215   train_loss : 0.0007\n",
            "epoch : 1215   val_loss : 0.0302\n",
            "epoch : 1215   val_loss : 0.0258\n",
            "new model saved at epoch 1215 with val_loss 0.025795187801122665\n",
            "epoch : 1216   train_loss : 0.0008\n",
            "epoch : 1216   train_loss : 0.0007\n",
            "epoch : 1216   train_loss : 0.0007\n",
            "epoch : 1216   train_loss : 0.0010\n",
            "epoch : 1216   val_loss : 0.0297\n",
            "epoch : 1216   val_loss : 0.0270\n",
            "epoch : 1217   train_loss : 0.0007\n",
            "epoch : 1217   train_loss : 0.0008\n",
            "epoch : 1217   train_loss : 0.0008\n",
            "epoch : 1217   train_loss : 0.0008\n",
            "epoch : 1217   val_loss : 0.0291\n",
            "epoch : 1217   val_loss : 0.0282\n",
            "epoch : 1218   train_loss : 0.0009\n",
            "epoch : 1218   train_loss : 0.0008\n",
            "epoch : 1218   train_loss : 0.0006\n",
            "epoch : 1218   train_loss : 0.0007\n",
            "epoch : 1218   val_loss : 0.0296\n",
            "epoch : 1218   val_loss : 0.0270\n",
            "epoch : 1219   train_loss : 0.0009\n",
            "epoch : 1219   train_loss : 0.0007\n",
            "epoch : 1219   train_loss : 0.0008\n",
            "epoch : 1219   train_loss : 0.0005\n",
            "epoch : 1219   val_loss : 0.0293\n",
            "epoch : 1219   val_loss : 0.0276\n",
            "epoch : 1220   train_loss : 0.0006\n",
            "epoch : 1220   train_loss : 0.0009\n",
            "epoch : 1220   train_loss : 0.0007\n",
            "epoch : 1220   train_loss : 0.0009\n",
            "epoch : 1220   val_loss : 0.0292\n",
            "epoch : 1220   val_loss : 0.0276\n",
            "epoch : 1221   train_loss : 0.0009\n",
            "epoch : 1221   train_loss : 0.0007\n",
            "epoch : 1221   train_loss : 0.0007\n",
            "epoch : 1221   train_loss : 0.0009\n",
            "epoch : 1221   val_loss : 0.0294\n",
            "epoch : 1221   val_loss : 0.0269\n",
            "epoch : 1222   train_loss : 0.0008\n",
            "epoch : 1222   train_loss : 0.0006\n",
            "epoch : 1222   train_loss : 0.0010\n",
            "epoch : 1222   train_loss : 0.0007\n",
            "epoch : 1222   val_loss : 0.0286\n",
            "epoch : 1222   val_loss : 0.0290\n",
            "epoch : 1223   train_loss : 0.0007\n",
            "epoch : 1223   train_loss : 0.0007\n",
            "epoch : 1223   train_loss : 0.0008\n",
            "epoch : 1223   train_loss : 0.0010\n",
            "epoch : 1223   val_loss : 0.0287\n",
            "epoch : 1223   val_loss : 0.0284\n",
            "epoch : 1224   train_loss : 0.0009\n",
            "epoch : 1224   train_loss : 0.0006\n",
            "epoch : 1224   train_loss : 0.0009\n",
            "epoch : 1224   train_loss : 0.0006\n",
            "epoch : 1224   val_loss : 0.0279\n",
            "epoch : 1224   val_loss : 0.0302\n",
            "epoch : 1225   train_loss : 0.0007\n",
            "epoch : 1225   train_loss : 0.0008\n",
            "epoch : 1225   train_loss : 0.0008\n",
            "epoch : 1225   train_loss : 0.0006\n",
            "epoch : 1225   val_loss : 0.0282\n",
            "epoch : 1225   val_loss : 0.0293\n",
            "epoch : 1226   train_loss : 0.0009\n",
            "epoch : 1226   train_loss : 0.0009\n",
            "epoch : 1226   train_loss : 0.0006\n",
            "epoch : 1226   train_loss : 0.0007\n",
            "epoch : 1226   val_loss : 0.0294\n",
            "epoch : 1226   val_loss : 0.0258\n",
            "new model saved at epoch 1226 with val_loss 0.025769462808966637\n",
            "epoch : 1227   train_loss : 0.0009\n",
            "epoch : 1227   train_loss : 0.0007\n",
            "epoch : 1227   train_loss : 0.0008\n",
            "epoch : 1227   train_loss : 0.0005\n",
            "epoch : 1227   val_loss : 0.0288\n",
            "epoch : 1227   val_loss : 0.0272\n",
            "epoch : 1228   train_loss : 0.0008\n",
            "epoch : 1228   train_loss : 0.0007\n",
            "epoch : 1228   train_loss : 0.0008\n",
            "epoch : 1228   train_loss : 0.0007\n",
            "epoch : 1228   val_loss : 0.0287\n",
            "epoch : 1228   val_loss : 0.0274\n",
            "epoch : 1229   train_loss : 0.0009\n",
            "epoch : 1229   train_loss : 0.0007\n",
            "epoch : 1229   train_loss : 0.0008\n",
            "epoch : 1229   train_loss : 0.0006\n",
            "epoch : 1229   val_loss : 0.0278\n",
            "epoch : 1229   val_loss : 0.0297\n",
            "epoch : 1230   train_loss : 0.0005\n",
            "epoch : 1230   train_loss : 0.0007\n",
            "epoch : 1230   train_loss : 0.0010\n",
            "epoch : 1230   train_loss : 0.0009\n",
            "epoch : 1230   val_loss : 0.0282\n",
            "epoch : 1230   val_loss : 0.0285\n",
            "epoch : 1231   train_loss : 0.0008\n",
            "epoch : 1231   train_loss : 0.0008\n",
            "epoch : 1231   train_loss : 0.0007\n",
            "epoch : 1231   train_loss : 0.0007\n",
            "epoch : 1231   val_loss : 0.0290\n",
            "epoch : 1231   val_loss : 0.0261\n",
            "epoch : 1232   train_loss : 0.0007\n",
            "epoch : 1232   train_loss : 0.0007\n",
            "epoch : 1232   train_loss : 0.0008\n",
            "epoch : 1232   train_loss : 0.0010\n",
            "epoch : 1232   val_loss : 0.0284\n",
            "epoch : 1232   val_loss : 0.0275\n",
            "epoch : 1233   train_loss : 0.0008\n",
            "epoch : 1233   train_loss : 0.0010\n",
            "epoch : 1233   train_loss : 0.0006\n",
            "epoch : 1233   train_loss : 0.0006\n",
            "epoch : 1233   val_loss : 0.0279\n",
            "epoch : 1233   val_loss : 0.0286\n",
            "epoch : 1234   train_loss : 0.0008\n",
            "epoch : 1234   train_loss : 0.0007\n",
            "epoch : 1234   train_loss : 0.0006\n",
            "epoch : 1234   train_loss : 0.0009\n",
            "epoch : 1234   val_loss : 0.0279\n",
            "epoch : 1234   val_loss : 0.0286\n",
            "epoch : 1235   train_loss : 0.0008\n",
            "epoch : 1235   train_loss : 0.0008\n",
            "epoch : 1235   train_loss : 0.0007\n",
            "epoch : 1235   train_loss : 0.0008\n",
            "epoch : 1235   val_loss : 0.0282\n",
            "epoch : 1235   val_loss : 0.0275\n",
            "epoch : 1236   train_loss : 0.0008\n",
            "epoch : 1236   train_loss : 0.0008\n",
            "epoch : 1236   train_loss : 0.0006\n",
            "epoch : 1236   train_loss : 0.0008\n",
            "epoch : 1236   val_loss : 0.0288\n",
            "epoch : 1236   val_loss : 0.0258\n",
            "epoch : 1237   train_loss : 0.0007\n",
            "epoch : 1237   train_loss : 0.0008\n",
            "epoch : 1237   train_loss : 0.0009\n",
            "epoch : 1237   train_loss : 0.0006\n",
            "epoch : 1237   val_loss : 0.0280\n",
            "epoch : 1237   val_loss : 0.0277\n",
            "epoch : 1238   train_loss : 0.0008\n",
            "epoch : 1238   train_loss : 0.0008\n",
            "epoch : 1238   train_loss : 0.0008\n",
            "epoch : 1238   train_loss : 0.0005\n",
            "epoch : 1238   val_loss : 0.0277\n",
            "epoch : 1238   val_loss : 0.0284\n",
            "epoch : 1239   train_loss : 0.0008\n",
            "epoch : 1239   train_loss : 0.0008\n",
            "epoch : 1239   train_loss : 0.0006\n",
            "epoch : 1239   train_loss : 0.0006\n",
            "epoch : 1239   val_loss : 0.0264\n",
            "epoch : 1239   val_loss : 0.0316\n",
            "epoch : 1240   train_loss : 0.0007\n",
            "epoch : 1240   train_loss : 0.0007\n",
            "epoch : 1240   train_loss : 0.0008\n",
            "epoch : 1240   train_loss : 0.0009\n",
            "epoch : 1240   val_loss : 0.0258\n",
            "epoch : 1240   val_loss : 0.0331\n",
            "epoch : 1241   train_loss : 0.0007\n",
            "epoch : 1241   train_loss : 0.0008\n",
            "epoch : 1241   train_loss : 0.0007\n",
            "epoch : 1241   train_loss : 0.0009\n",
            "epoch : 1241   val_loss : 0.0280\n",
            "epoch : 1241   val_loss : 0.0270\n",
            "epoch : 1242   train_loss : 0.0006\n",
            "epoch : 1242   train_loss : 0.0007\n",
            "epoch : 1242   train_loss : 0.0009\n",
            "epoch : 1242   train_loss : 0.0007\n",
            "epoch : 1242   val_loss : 0.0288\n",
            "epoch : 1242   val_loss : 0.0248\n",
            "new model saved at epoch 1242 with val_loss 0.024765556678175926\n",
            "epoch : 1243   train_loss : 0.0008\n",
            "epoch : 1243   train_loss : 0.0005\n",
            "epoch : 1243   train_loss : 0.0009\n",
            "epoch : 1243   train_loss : 0.0007\n",
            "epoch : 1243   val_loss : 0.0273\n",
            "epoch : 1243   val_loss : 0.0285\n",
            "epoch : 1244   train_loss : 0.0007\n",
            "epoch : 1244   train_loss : 0.0006\n",
            "epoch : 1244   train_loss : 0.0009\n",
            "epoch : 1244   train_loss : 0.0007\n",
            "epoch : 1244   val_loss : 0.0272\n",
            "epoch : 1244   val_loss : 0.0287\n",
            "epoch : 1245   train_loss : 0.0007\n",
            "epoch : 1245   train_loss : 0.0005\n",
            "epoch : 1245   train_loss : 0.0009\n",
            "epoch : 1245   train_loss : 0.0009\n",
            "epoch : 1245   val_loss : 0.0270\n",
            "epoch : 1245   val_loss : 0.0290\n",
            "epoch : 1246   train_loss : 0.0008\n",
            "epoch : 1246   train_loss : 0.0006\n",
            "epoch : 1246   train_loss : 0.0008\n",
            "epoch : 1246   train_loss : 0.0008\n",
            "epoch : 1246   val_loss : 0.0266\n",
            "epoch : 1246   val_loss : 0.0298\n",
            "epoch : 1247   train_loss : 0.0007\n",
            "epoch : 1247   train_loss : 0.0007\n",
            "epoch : 1247   train_loss : 0.0009\n",
            "epoch : 1247   train_loss : 0.0006\n",
            "epoch : 1247   val_loss : 0.0274\n",
            "epoch : 1247   val_loss : 0.0277\n",
            "epoch : 1248   train_loss : 0.0006\n",
            "epoch : 1248   train_loss : 0.0009\n",
            "epoch : 1248   train_loss : 0.0007\n",
            "epoch : 1248   train_loss : 0.0007\n",
            "epoch : 1248   val_loss : 0.0280\n",
            "epoch : 1248   val_loss : 0.0258\n",
            "epoch : 1249   train_loss : 0.0007\n",
            "epoch : 1249   train_loss : 0.0009\n",
            "epoch : 1249   train_loss : 0.0007\n",
            "epoch : 1249   train_loss : 0.0006\n",
            "epoch : 1249   val_loss : 0.0275\n",
            "epoch : 1249   val_loss : 0.0271\n",
            "epoch : 1250   train_loss : 0.0008\n",
            "epoch : 1250   train_loss : 0.0007\n",
            "epoch : 1250   train_loss : 0.0007\n",
            "epoch : 1250   train_loss : 0.0008\n",
            "epoch : 1250   val_loss : 0.0267\n",
            "epoch : 1250   val_loss : 0.0290\n",
            "epoch : 1251   train_loss : 0.0008\n",
            "epoch : 1251   train_loss : 0.0009\n",
            "epoch : 1251   train_loss : 0.0007\n",
            "epoch : 1251   train_loss : 0.0005\n",
            "epoch : 1251   val_loss : 0.0273\n",
            "epoch : 1251   val_loss : 0.0273\n",
            "epoch : 1252   train_loss : 0.0006\n",
            "epoch : 1252   train_loss : 0.0006\n",
            "epoch : 1252   train_loss : 0.0009\n",
            "epoch : 1252   train_loss : 0.0007\n",
            "epoch : 1252   val_loss : 0.0280\n",
            "epoch : 1252   val_loss : 0.0252\n",
            "epoch : 1253   train_loss : 0.0007\n",
            "epoch : 1253   train_loss : 0.0008\n",
            "epoch : 1253   train_loss : 0.0006\n",
            "epoch : 1253   train_loss : 0.0008\n",
            "epoch : 1253   val_loss : 0.0271\n",
            "epoch : 1253   val_loss : 0.0274\n",
            "epoch : 1254   train_loss : 0.0009\n",
            "epoch : 1254   train_loss : 0.0006\n",
            "epoch : 1254   train_loss : 0.0008\n",
            "epoch : 1254   train_loss : 0.0005\n",
            "epoch : 1254   val_loss : 0.0271\n",
            "epoch : 1254   val_loss : 0.0273\n",
            "epoch : 1255   train_loss : 0.0006\n",
            "epoch : 1255   train_loss : 0.0007\n",
            "epoch : 1255   train_loss : 0.0007\n",
            "epoch : 1255   train_loss : 0.0009\n",
            "epoch : 1255   val_loss : 0.0268\n",
            "epoch : 1255   val_loss : 0.0280\n",
            "epoch : 1256   train_loss : 0.0007\n",
            "epoch : 1256   train_loss : 0.0007\n",
            "epoch : 1256   train_loss : 0.0009\n",
            "epoch : 1256   train_loss : 0.0007\n",
            "epoch : 1256   val_loss : 0.0283\n",
            "epoch : 1256   val_loss : 0.0236\n",
            "new model saved at epoch 1256 with val_loss 0.02361452765762806\n",
            "epoch : 1257   train_loss : 0.0008\n",
            "epoch : 1257   train_loss : 0.0006\n",
            "epoch : 1257   train_loss : 0.0007\n",
            "epoch : 1257   train_loss : 0.0009\n",
            "epoch : 1257   val_loss : 0.0269\n",
            "epoch : 1257   val_loss : 0.0273\n",
            "epoch : 1258   train_loss : 0.0006\n",
            "epoch : 1258   train_loss : 0.0008\n",
            "epoch : 1258   train_loss : 0.0007\n",
            "epoch : 1258   train_loss : 0.0008\n",
            "epoch : 1258   val_loss : 0.0275\n",
            "epoch : 1258   val_loss : 0.0256\n",
            "epoch : 1259   train_loss : 0.0008\n",
            "epoch : 1259   train_loss : 0.0008\n",
            "epoch : 1259   train_loss : 0.0006\n",
            "epoch : 1259   train_loss : 0.0005\n",
            "epoch : 1259   val_loss : 0.0263\n",
            "epoch : 1259   val_loss : 0.0285\n",
            "epoch : 1260   train_loss : 0.0007\n",
            "epoch : 1260   train_loss : 0.0006\n",
            "epoch : 1260   train_loss : 0.0009\n",
            "epoch : 1260   train_loss : 0.0007\n",
            "epoch : 1260   val_loss : 0.0275\n",
            "epoch : 1260   val_loss : 0.0253\n",
            "epoch : 1261   train_loss : 0.0010\n",
            "epoch : 1261   train_loss : 0.0005\n",
            "epoch : 1261   train_loss : 0.0006\n",
            "epoch : 1261   train_loss : 0.0009\n",
            "epoch : 1261   val_loss : 0.0264\n",
            "epoch : 1261   val_loss : 0.0279\n",
            "epoch : 1262   train_loss : 0.0008\n",
            "epoch : 1262   train_loss : 0.0007\n",
            "epoch : 1262   train_loss : 0.0008\n",
            "epoch : 1262   train_loss : 0.0005\n",
            "epoch : 1262   val_loss : 0.0267\n",
            "epoch : 1262   val_loss : 0.0268\n",
            "epoch : 1263   train_loss : 0.0006\n",
            "epoch : 1263   train_loss : 0.0008\n",
            "epoch : 1263   train_loss : 0.0006\n",
            "epoch : 1263   train_loss : 0.0009\n",
            "epoch : 1263   val_loss : 0.0270\n",
            "epoch : 1263   val_loss : 0.0258\n",
            "epoch : 1264   train_loss : 0.0006\n",
            "epoch : 1264   train_loss : 0.0008\n",
            "epoch : 1264   train_loss : 0.0007\n",
            "epoch : 1264   train_loss : 0.0009\n",
            "epoch : 1264   val_loss : 0.0282\n",
            "epoch : 1264   val_loss : 0.0225\n",
            "new model saved at epoch 1264 with val_loss 0.022528208792209625\n",
            "epoch : 1265   train_loss : 0.0008\n",
            "epoch : 1265   train_loss : 0.0007\n",
            "epoch : 1265   train_loss : 0.0006\n",
            "epoch : 1265   train_loss : 0.0007\n",
            "epoch : 1265   val_loss : 0.0272\n",
            "epoch : 1265   val_loss : 0.0251\n",
            "epoch : 1266   train_loss : 0.0008\n",
            "epoch : 1266   train_loss : 0.0006\n",
            "epoch : 1266   train_loss : 0.0006\n",
            "epoch : 1266   train_loss : 0.0008\n",
            "epoch : 1266   val_loss : 0.0274\n",
            "epoch : 1266   val_loss : 0.0243\n",
            "epoch : 1267   train_loss : 0.0008\n",
            "epoch : 1267   train_loss : 0.0008\n",
            "epoch : 1267   train_loss : 0.0006\n",
            "epoch : 1267   train_loss : 0.0005\n",
            "epoch : 1267   val_loss : 0.0258\n",
            "epoch : 1267   val_loss : 0.0283\n",
            "epoch : 1268   train_loss : 0.0009\n",
            "epoch : 1268   train_loss : 0.0006\n",
            "epoch : 1268   train_loss : 0.0006\n",
            "epoch : 1268   train_loss : 0.0006\n",
            "epoch : 1268   val_loss : 0.0264\n",
            "epoch : 1268   val_loss : 0.0267\n",
            "epoch : 1269   train_loss : 0.0007\n",
            "epoch : 1269   train_loss : 0.0006\n",
            "epoch : 1269   train_loss : 0.0008\n",
            "epoch : 1269   train_loss : 0.0006\n",
            "epoch : 1269   val_loss : 0.0264\n",
            "epoch : 1269   val_loss : 0.0264\n",
            "epoch : 1270   train_loss : 0.0006\n",
            "epoch : 1270   train_loss : 0.0008\n",
            "epoch : 1270   train_loss : 0.0007\n",
            "epoch : 1270   train_loss : 0.0008\n",
            "epoch : 1270   val_loss : 0.0262\n",
            "epoch : 1270   val_loss : 0.0268\n",
            "epoch : 1271   train_loss : 0.0006\n",
            "epoch : 1271   train_loss : 0.0008\n",
            "epoch : 1271   train_loss : 0.0008\n",
            "epoch : 1271   train_loss : 0.0006\n",
            "epoch : 1271   val_loss : 0.0260\n",
            "epoch : 1271   val_loss : 0.0273\n",
            "epoch : 1272   train_loss : 0.0007\n",
            "epoch : 1272   train_loss : 0.0007\n",
            "epoch : 1272   train_loss : 0.0007\n",
            "epoch : 1272   train_loss : 0.0006\n",
            "epoch : 1272   val_loss : 0.0265\n",
            "epoch : 1272   val_loss : 0.0257\n",
            "epoch : 1273   train_loss : 0.0006\n",
            "epoch : 1273   train_loss : 0.0006\n",
            "epoch : 1273   train_loss : 0.0008\n",
            "epoch : 1273   train_loss : 0.0009\n",
            "epoch : 1273   val_loss : 0.0266\n",
            "epoch : 1273   val_loss : 0.0252\n",
            "epoch : 1274   train_loss : 0.0007\n",
            "epoch : 1274   train_loss : 0.0007\n",
            "epoch : 1274   train_loss : 0.0007\n",
            "epoch : 1274   train_loss : 0.0006\n",
            "epoch : 1274   val_loss : 0.0256\n",
            "epoch : 1274   val_loss : 0.0278\n",
            "epoch : 1275   train_loss : 0.0008\n",
            "epoch : 1275   train_loss : 0.0006\n",
            "epoch : 1275   train_loss : 0.0007\n",
            "epoch : 1275   train_loss : 0.0008\n",
            "epoch : 1275   val_loss : 0.0262\n",
            "epoch : 1275   val_loss : 0.0260\n",
            "epoch : 1276   train_loss : 0.0007\n",
            "epoch : 1276   train_loss : 0.0006\n",
            "epoch : 1276   train_loss : 0.0008\n",
            "epoch : 1276   train_loss : 0.0008\n",
            "epoch : 1276   val_loss : 0.0262\n",
            "epoch : 1276   val_loss : 0.0259\n",
            "epoch : 1277   train_loss : 0.0006\n",
            "epoch : 1277   train_loss : 0.0009\n",
            "epoch : 1277   train_loss : 0.0006\n",
            "epoch : 1277   train_loss : 0.0007\n",
            "epoch : 1277   val_loss : 0.0256\n",
            "epoch : 1277   val_loss : 0.0272\n",
            "epoch : 1278   train_loss : 0.0007\n",
            "epoch : 1278   train_loss : 0.0007\n",
            "epoch : 1278   train_loss : 0.0008\n",
            "epoch : 1278   train_loss : 0.0005\n",
            "epoch : 1278   val_loss : 0.0256\n",
            "epoch : 1278   val_loss : 0.0269\n",
            "epoch : 1279   train_loss : 0.0007\n",
            "epoch : 1279   train_loss : 0.0007\n",
            "epoch : 1279   train_loss : 0.0007\n",
            "epoch : 1279   train_loss : 0.0009\n",
            "epoch : 1279   val_loss : 0.0264\n",
            "epoch : 1279   val_loss : 0.0246\n",
            "epoch : 1280   train_loss : 0.0005\n",
            "epoch : 1280   train_loss : 0.0008\n",
            "epoch : 1280   train_loss : 0.0007\n",
            "epoch : 1280   train_loss : 0.0008\n",
            "epoch : 1280   val_loss : 0.0259\n",
            "epoch : 1280   val_loss : 0.0260\n",
            "epoch : 1281   train_loss : 0.0008\n",
            "epoch : 1281   train_loss : 0.0007\n",
            "epoch : 1281   train_loss : 0.0006\n",
            "epoch : 1281   train_loss : 0.0008\n",
            "epoch : 1281   val_loss : 0.0257\n",
            "epoch : 1281   val_loss : 0.0262\n",
            "epoch : 1282   train_loss : 0.0007\n",
            "epoch : 1282   train_loss : 0.0007\n",
            "epoch : 1282   train_loss : 0.0006\n",
            "epoch : 1282   train_loss : 0.0009\n",
            "epoch : 1282   val_loss : 0.0254\n",
            "epoch : 1282   val_loss : 0.0269\n",
            "epoch : 1283   train_loss : 0.0008\n",
            "epoch : 1283   train_loss : 0.0007\n",
            "epoch : 1283   train_loss : 0.0007\n",
            "epoch : 1283   train_loss : 0.0006\n",
            "epoch : 1283   val_loss : 0.0265\n",
            "epoch : 1283   val_loss : 0.0238\n",
            "epoch : 1284   train_loss : 0.0007\n",
            "epoch : 1284   train_loss : 0.0006\n",
            "epoch : 1284   train_loss : 0.0007\n",
            "epoch : 1284   train_loss : 0.0009\n",
            "epoch : 1284   val_loss : 0.0260\n",
            "epoch : 1284   val_loss : 0.0250\n",
            "epoch : 1285   train_loss : 0.0006\n",
            "epoch : 1285   train_loss : 0.0007\n",
            "epoch : 1285   train_loss : 0.0006\n",
            "epoch : 1285   train_loss : 0.0009\n",
            "epoch : 1285   val_loss : 0.0254\n",
            "epoch : 1285   val_loss : 0.0265\n",
            "epoch : 1286   train_loss : 0.0006\n",
            "epoch : 1286   train_loss : 0.0006\n",
            "epoch : 1286   train_loss : 0.0009\n",
            "epoch : 1286   train_loss : 0.0006\n",
            "epoch : 1286   val_loss : 0.0255\n",
            "epoch : 1286   val_loss : 0.0260\n",
            "epoch : 1287   train_loss : 0.0007\n",
            "epoch : 1287   train_loss : 0.0007\n",
            "epoch : 1287   train_loss : 0.0008\n",
            "epoch : 1287   train_loss : 0.0005\n",
            "epoch : 1287   val_loss : 0.0263\n",
            "epoch : 1287   val_loss : 0.0238\n",
            "epoch : 1288   train_loss : 0.0008\n",
            "epoch : 1288   train_loss : 0.0006\n",
            "epoch : 1288   train_loss : 0.0007\n",
            "epoch : 1288   train_loss : 0.0006\n",
            "epoch : 1288   val_loss : 0.0245\n",
            "epoch : 1288   val_loss : 0.0284\n",
            "epoch : 1289   train_loss : 0.0005\n",
            "epoch : 1289   train_loss : 0.0009\n",
            "epoch : 1289   train_loss : 0.0006\n",
            "epoch : 1289   train_loss : 0.0008\n",
            "epoch : 1289   val_loss : 0.0266\n",
            "epoch : 1289   val_loss : 0.0226\n",
            "epoch : 1290   train_loss : 0.0007\n",
            "epoch : 1290   train_loss : 0.0006\n",
            "epoch : 1290   train_loss : 0.0008\n",
            "epoch : 1290   train_loss : 0.0006\n",
            "epoch : 1290   val_loss : 0.0252\n",
            "epoch : 1290   val_loss : 0.0262\n",
            "epoch : 1291   train_loss : 0.0007\n",
            "epoch : 1291   train_loss : 0.0006\n",
            "epoch : 1291   train_loss : 0.0009\n",
            "epoch : 1291   train_loss : 0.0005\n",
            "epoch : 1291   val_loss : 0.0258\n",
            "epoch : 1291   val_loss : 0.0243\n",
            "epoch : 1292   train_loss : 0.0007\n",
            "epoch : 1292   train_loss : 0.0007\n",
            "epoch : 1292   train_loss : 0.0006\n",
            "epoch : 1292   train_loss : 0.0006\n",
            "epoch : 1292   val_loss : 0.0249\n",
            "epoch : 1292   val_loss : 0.0267\n",
            "epoch : 1293   train_loss : 0.0007\n",
            "epoch : 1293   train_loss : 0.0007\n",
            "epoch : 1293   train_loss : 0.0008\n",
            "epoch : 1293   train_loss : 0.0005\n",
            "epoch : 1293   val_loss : 0.0251\n",
            "epoch : 1293   val_loss : 0.0259\n",
            "epoch : 1294   train_loss : 0.0007\n",
            "epoch : 1294   train_loss : 0.0009\n",
            "epoch : 1294   train_loss : 0.0006\n",
            "epoch : 1294   train_loss : 0.0006\n",
            "epoch : 1294   val_loss : 0.0261\n",
            "epoch : 1294   val_loss : 0.0232\n",
            "epoch : 1295   train_loss : 0.0008\n",
            "epoch : 1295   train_loss : 0.0006\n",
            "epoch : 1295   train_loss : 0.0007\n",
            "epoch : 1295   train_loss : 0.0007\n",
            "epoch : 1295   val_loss : 0.0253\n",
            "epoch : 1295   val_loss : 0.0252\n",
            "epoch : 1296   train_loss : 0.0006\n",
            "epoch : 1296   train_loss : 0.0007\n",
            "epoch : 1296   train_loss : 0.0008\n",
            "epoch : 1296   train_loss : 0.0005\n",
            "epoch : 1296   val_loss : 0.0263\n",
            "epoch : 1296   val_loss : 0.0222\n",
            "new model saved at epoch 1296 with val_loss 0.02215665765106678\n",
            "epoch : 1297   train_loss : 0.0007\n",
            "epoch : 1297   train_loss : 0.0005\n",
            "epoch : 1297   train_loss : 0.0008\n",
            "epoch : 1297   train_loss : 0.0008\n",
            "epoch : 1297   val_loss : 0.0247\n",
            "epoch : 1297   val_loss : 0.0264\n",
            "epoch : 1298   train_loss : 0.0007\n",
            "epoch : 1298   train_loss : 0.0007\n",
            "epoch : 1298   train_loss : 0.0007\n",
            "epoch : 1298   train_loss : 0.0007\n",
            "epoch : 1298   val_loss : 0.0251\n",
            "epoch : 1298   val_loss : 0.0252\n",
            "epoch : 1299   train_loss : 0.0006\n",
            "epoch : 1299   train_loss : 0.0008\n",
            "epoch : 1299   train_loss : 0.0006\n",
            "epoch : 1299   train_loss : 0.0006\n",
            "epoch : 1299   val_loss : 0.0241\n",
            "epoch : 1299   val_loss : 0.0276\n",
            "epoch : 1300   train_loss : 0.0008\n",
            "epoch : 1300   train_loss : 0.0006\n",
            "epoch : 1300   train_loss : 0.0006\n",
            "epoch : 1300   train_loss : 0.0006\n",
            "epoch : 1300   val_loss : 0.0255\n",
            "epoch : 1300   val_loss : 0.0238\n",
            "epoch : 1301   train_loss : 0.0007\n",
            "epoch : 1301   train_loss : 0.0006\n",
            "epoch : 1301   train_loss : 0.0007\n",
            "epoch : 1301   train_loss : 0.0006\n",
            "epoch : 1301   val_loss : 0.0251\n",
            "epoch : 1301   val_loss : 0.0245\n",
            "epoch : 1302   train_loss : 0.0005\n",
            "epoch : 1302   train_loss : 0.0009\n",
            "epoch : 1302   train_loss : 0.0006\n",
            "epoch : 1302   train_loss : 0.0008\n",
            "epoch : 1302   val_loss : 0.0238\n",
            "epoch : 1302   val_loss : 0.0280\n",
            "epoch : 1303   train_loss : 0.0006\n",
            "epoch : 1303   train_loss : 0.0006\n",
            "epoch : 1303   train_loss : 0.0009\n",
            "epoch : 1303   train_loss : 0.0007\n",
            "epoch : 1303   val_loss : 0.0252\n",
            "epoch : 1303   val_loss : 0.0240\n",
            "epoch : 1304   train_loss : 0.0008\n",
            "epoch : 1304   train_loss : 0.0007\n",
            "epoch : 1304   train_loss : 0.0006\n",
            "epoch : 1304   train_loss : 0.0006\n",
            "epoch : 1304   val_loss : 0.0248\n",
            "epoch : 1304   val_loss : 0.0248\n",
            "epoch : 1305   train_loss : 0.0005\n",
            "epoch : 1305   train_loss : 0.0008\n",
            "epoch : 1305   train_loss : 0.0007\n",
            "epoch : 1305   train_loss : 0.0006\n",
            "epoch : 1305   val_loss : 0.0238\n",
            "epoch : 1305   val_loss : 0.0272\n",
            "epoch : 1306   train_loss : 0.0006\n",
            "epoch : 1306   train_loss : 0.0006\n",
            "epoch : 1306   train_loss : 0.0008\n",
            "epoch : 1306   train_loss : 0.0007\n",
            "epoch : 1306   val_loss : 0.0253\n",
            "epoch : 1306   val_loss : 0.0231\n",
            "epoch : 1307   train_loss : 0.0007\n",
            "epoch : 1307   train_loss : 0.0006\n",
            "epoch : 1307   train_loss : 0.0005\n",
            "epoch : 1307   train_loss : 0.0010\n",
            "epoch : 1307   val_loss : 0.0250\n",
            "epoch : 1307   val_loss : 0.0237\n",
            "epoch : 1308   train_loss : 0.0006\n",
            "epoch : 1308   train_loss : 0.0007\n",
            "epoch : 1308   train_loss : 0.0007\n",
            "epoch : 1308   train_loss : 0.0006\n",
            "epoch : 1308   val_loss : 0.0245\n",
            "epoch : 1308   val_loss : 0.0250\n",
            "epoch : 1309   train_loss : 0.0007\n",
            "epoch : 1309   train_loss : 0.0006\n",
            "epoch : 1309   train_loss : 0.0006\n",
            "epoch : 1309   train_loss : 0.0008\n",
            "epoch : 1309   val_loss : 0.0245\n",
            "epoch : 1309   val_loss : 0.0248\n",
            "epoch : 1310   train_loss : 0.0007\n",
            "epoch : 1310   train_loss : 0.0006\n",
            "epoch : 1310   train_loss : 0.0008\n",
            "epoch : 1310   train_loss : 0.0005\n",
            "epoch : 1310   val_loss : 0.0247\n",
            "epoch : 1310   val_loss : 0.0241\n",
            "epoch : 1311   train_loss : 0.0007\n",
            "epoch : 1311   train_loss : 0.0007\n",
            "epoch : 1311   train_loss : 0.0007\n",
            "epoch : 1311   train_loss : 0.0005\n",
            "epoch : 1311   val_loss : 0.0253\n",
            "epoch : 1311   val_loss : 0.0222\n",
            "epoch : 1312   train_loss : 0.0006\n",
            "epoch : 1312   train_loss : 0.0006\n",
            "epoch : 1312   train_loss : 0.0007\n",
            "epoch : 1312   train_loss : 0.0008\n",
            "epoch : 1312   val_loss : 0.0244\n",
            "epoch : 1312   val_loss : 0.0247\n",
            "epoch : 1313   train_loss : 0.0007\n",
            "epoch : 1313   train_loss : 0.0005\n",
            "epoch : 1313   train_loss : 0.0006\n",
            "epoch : 1313   train_loss : 0.0009\n",
            "epoch : 1313   val_loss : 0.0246\n",
            "epoch : 1313   val_loss : 0.0239\n",
            "epoch : 1314   train_loss : 0.0006\n",
            "epoch : 1314   train_loss : 0.0006\n",
            "epoch : 1314   train_loss : 0.0007\n",
            "epoch : 1314   train_loss : 0.0009\n",
            "epoch : 1314   val_loss : 0.0239\n",
            "epoch : 1314   val_loss : 0.0255\n",
            "epoch : 1315   train_loss : 0.0008\n",
            "epoch : 1315   train_loss : 0.0006\n",
            "epoch : 1315   train_loss : 0.0007\n",
            "epoch : 1315   train_loss : 0.0005\n",
            "epoch : 1315   val_loss : 0.0248\n",
            "epoch : 1315   val_loss : 0.0231\n",
            "epoch : 1316   train_loss : 0.0008\n",
            "epoch : 1316   train_loss : 0.0005\n",
            "epoch : 1316   train_loss : 0.0006\n",
            "epoch : 1316   train_loss : 0.0009\n",
            "epoch : 1316   val_loss : 0.0249\n",
            "epoch : 1316   val_loss : 0.0227\n",
            "epoch : 1317   train_loss : 0.0006\n",
            "epoch : 1317   train_loss : 0.0007\n",
            "epoch : 1317   train_loss : 0.0007\n",
            "epoch : 1317   train_loss : 0.0008\n",
            "epoch : 1317   val_loss : 0.0240\n",
            "epoch : 1317   val_loss : 0.0250\n",
            "epoch : 1318   train_loss : 0.0008\n",
            "epoch : 1318   train_loss : 0.0006\n",
            "epoch : 1318   train_loss : 0.0006\n",
            "epoch : 1318   train_loss : 0.0006\n",
            "epoch : 1318   val_loss : 0.0238\n",
            "epoch : 1318   val_loss : 0.0252\n",
            "epoch : 1319   train_loss : 0.0008\n",
            "epoch : 1319   train_loss : 0.0006\n",
            "epoch : 1319   train_loss : 0.0006\n",
            "epoch : 1319   train_loss : 0.0007\n",
            "epoch : 1319   val_loss : 0.0238\n",
            "epoch : 1319   val_loss : 0.0250\n",
            "epoch : 1320   train_loss : 0.0007\n",
            "epoch : 1320   train_loss : 0.0006\n",
            "epoch : 1320   train_loss : 0.0006\n",
            "epoch : 1320   train_loss : 0.0006\n",
            "epoch : 1320   val_loss : 0.0244\n",
            "epoch : 1320   val_loss : 0.0234\n",
            "epoch : 1321   train_loss : 0.0006\n",
            "epoch : 1321   train_loss : 0.0007\n",
            "epoch : 1321   train_loss : 0.0006\n",
            "epoch : 1321   train_loss : 0.0008\n",
            "epoch : 1321   val_loss : 0.0243\n",
            "epoch : 1321   val_loss : 0.0235\n",
            "epoch : 1322   train_loss : 0.0006\n",
            "epoch : 1322   train_loss : 0.0007\n",
            "epoch : 1322   train_loss : 0.0007\n",
            "epoch : 1322   train_loss : 0.0005\n",
            "epoch : 1322   val_loss : 0.0241\n",
            "epoch : 1322   val_loss : 0.0240\n",
            "epoch : 1323   train_loss : 0.0005\n",
            "epoch : 1323   train_loss : 0.0007\n",
            "epoch : 1323   train_loss : 0.0007\n",
            "epoch : 1323   train_loss : 0.0007\n",
            "epoch : 1323   val_loss : 0.0244\n",
            "epoch : 1323   val_loss : 0.0229\n",
            "epoch : 1324   train_loss : 0.0007\n",
            "epoch : 1324   train_loss : 0.0007\n",
            "epoch : 1324   train_loss : 0.0006\n",
            "epoch : 1324   train_loss : 0.0006\n",
            "epoch : 1324   val_loss : 0.0241\n",
            "epoch : 1324   val_loss : 0.0235\n",
            "epoch : 1325   train_loss : 0.0007\n",
            "epoch : 1325   train_loss : 0.0006\n",
            "epoch : 1325   train_loss : 0.0007\n",
            "epoch : 1325   train_loss : 0.0005\n",
            "epoch : 1325   val_loss : 0.0233\n",
            "epoch : 1325   val_loss : 0.0255\n",
            "epoch : 1326   train_loss : 0.0007\n",
            "epoch : 1326   train_loss : 0.0006\n",
            "epoch : 1326   train_loss : 0.0007\n",
            "epoch : 1326   train_loss : 0.0007\n",
            "epoch : 1326   val_loss : 0.0239\n",
            "epoch : 1326   val_loss : 0.0237\n",
            "epoch : 1327   train_loss : 0.0005\n",
            "epoch : 1327   train_loss : 0.0009\n",
            "epoch : 1327   train_loss : 0.0006\n",
            "epoch : 1327   train_loss : 0.0005\n",
            "epoch : 1327   val_loss : 0.0227\n",
            "epoch : 1327   val_loss : 0.0268\n",
            "epoch : 1328   train_loss : 0.0005\n",
            "epoch : 1328   train_loss : 0.0008\n",
            "epoch : 1328   train_loss : 0.0006\n",
            "epoch : 1328   train_loss : 0.0009\n",
            "epoch : 1328   val_loss : 0.0239\n",
            "epoch : 1328   val_loss : 0.0236\n",
            "epoch : 1329   train_loss : 0.0007\n",
            "epoch : 1329   train_loss : 0.0006\n",
            "epoch : 1329   train_loss : 0.0007\n",
            "epoch : 1329   train_loss : 0.0006\n",
            "epoch : 1329   val_loss : 0.0242\n",
            "epoch : 1329   val_loss : 0.0224\n",
            "epoch : 1330   train_loss : 0.0006\n",
            "epoch : 1330   train_loss : 0.0008\n",
            "epoch : 1330   train_loss : 0.0006\n",
            "epoch : 1330   train_loss : 0.0005\n",
            "epoch : 1330   val_loss : 0.0232\n",
            "epoch : 1330   val_loss : 0.0251\n",
            "epoch : 1331   train_loss : 0.0006\n",
            "epoch : 1331   train_loss : 0.0007\n",
            "epoch : 1331   train_loss : 0.0006\n",
            "epoch : 1331   train_loss : 0.0007\n",
            "epoch : 1331   val_loss : 0.0246\n",
            "epoch : 1331   val_loss : 0.0212\n",
            "new model saved at epoch 1331 with val_loss 0.021241167560219765\n",
            "epoch : 1332   train_loss : 0.0006\n",
            "epoch : 1332   train_loss : 0.0007\n",
            "epoch : 1332   train_loss : 0.0006\n",
            "epoch : 1332   train_loss : 0.0009\n",
            "epoch : 1332   val_loss : 0.0235\n",
            "epoch : 1332   val_loss : 0.0240\n",
            "epoch : 1333   train_loss : 0.0007\n",
            "epoch : 1333   train_loss : 0.0006\n",
            "epoch : 1333   train_loss : 0.0006\n",
            "epoch : 1333   train_loss : 0.0006\n",
            "epoch : 1333   val_loss : 0.0239\n",
            "epoch : 1333   val_loss : 0.0228\n",
            "epoch : 1334   train_loss : 0.0005\n",
            "epoch : 1334   train_loss : 0.0008\n",
            "epoch : 1334   train_loss : 0.0006\n",
            "epoch : 1334   train_loss : 0.0006\n",
            "epoch : 1334   val_loss : 0.0242\n",
            "epoch : 1334   val_loss : 0.0217\n",
            "epoch : 1335   train_loss : 0.0006\n",
            "epoch : 1335   train_loss : 0.0006\n",
            "epoch : 1335   train_loss : 0.0007\n",
            "epoch : 1335   train_loss : 0.0008\n",
            "epoch : 1335   val_loss : 0.0238\n",
            "epoch : 1335   val_loss : 0.0226\n",
            "epoch : 1336   train_loss : 0.0006\n",
            "epoch : 1336   train_loss : 0.0006\n",
            "epoch : 1336   train_loss : 0.0007\n",
            "epoch : 1336   train_loss : 0.0007\n",
            "epoch : 1336   val_loss : 0.0230\n",
            "epoch : 1336   val_loss : 0.0247\n",
            "epoch : 1337   train_loss : 0.0005\n",
            "epoch : 1337   train_loss : 0.0008\n",
            "epoch : 1337   train_loss : 0.0006\n",
            "epoch : 1337   train_loss : 0.0008\n",
            "epoch : 1337   val_loss : 0.0237\n",
            "epoch : 1337   val_loss : 0.0225\n",
            "epoch : 1338   train_loss : 0.0006\n",
            "epoch : 1338   train_loss : 0.0007\n",
            "epoch : 1338   train_loss : 0.0007\n",
            "epoch : 1338   train_loss : 0.0006\n",
            "epoch : 1338   val_loss : 0.0233\n",
            "epoch : 1338   val_loss : 0.0234\n",
            "epoch : 1339   train_loss : 0.0007\n",
            "epoch : 1339   train_loss : 0.0007\n",
            "epoch : 1339   train_loss : 0.0005\n",
            "epoch : 1339   train_loss : 0.0007\n",
            "epoch : 1339   val_loss : 0.0236\n",
            "epoch : 1339   val_loss : 0.0226\n",
            "epoch : 1340   train_loss : 0.0007\n",
            "epoch : 1340   train_loss : 0.0007\n",
            "epoch : 1340   train_loss : 0.0005\n",
            "epoch : 1340   train_loss : 0.0007\n",
            "epoch : 1340   val_loss : 0.0231\n",
            "epoch : 1340   val_loss : 0.0237\n",
            "epoch : 1341   train_loss : 0.0006\n",
            "epoch : 1341   train_loss : 0.0008\n",
            "epoch : 1341   train_loss : 0.0006\n",
            "epoch : 1341   train_loss : 0.0006\n",
            "epoch : 1341   val_loss : 0.0227\n",
            "epoch : 1341   val_loss : 0.0247\n",
            "epoch : 1342   train_loss : 0.0006\n",
            "epoch : 1342   train_loss : 0.0009\n",
            "epoch : 1342   train_loss : 0.0006\n",
            "epoch : 1342   train_loss : 0.0004\n",
            "epoch : 1342   val_loss : 0.0233\n",
            "epoch : 1342   val_loss : 0.0229\n",
            "epoch : 1343   train_loss : 0.0008\n",
            "epoch : 1343   train_loss : 0.0006\n",
            "epoch : 1343   train_loss : 0.0006\n",
            "epoch : 1343   train_loss : 0.0007\n",
            "epoch : 1343   val_loss : 0.0231\n",
            "epoch : 1343   val_loss : 0.0234\n",
            "epoch : 1344   train_loss : 0.0006\n",
            "epoch : 1344   train_loss : 0.0006\n",
            "epoch : 1344   train_loss : 0.0006\n",
            "epoch : 1344   train_loss : 0.0007\n",
            "epoch : 1344   val_loss : 0.0238\n",
            "epoch : 1344   val_loss : 0.0213\n",
            "epoch : 1345   train_loss : 0.0007\n",
            "epoch : 1345   train_loss : 0.0006\n",
            "epoch : 1345   train_loss : 0.0006\n",
            "epoch : 1345   train_loss : 0.0005\n",
            "epoch : 1345   val_loss : 0.0232\n",
            "epoch : 1345   val_loss : 0.0226\n",
            "epoch : 1346   train_loss : 0.0005\n",
            "epoch : 1346   train_loss : 0.0007\n",
            "epoch : 1346   train_loss : 0.0007\n",
            "epoch : 1346   train_loss : 0.0006\n",
            "epoch : 1346   val_loss : 0.0221\n",
            "epoch : 1346   val_loss : 0.0254\n",
            "epoch : 1347   train_loss : 0.0007\n",
            "epoch : 1347   train_loss : 0.0006\n",
            "epoch : 1347   train_loss : 0.0007\n",
            "epoch : 1347   train_loss : 0.0005\n",
            "epoch : 1347   val_loss : 0.0225\n",
            "epoch : 1347   val_loss : 0.0242\n",
            "epoch : 1348   train_loss : 0.0006\n",
            "epoch : 1348   train_loss : 0.0006\n",
            "epoch : 1348   train_loss : 0.0007\n",
            "epoch : 1348   train_loss : 0.0006\n",
            "epoch : 1348   val_loss : 0.0224\n",
            "epoch : 1348   val_loss : 0.0244\n",
            "epoch : 1349   train_loss : 0.0006\n",
            "epoch : 1349   train_loss : 0.0007\n",
            "epoch : 1349   train_loss : 0.0006\n",
            "epoch : 1349   train_loss : 0.0005\n",
            "epoch : 1349   val_loss : 0.0231\n",
            "epoch : 1349   val_loss : 0.0223\n",
            "epoch : 1350   train_loss : 0.0007\n",
            "epoch : 1350   train_loss : 0.0006\n",
            "epoch : 1350   train_loss : 0.0006\n",
            "epoch : 1350   train_loss : 0.0006\n",
            "epoch : 1350   val_loss : 0.0224\n",
            "epoch : 1350   val_loss : 0.0240\n",
            "epoch : 1351   train_loss : 0.0008\n",
            "epoch : 1351   train_loss : 0.0006\n",
            "epoch : 1351   train_loss : 0.0006\n",
            "epoch : 1351   train_loss : 0.0005\n",
            "epoch : 1351   val_loss : 0.0229\n",
            "epoch : 1351   val_loss : 0.0226\n",
            "epoch : 1352   train_loss : 0.0007\n",
            "epoch : 1352   train_loss : 0.0007\n",
            "epoch : 1352   train_loss : 0.0006\n",
            "epoch : 1352   train_loss : 0.0005\n",
            "epoch : 1352   val_loss : 0.0219\n",
            "epoch : 1352   val_loss : 0.0250\n",
            "epoch : 1353   train_loss : 0.0006\n",
            "epoch : 1353   train_loss : 0.0006\n",
            "epoch : 1353   train_loss : 0.0007\n",
            "epoch : 1353   train_loss : 0.0007\n",
            "epoch : 1353   val_loss : 0.0228\n",
            "epoch : 1353   val_loss : 0.0226\n",
            "epoch : 1354   train_loss : 0.0008\n",
            "epoch : 1354   train_loss : 0.0005\n",
            "epoch : 1354   train_loss : 0.0007\n",
            "epoch : 1354   train_loss : 0.0004\n",
            "epoch : 1354   val_loss : 0.0224\n",
            "epoch : 1354   val_loss : 0.0234\n",
            "epoch : 1355   train_loss : 0.0006\n",
            "epoch : 1355   train_loss : 0.0006\n",
            "epoch : 1355   train_loss : 0.0006\n",
            "epoch : 1355   train_loss : 0.0008\n",
            "epoch : 1355   val_loss : 0.0226\n",
            "epoch : 1355   val_loss : 0.0227\n",
            "epoch : 1356   train_loss : 0.0006\n",
            "epoch : 1356   train_loss : 0.0006\n",
            "epoch : 1356   train_loss : 0.0006\n",
            "epoch : 1356   train_loss : 0.0007\n",
            "epoch : 1356   val_loss : 0.0236\n",
            "epoch : 1356   val_loss : 0.0200\n",
            "new model saved at epoch 1356 with val_loss 0.0199871938675642\n",
            "epoch : 1357   train_loss : 0.0008\n",
            "epoch : 1357   train_loss : 0.0005\n",
            "epoch : 1357   train_loss : 0.0005\n",
            "epoch : 1357   train_loss : 0.0007\n",
            "epoch : 1357   val_loss : 0.0218\n",
            "epoch : 1357   val_loss : 0.0246\n",
            "epoch : 1358   train_loss : 0.0007\n",
            "epoch : 1358   train_loss : 0.0006\n",
            "epoch : 1358   train_loss : 0.0007\n",
            "epoch : 1358   train_loss : 0.0005\n",
            "epoch : 1358   val_loss : 0.0228\n",
            "epoch : 1358   val_loss : 0.0218\n",
            "epoch : 1359   train_loss : 0.0006\n",
            "epoch : 1359   train_loss : 0.0007\n",
            "epoch : 1359   train_loss : 0.0005\n",
            "epoch : 1359   train_loss : 0.0008\n",
            "epoch : 1359   val_loss : 0.0229\n",
            "epoch : 1359   val_loss : 0.0214\n",
            "epoch : 1360   train_loss : 0.0005\n",
            "epoch : 1360   train_loss : 0.0007\n",
            "epoch : 1360   train_loss : 0.0007\n",
            "epoch : 1360   train_loss : 0.0007\n",
            "epoch : 1360   val_loss : 0.0225\n",
            "epoch : 1360   val_loss : 0.0222\n",
            "epoch : 1361   train_loss : 0.0007\n",
            "epoch : 1361   train_loss : 0.0005\n",
            "epoch : 1361   train_loss : 0.0006\n",
            "epoch : 1361   train_loss : 0.0007\n",
            "epoch : 1361   val_loss : 0.0225\n",
            "epoch : 1361   val_loss : 0.0223\n",
            "epoch : 1362   train_loss : 0.0008\n",
            "epoch : 1362   train_loss : 0.0006\n",
            "epoch : 1362   train_loss : 0.0005\n",
            "epoch : 1362   train_loss : 0.0005\n",
            "epoch : 1362   val_loss : 0.0228\n",
            "epoch : 1362   val_loss : 0.0212\n",
            "epoch : 1363   train_loss : 0.0006\n",
            "epoch : 1363   train_loss : 0.0005\n",
            "epoch : 1363   train_loss : 0.0007\n",
            "epoch : 1363   train_loss : 0.0006\n",
            "epoch : 1363   val_loss : 0.0227\n",
            "epoch : 1363   val_loss : 0.0214\n",
            "epoch : 1364   train_loss : 0.0005\n",
            "epoch : 1364   train_loss : 0.0005\n",
            "epoch : 1364   train_loss : 0.0007\n",
            "epoch : 1364   train_loss : 0.0008\n",
            "epoch : 1364   val_loss : 0.0221\n",
            "epoch : 1364   val_loss : 0.0228\n",
            "epoch : 1365   train_loss : 0.0005\n",
            "epoch : 1365   train_loss : 0.0007\n",
            "epoch : 1365   train_loss : 0.0006\n",
            "epoch : 1365   train_loss : 0.0006\n",
            "epoch : 1365   val_loss : 0.0221\n",
            "epoch : 1365   val_loss : 0.0227\n",
            "epoch : 1366   train_loss : 0.0005\n",
            "epoch : 1366   train_loss : 0.0006\n",
            "epoch : 1366   train_loss : 0.0007\n",
            "epoch : 1366   train_loss : 0.0007\n",
            "epoch : 1366   val_loss : 0.0218\n",
            "epoch : 1366   val_loss : 0.0233\n",
            "epoch : 1367   train_loss : 0.0006\n",
            "epoch : 1367   train_loss : 0.0006\n",
            "epoch : 1367   train_loss : 0.0008\n",
            "epoch : 1367   train_loss : 0.0005\n",
            "epoch : 1367   val_loss : 0.0219\n",
            "epoch : 1367   val_loss : 0.0227\n",
            "epoch : 1368   train_loss : 0.0005\n",
            "epoch : 1368   train_loss : 0.0005\n",
            "epoch : 1368   train_loss : 0.0007\n",
            "epoch : 1368   train_loss : 0.0008\n",
            "epoch : 1368   val_loss : 0.0225\n",
            "epoch : 1368   val_loss : 0.0210\n",
            "epoch : 1369   train_loss : 0.0007\n",
            "epoch : 1369   train_loss : 0.0006\n",
            "epoch : 1369   train_loss : 0.0006\n",
            "epoch : 1369   train_loss : 0.0006\n",
            "epoch : 1369   val_loss : 0.0228\n",
            "epoch : 1369   val_loss : 0.0200\n",
            "epoch : 1370   train_loss : 0.0006\n",
            "epoch : 1370   train_loss : 0.0006\n",
            "epoch : 1370   train_loss : 0.0007\n",
            "epoch : 1370   train_loss : 0.0006\n",
            "epoch : 1370   val_loss : 0.0220\n",
            "epoch : 1370   val_loss : 0.0222\n",
            "epoch : 1371   train_loss : 0.0006\n",
            "epoch : 1371   train_loss : 0.0006\n",
            "epoch : 1371   train_loss : 0.0007\n",
            "epoch : 1371   train_loss : 0.0005\n",
            "epoch : 1371   val_loss : 0.0222\n",
            "epoch : 1371   val_loss : 0.0214\n",
            "epoch : 1372   train_loss : 0.0006\n",
            "epoch : 1372   train_loss : 0.0006\n",
            "epoch : 1372   train_loss : 0.0005\n",
            "epoch : 1372   train_loss : 0.0010\n",
            "epoch : 1372   val_loss : 0.0221\n",
            "epoch : 1372   val_loss : 0.0214\n",
            "epoch : 1373   train_loss : 0.0005\n",
            "epoch : 1373   train_loss : 0.0007\n",
            "epoch : 1373   train_loss : 0.0005\n",
            "epoch : 1373   train_loss : 0.0010\n",
            "epoch : 1373   val_loss : 0.0229\n",
            "epoch : 1373   val_loss : 0.0191\n",
            "new model saved at epoch 1373 with val_loss 0.019143816083669662\n",
            "epoch : 1374   train_loss : 0.0005\n",
            "epoch : 1374   train_loss : 0.0007\n",
            "epoch : 1374   train_loss : 0.0005\n",
            "epoch : 1374   train_loss : 0.0007\n",
            "epoch : 1374   val_loss : 0.0208\n",
            "epoch : 1374   val_loss : 0.0247\n",
            "epoch : 1375   train_loss : 0.0008\n",
            "epoch : 1375   train_loss : 0.0007\n",
            "epoch : 1375   train_loss : 0.0005\n",
            "epoch : 1375   train_loss : 0.0004\n",
            "epoch : 1375   val_loss : 0.0225\n",
            "epoch : 1375   val_loss : 0.0200\n",
            "epoch : 1376   train_loss : 0.0006\n",
            "epoch : 1376   train_loss : 0.0006\n",
            "epoch : 1376   train_loss : 0.0007\n",
            "epoch : 1376   train_loss : 0.0006\n",
            "epoch : 1376   val_loss : 0.0227\n",
            "epoch : 1376   val_loss : 0.0193\n",
            "epoch : 1377   train_loss : 0.0005\n",
            "epoch : 1377   train_loss : 0.0006\n",
            "epoch : 1377   train_loss : 0.0007\n",
            "epoch : 1377   train_loss : 0.0007\n",
            "epoch : 1377   val_loss : 0.0217\n",
            "epoch : 1377   val_loss : 0.0220\n",
            "epoch : 1378   train_loss : 0.0006\n",
            "epoch : 1378   train_loss : 0.0008\n",
            "epoch : 1378   train_loss : 0.0005\n",
            "epoch : 1378   train_loss : 0.0005\n",
            "epoch : 1378   val_loss : 0.0216\n",
            "epoch : 1378   val_loss : 0.0221\n",
            "epoch : 1379   train_loss : 0.0006\n",
            "epoch : 1379   train_loss : 0.0006\n",
            "epoch : 1379   train_loss : 0.0006\n",
            "epoch : 1379   train_loss : 0.0006\n",
            "epoch : 1379   val_loss : 0.0218\n",
            "epoch : 1379   val_loss : 0.0214\n",
            "epoch : 1380   train_loss : 0.0005\n",
            "epoch : 1380   train_loss : 0.0007\n",
            "epoch : 1380   train_loss : 0.0007\n",
            "epoch : 1380   train_loss : 0.0005\n",
            "epoch : 1380   val_loss : 0.0217\n",
            "epoch : 1380   val_loss : 0.0214\n",
            "epoch : 1381   train_loss : 0.0007\n",
            "epoch : 1381   train_loss : 0.0006\n",
            "epoch : 1381   train_loss : 0.0006\n",
            "epoch : 1381   train_loss : 0.0005\n",
            "epoch : 1381   val_loss : 0.0213\n",
            "epoch : 1381   val_loss : 0.0223\n",
            "epoch : 1382   train_loss : 0.0006\n",
            "epoch : 1382   train_loss : 0.0007\n",
            "epoch : 1382   train_loss : 0.0005\n",
            "epoch : 1382   train_loss : 0.0006\n",
            "epoch : 1382   val_loss : 0.0219\n",
            "epoch : 1382   val_loss : 0.0207\n",
            "epoch : 1383   train_loss : 0.0005\n",
            "epoch : 1383   train_loss : 0.0007\n",
            "epoch : 1383   train_loss : 0.0006\n",
            "epoch : 1383   train_loss : 0.0006\n",
            "epoch : 1383   val_loss : 0.0220\n",
            "epoch : 1383   val_loss : 0.0203\n",
            "epoch : 1384   train_loss : 0.0006\n",
            "epoch : 1384   train_loss : 0.0005\n",
            "epoch : 1384   train_loss : 0.0007\n",
            "epoch : 1384   train_loss : 0.0006\n",
            "epoch : 1384   val_loss : 0.0218\n",
            "epoch : 1384   val_loss : 0.0206\n",
            "epoch : 1385   train_loss : 0.0006\n",
            "epoch : 1385   train_loss : 0.0006\n",
            "epoch : 1385   train_loss : 0.0006\n",
            "epoch : 1385   train_loss : 0.0006\n",
            "epoch : 1385   val_loss : 0.0210\n",
            "epoch : 1385   val_loss : 0.0226\n",
            "epoch : 1386   train_loss : 0.0008\n",
            "epoch : 1386   train_loss : 0.0005\n",
            "epoch : 1386   train_loss : 0.0006\n",
            "epoch : 1386   train_loss : 0.0006\n",
            "epoch : 1386   val_loss : 0.0222\n",
            "epoch : 1386   val_loss : 0.0194\n",
            "epoch : 1387   train_loss : 0.0005\n",
            "epoch : 1387   train_loss : 0.0005\n",
            "epoch : 1387   train_loss : 0.0008\n",
            "epoch : 1387   train_loss : 0.0005\n",
            "epoch : 1387   val_loss : 0.0216\n",
            "epoch : 1387   val_loss : 0.0206\n",
            "epoch : 1388   train_loss : 0.0007\n",
            "epoch : 1388   train_loss : 0.0006\n",
            "epoch : 1388   train_loss : 0.0005\n",
            "epoch : 1388   train_loss : 0.0005\n",
            "epoch : 1388   val_loss : 0.0210\n",
            "epoch : 1388   val_loss : 0.0220\n",
            "epoch : 1389   train_loss : 0.0007\n",
            "epoch : 1389   train_loss : 0.0006\n",
            "epoch : 1389   train_loss : 0.0005\n",
            "epoch : 1389   train_loss : 0.0004\n",
            "epoch : 1389   val_loss : 0.0214\n",
            "epoch : 1389   val_loss : 0.0210\n",
            "epoch : 1390   train_loss : 0.0008\n",
            "epoch : 1390   train_loss : 0.0005\n",
            "epoch : 1390   train_loss : 0.0005\n",
            "epoch : 1390   train_loss : 0.0006\n",
            "epoch : 1390   val_loss : 0.0220\n",
            "epoch : 1390   val_loss : 0.0190\n",
            "new model saved at epoch 1390 with val_loss 0.01904398389160633\n",
            "epoch : 1391   train_loss : 0.0006\n",
            "epoch : 1391   train_loss : 0.0006\n",
            "epoch : 1391   train_loss : 0.0006\n",
            "epoch : 1391   train_loss : 0.0005\n",
            "epoch : 1391   val_loss : 0.0211\n",
            "epoch : 1391   val_loss : 0.0215\n",
            "epoch : 1392   train_loss : 0.0005\n",
            "epoch : 1392   train_loss : 0.0007\n",
            "epoch : 1392   train_loss : 0.0006\n",
            "epoch : 1392   train_loss : 0.0006\n",
            "epoch : 1392   val_loss : 0.0222\n",
            "epoch : 1392   val_loss : 0.0183\n",
            "new model saved at epoch 1392 with val_loss 0.018255673348903656\n",
            "epoch : 1393   train_loss : 0.0007\n",
            "epoch : 1393   train_loss : 0.0006\n",
            "epoch : 1393   train_loss : 0.0006\n",
            "epoch : 1393   train_loss : 0.0004\n",
            "epoch : 1393   val_loss : 0.0209\n",
            "epoch : 1393   val_loss : 0.0217\n",
            "epoch : 1394   train_loss : 0.0006\n",
            "epoch : 1394   train_loss : 0.0004\n",
            "epoch : 1394   train_loss : 0.0007\n",
            "epoch : 1394   train_loss : 0.0007\n",
            "epoch : 1394   val_loss : 0.0212\n",
            "epoch : 1394   val_loss : 0.0207\n",
            "epoch : 1395   train_loss : 0.0007\n",
            "epoch : 1395   train_loss : 0.0007\n",
            "epoch : 1395   train_loss : 0.0005\n",
            "epoch : 1395   train_loss : 0.0005\n",
            "epoch : 1395   val_loss : 0.0211\n",
            "epoch : 1395   val_loss : 0.0209\n",
            "epoch : 1396   train_loss : 0.0006\n",
            "epoch : 1396   train_loss : 0.0005\n",
            "epoch : 1396   train_loss : 0.0006\n",
            "epoch : 1396   train_loss : 0.0007\n",
            "epoch : 1396   val_loss : 0.0208\n",
            "epoch : 1396   val_loss : 0.0216\n",
            "epoch : 1397   train_loss : 0.0006\n",
            "epoch : 1397   train_loss : 0.0005\n",
            "epoch : 1397   train_loss : 0.0007\n",
            "epoch : 1397   train_loss : 0.0004\n",
            "epoch : 1397   val_loss : 0.0210\n",
            "epoch : 1397   val_loss : 0.0209\n",
            "epoch : 1398   train_loss : 0.0004\n",
            "epoch : 1398   train_loss : 0.0008\n",
            "epoch : 1398   train_loss : 0.0005\n",
            "epoch : 1398   train_loss : 0.0006\n",
            "epoch : 1398   val_loss : 0.0214\n",
            "epoch : 1398   val_loss : 0.0196\n",
            "epoch : 1399   train_loss : 0.0006\n",
            "epoch : 1399   train_loss : 0.0006\n",
            "epoch : 1399   train_loss : 0.0007\n",
            "epoch : 1399   train_loss : 0.0005\n",
            "epoch : 1399   val_loss : 0.0209\n",
            "epoch : 1399   val_loss : 0.0208\n",
            "epoch : 1400   train_loss : 0.0006\n",
            "epoch : 1400   train_loss : 0.0007\n",
            "epoch : 1400   train_loss : 0.0005\n",
            "epoch : 1400   train_loss : 0.0005\n",
            "epoch : 1400   val_loss : 0.0205\n",
            "epoch : 1400   val_loss : 0.0217\n",
            "epoch : 1401   train_loss : 0.0006\n",
            "epoch : 1401   train_loss : 0.0005\n",
            "epoch : 1401   train_loss : 0.0006\n",
            "epoch : 1401   train_loss : 0.0007\n",
            "epoch : 1401   val_loss : 0.0203\n",
            "epoch : 1401   val_loss : 0.0222\n",
            "epoch : 1402   train_loss : 0.0005\n",
            "epoch : 1402   train_loss : 0.0005\n",
            "epoch : 1402   train_loss : 0.0007\n",
            "epoch : 1402   train_loss : 0.0007\n",
            "epoch : 1402   val_loss : 0.0209\n",
            "epoch : 1402   val_loss : 0.0202\n",
            "epoch : 1403   train_loss : 0.0004\n",
            "epoch : 1403   train_loss : 0.0008\n",
            "epoch : 1403   train_loss : 0.0005\n",
            "epoch : 1403   train_loss : 0.0007\n",
            "epoch : 1403   val_loss : 0.0206\n",
            "epoch : 1403   val_loss : 0.0210\n",
            "epoch : 1404   train_loss : 0.0006\n",
            "epoch : 1404   train_loss : 0.0004\n",
            "epoch : 1404   train_loss : 0.0006\n",
            "epoch : 1404   train_loss : 0.0008\n",
            "epoch : 1404   val_loss : 0.0208\n",
            "epoch : 1404   val_loss : 0.0203\n",
            "epoch : 1405   train_loss : 0.0006\n",
            "epoch : 1405   train_loss : 0.0006\n",
            "epoch : 1405   train_loss : 0.0006\n",
            "epoch : 1405   train_loss : 0.0006\n",
            "epoch : 1405   val_loss : 0.0204\n",
            "epoch : 1405   val_loss : 0.0212\n",
            "epoch : 1406   train_loss : 0.0005\n",
            "epoch : 1406   train_loss : 0.0006\n",
            "epoch : 1406   train_loss : 0.0007\n",
            "epoch : 1406   train_loss : 0.0005\n",
            "epoch : 1406   val_loss : 0.0211\n",
            "epoch : 1406   val_loss : 0.0193\n",
            "epoch : 1407   train_loss : 0.0006\n",
            "epoch : 1407   train_loss : 0.0005\n",
            "epoch : 1407   train_loss : 0.0006\n",
            "epoch : 1407   train_loss : 0.0006\n",
            "epoch : 1407   val_loss : 0.0202\n",
            "epoch : 1407   val_loss : 0.0215\n",
            "epoch : 1408   train_loss : 0.0005\n",
            "epoch : 1408   train_loss : 0.0004\n",
            "epoch : 1408   train_loss : 0.0006\n",
            "epoch : 1408   train_loss : 0.0009\n",
            "epoch : 1408   val_loss : 0.0204\n",
            "epoch : 1408   val_loss : 0.0208\n",
            "epoch : 1409   train_loss : 0.0006\n",
            "epoch : 1409   train_loss : 0.0005\n",
            "epoch : 1409   train_loss : 0.0006\n",
            "epoch : 1409   train_loss : 0.0005\n",
            "epoch : 1409   val_loss : 0.0198\n",
            "epoch : 1409   val_loss : 0.0224\n",
            "epoch : 1410   train_loss : 0.0008\n",
            "epoch : 1410   train_loss : 0.0004\n",
            "epoch : 1410   train_loss : 0.0004\n",
            "epoch : 1410   train_loss : 0.0008\n",
            "epoch : 1410   val_loss : 0.0197\n",
            "epoch : 1410   val_loss : 0.0226\n",
            "epoch : 1411   train_loss : 0.0008\n",
            "epoch : 1411   train_loss : 0.0006\n",
            "epoch : 1411   train_loss : 0.0004\n",
            "epoch : 1411   train_loss : 0.0004\n",
            "epoch : 1411   val_loss : 0.0198\n",
            "epoch : 1411   val_loss : 0.0222\n",
            "epoch : 1412   train_loss : 0.0004\n",
            "epoch : 1412   train_loss : 0.0005\n",
            "epoch : 1412   train_loss : 0.0009\n",
            "epoch : 1412   train_loss : 0.0005\n",
            "epoch : 1412   val_loss : 0.0201\n",
            "epoch : 1412   val_loss : 0.0211\n",
            "epoch : 1413   train_loss : 0.0005\n",
            "epoch : 1413   train_loss : 0.0006\n",
            "epoch : 1413   train_loss : 0.0007\n",
            "epoch : 1413   train_loss : 0.0005\n",
            "epoch : 1413   val_loss : 0.0202\n",
            "epoch : 1413   val_loss : 0.0207\n",
            "epoch : 1414   train_loss : 0.0006\n",
            "epoch : 1414   train_loss : 0.0006\n",
            "epoch : 1414   train_loss : 0.0007\n",
            "epoch : 1414   train_loss : 0.0004\n",
            "epoch : 1414   val_loss : 0.0200\n",
            "epoch : 1414   val_loss : 0.0210\n",
            "epoch : 1415   train_loss : 0.0006\n",
            "epoch : 1415   train_loss : 0.0006\n",
            "epoch : 1415   train_loss : 0.0006\n",
            "epoch : 1415   train_loss : 0.0004\n",
            "epoch : 1415   val_loss : 0.0212\n",
            "epoch : 1415   val_loss : 0.0178\n",
            "new model saved at epoch 1415 with val_loss 0.017751984298229218\n",
            "epoch : 1416   train_loss : 0.0005\n",
            "epoch : 1416   train_loss : 0.0005\n",
            "epoch : 1416   train_loss : 0.0007\n",
            "epoch : 1416   train_loss : 0.0005\n",
            "epoch : 1416   val_loss : 0.0204\n",
            "epoch : 1416   val_loss : 0.0198\n",
            "epoch : 1417   train_loss : 0.0007\n",
            "epoch : 1417   train_loss : 0.0005\n",
            "epoch : 1417   train_loss : 0.0005\n",
            "epoch : 1417   train_loss : 0.0005\n",
            "epoch : 1417   val_loss : 0.0199\n",
            "epoch : 1417   val_loss : 0.0208\n",
            "epoch : 1418   train_loss : 0.0006\n",
            "epoch : 1418   train_loss : 0.0006\n",
            "epoch : 1418   train_loss : 0.0006\n",
            "epoch : 1418   train_loss : 0.0006\n",
            "epoch : 1418   val_loss : 0.0207\n",
            "epoch : 1418   val_loss : 0.0187\n",
            "epoch : 1419   train_loss : 0.0006\n",
            "epoch : 1419   train_loss : 0.0006\n",
            "epoch : 1419   train_loss : 0.0006\n",
            "epoch : 1419   train_loss : 0.0005\n",
            "epoch : 1419   val_loss : 0.0195\n",
            "epoch : 1419   val_loss : 0.0218\n",
            "epoch : 1420   train_loss : 0.0005\n",
            "epoch : 1420   train_loss : 0.0006\n",
            "epoch : 1420   train_loss : 0.0006\n",
            "epoch : 1420   train_loss : 0.0005\n",
            "epoch : 1420   val_loss : 0.0200\n",
            "epoch : 1420   val_loss : 0.0202\n",
            "epoch : 1421   train_loss : 0.0008\n",
            "epoch : 1421   train_loss : 0.0005\n",
            "epoch : 1421   train_loss : 0.0006\n",
            "epoch : 1421   train_loss : 0.0003\n",
            "epoch : 1421   val_loss : 0.0207\n",
            "epoch : 1421   val_loss : 0.0182\n",
            "epoch : 1422   train_loss : 0.0005\n",
            "epoch : 1422   train_loss : 0.0005\n",
            "epoch : 1422   train_loss : 0.0007\n",
            "epoch : 1422   train_loss : 0.0007\n",
            "epoch : 1422   val_loss : 0.0197\n",
            "epoch : 1422   val_loss : 0.0207\n",
            "epoch : 1423   train_loss : 0.0004\n",
            "epoch : 1423   train_loss : 0.0005\n",
            "epoch : 1423   train_loss : 0.0006\n",
            "epoch : 1423   train_loss : 0.0009\n",
            "epoch : 1423   val_loss : 0.0202\n",
            "epoch : 1423   val_loss : 0.0193\n",
            "epoch : 1424   train_loss : 0.0005\n",
            "epoch : 1424   train_loss : 0.0005\n",
            "epoch : 1424   train_loss : 0.0006\n",
            "epoch : 1424   train_loss : 0.0007\n",
            "epoch : 1424   val_loss : 0.0200\n",
            "epoch : 1424   val_loss : 0.0198\n",
            "epoch : 1425   train_loss : 0.0006\n",
            "epoch : 1425   train_loss : 0.0005\n",
            "epoch : 1425   train_loss : 0.0004\n",
            "epoch : 1425   train_loss : 0.0008\n",
            "epoch : 1425   val_loss : 0.0204\n",
            "epoch : 1425   val_loss : 0.0185\n",
            "epoch : 1426   train_loss : 0.0007\n",
            "epoch : 1426   train_loss : 0.0006\n",
            "epoch : 1426   train_loss : 0.0005\n",
            "epoch : 1426   train_loss : 0.0004\n",
            "epoch : 1426   val_loss : 0.0192\n",
            "epoch : 1426   val_loss : 0.0215\n",
            "epoch : 1427   train_loss : 0.0006\n",
            "epoch : 1427   train_loss : 0.0006\n",
            "epoch : 1427   train_loss : 0.0004\n",
            "epoch : 1427   train_loss : 0.0007\n",
            "epoch : 1427   val_loss : 0.0187\n",
            "epoch : 1427   val_loss : 0.0227\n",
            "epoch : 1428   train_loss : 0.0006\n",
            "epoch : 1428   train_loss : 0.0004\n",
            "epoch : 1428   train_loss : 0.0005\n",
            "epoch : 1428   train_loss : 0.0010\n",
            "epoch : 1428   val_loss : 0.0204\n",
            "epoch : 1428   val_loss : 0.0181\n",
            "epoch : 1429   train_loss : 0.0005\n",
            "epoch : 1429   train_loss : 0.0007\n",
            "epoch : 1429   train_loss : 0.0006\n",
            "epoch : 1429   train_loss : 0.0004\n",
            "epoch : 1429   val_loss : 0.0197\n",
            "epoch : 1429   val_loss : 0.0197\n",
            "epoch : 1430   train_loss : 0.0005\n",
            "epoch : 1430   train_loss : 0.0006\n",
            "epoch : 1430   train_loss : 0.0006\n",
            "epoch : 1430   train_loss : 0.0004\n",
            "epoch : 1430   val_loss : 0.0198\n",
            "epoch : 1430   val_loss : 0.0194\n",
            "epoch : 1431   train_loss : 0.0005\n",
            "epoch : 1431   train_loss : 0.0006\n",
            "epoch : 1431   train_loss : 0.0005\n",
            "epoch : 1431   train_loss : 0.0008\n",
            "epoch : 1431   val_loss : 0.0202\n",
            "epoch : 1431   val_loss : 0.0183\n",
            "epoch : 1432   train_loss : 0.0006\n",
            "epoch : 1432   train_loss : 0.0007\n",
            "epoch : 1432   train_loss : 0.0004\n",
            "epoch : 1432   train_loss : 0.0005\n",
            "epoch : 1432   val_loss : 0.0201\n",
            "epoch : 1432   val_loss : 0.0183\n",
            "epoch : 1433   train_loss : 0.0005\n",
            "epoch : 1433   train_loss : 0.0006\n",
            "epoch : 1433   train_loss : 0.0005\n",
            "epoch : 1433   train_loss : 0.0007\n",
            "epoch : 1433   val_loss : 0.0197\n",
            "epoch : 1433   val_loss : 0.0194\n",
            "epoch : 1434   train_loss : 0.0004\n",
            "epoch : 1434   train_loss : 0.0007\n",
            "epoch : 1434   train_loss : 0.0005\n",
            "epoch : 1434   train_loss : 0.0006\n",
            "epoch : 1434   val_loss : 0.0197\n",
            "epoch : 1434   val_loss : 0.0192\n",
            "epoch : 1435   train_loss : 0.0005\n",
            "epoch : 1435   train_loss : 0.0004\n",
            "epoch : 1435   train_loss : 0.0008\n",
            "epoch : 1435   train_loss : 0.0005\n",
            "epoch : 1435   val_loss : 0.0199\n",
            "epoch : 1435   val_loss : 0.0186\n",
            "epoch : 1436   train_loss : 0.0006\n",
            "epoch : 1436   train_loss : 0.0007\n",
            "epoch : 1436   train_loss : 0.0004\n",
            "epoch : 1436   train_loss : 0.0005\n",
            "epoch : 1436   val_loss : 0.0198\n",
            "epoch : 1436   val_loss : 0.0187\n",
            "epoch : 1437   train_loss : 0.0006\n",
            "epoch : 1437   train_loss : 0.0005\n",
            "epoch : 1437   train_loss : 0.0005\n",
            "epoch : 1437   train_loss : 0.0008\n",
            "epoch : 1437   val_loss : 0.0202\n",
            "epoch : 1437   val_loss : 0.0175\n",
            "new model saved at epoch 1437 with val_loss 0.01747369021177292\n",
            "epoch : 1438   train_loss : 0.0005\n",
            "epoch : 1438   train_loss : 0.0006\n",
            "epoch : 1438   train_loss : 0.0006\n",
            "epoch : 1438   train_loss : 0.0005\n",
            "epoch : 1438   val_loss : 0.0182\n",
            "epoch : 1438   val_loss : 0.0225\n",
            "epoch : 1439   train_loss : 0.0006\n",
            "epoch : 1439   train_loss : 0.0004\n",
            "epoch : 1439   train_loss : 0.0007\n",
            "epoch : 1439   train_loss : 0.0005\n",
            "epoch : 1439   val_loss : 0.0193\n",
            "epoch : 1439   val_loss : 0.0196\n",
            "epoch : 1440   train_loss : 0.0006\n",
            "epoch : 1440   train_loss : 0.0005\n",
            "epoch : 1440   train_loss : 0.0006\n",
            "epoch : 1440   train_loss : 0.0006\n",
            "epoch : 1440   val_loss : 0.0192\n",
            "epoch : 1440   val_loss : 0.0197\n",
            "epoch : 1441   train_loss : 0.0005\n",
            "epoch : 1441   train_loss : 0.0006\n",
            "epoch : 1441   train_loss : 0.0006\n",
            "epoch : 1441   train_loss : 0.0006\n",
            "epoch : 1441   val_loss : 0.0200\n",
            "epoch : 1441   val_loss : 0.0174\n",
            "new model saved at epoch 1441 with val_loss 0.01740933023393154\n",
            "epoch : 1442   train_loss : 0.0005\n",
            "epoch : 1442   train_loss : 0.0006\n",
            "epoch : 1442   train_loss : 0.0006\n",
            "epoch : 1442   train_loss : 0.0004\n",
            "epoch : 1442   val_loss : 0.0193\n",
            "epoch : 1442   val_loss : 0.0192\n",
            "epoch : 1443   train_loss : 0.0005\n",
            "epoch : 1443   train_loss : 0.0006\n",
            "epoch : 1443   train_loss : 0.0005\n",
            "epoch : 1443   train_loss : 0.0006\n",
            "epoch : 1443   val_loss : 0.0191\n",
            "epoch : 1443   val_loss : 0.0196\n",
            "epoch : 1444   train_loss : 0.0007\n",
            "epoch : 1444   train_loss : 0.0005\n",
            "epoch : 1444   train_loss : 0.0005\n",
            "epoch : 1444   train_loss : 0.0005\n",
            "epoch : 1444   val_loss : 0.0192\n",
            "epoch : 1444   val_loss : 0.0192\n",
            "epoch : 1445   train_loss : 0.0005\n",
            "epoch : 1445   train_loss : 0.0005\n",
            "epoch : 1445   train_loss : 0.0007\n",
            "epoch : 1445   train_loss : 0.0005\n",
            "epoch : 1445   val_loss : 0.0194\n",
            "epoch : 1445   val_loss : 0.0184\n",
            "epoch : 1446   train_loss : 0.0006\n",
            "epoch : 1446   train_loss : 0.0005\n",
            "epoch : 1446   train_loss : 0.0006\n",
            "epoch : 1446   train_loss : 0.0006\n",
            "epoch : 1446   val_loss : 0.0193\n",
            "epoch : 1446   val_loss : 0.0185\n",
            "epoch : 1447   train_loss : 0.0006\n",
            "epoch : 1447   train_loss : 0.0005\n",
            "epoch : 1447   train_loss : 0.0005\n",
            "epoch : 1447   train_loss : 0.0006\n",
            "epoch : 1447   val_loss : 0.0186\n",
            "epoch : 1447   val_loss : 0.0205\n",
            "epoch : 1448   train_loss : 0.0005\n",
            "epoch : 1448   train_loss : 0.0005\n",
            "epoch : 1448   train_loss : 0.0006\n",
            "epoch : 1448   train_loss : 0.0007\n",
            "epoch : 1448   val_loss : 0.0190\n",
            "epoch : 1448   val_loss : 0.0191\n",
            "epoch : 1449   train_loss : 0.0006\n",
            "epoch : 1449   train_loss : 0.0006\n",
            "epoch : 1449   train_loss : 0.0006\n",
            "epoch : 1449   train_loss : 0.0004\n",
            "epoch : 1449   val_loss : 0.0186\n",
            "epoch : 1449   val_loss : 0.0202\n",
            "epoch : 1450   train_loss : 0.0005\n",
            "epoch : 1450   train_loss : 0.0007\n",
            "epoch : 1450   train_loss : 0.0005\n",
            "epoch : 1450   train_loss : 0.0004\n",
            "epoch : 1450   val_loss : 0.0191\n",
            "epoch : 1450   val_loss : 0.0186\n",
            "epoch : 1451   train_loss : 0.0005\n",
            "epoch : 1451   train_loss : 0.0005\n",
            "epoch : 1451   train_loss : 0.0006\n",
            "epoch : 1451   train_loss : 0.0007\n",
            "epoch : 1451   val_loss : 0.0188\n",
            "epoch : 1451   val_loss : 0.0194\n",
            "epoch : 1452   train_loss : 0.0006\n",
            "epoch : 1452   train_loss : 0.0005\n",
            "epoch : 1452   train_loss : 0.0005\n",
            "epoch : 1452   train_loss : 0.0007\n",
            "epoch : 1452   val_loss : 0.0192\n",
            "epoch : 1452   val_loss : 0.0181\n",
            "epoch : 1453   train_loss : 0.0006\n",
            "epoch : 1453   train_loss : 0.0005\n",
            "epoch : 1453   train_loss : 0.0006\n",
            "epoch : 1453   train_loss : 0.0006\n",
            "epoch : 1453   val_loss : 0.0189\n",
            "epoch : 1453   val_loss : 0.0189\n",
            "epoch : 1454   train_loss : 0.0005\n",
            "epoch : 1454   train_loss : 0.0005\n",
            "epoch : 1454   train_loss : 0.0006\n",
            "epoch : 1454   train_loss : 0.0006\n",
            "epoch : 1454   val_loss : 0.0186\n",
            "epoch : 1454   val_loss : 0.0196\n",
            "epoch : 1455   train_loss : 0.0006\n",
            "epoch : 1455   train_loss : 0.0005\n",
            "epoch : 1455   train_loss : 0.0005\n",
            "epoch : 1455   train_loss : 0.0005\n",
            "epoch : 1455   val_loss : 0.0189\n",
            "epoch : 1455   val_loss : 0.0186\n",
            "epoch : 1456   train_loss : 0.0005\n",
            "epoch : 1456   train_loss : 0.0005\n",
            "epoch : 1456   train_loss : 0.0005\n",
            "epoch : 1456   train_loss : 0.0007\n",
            "epoch : 1456   val_loss : 0.0186\n",
            "epoch : 1456   val_loss : 0.0191\n",
            "epoch : 1457   train_loss : 0.0006\n",
            "epoch : 1457   train_loss : 0.0005\n",
            "epoch : 1457   train_loss : 0.0005\n",
            "epoch : 1457   train_loss : 0.0006\n",
            "epoch : 1457   val_loss : 0.0192\n",
            "epoch : 1457   val_loss : 0.0175\n",
            "epoch : 1458   train_loss : 0.0005\n",
            "epoch : 1458   train_loss : 0.0005\n",
            "epoch : 1458   train_loss : 0.0006\n",
            "epoch : 1458   train_loss : 0.0005\n",
            "epoch : 1458   val_loss : 0.0193\n",
            "epoch : 1458   val_loss : 0.0171\n",
            "new model saved at epoch 1458 with val_loss 0.0171322263777256\n",
            "epoch : 1459   train_loss : 0.0006\n",
            "epoch : 1459   train_loss : 0.0004\n",
            "epoch : 1459   train_loss : 0.0005\n",
            "epoch : 1459   train_loss : 0.0007\n",
            "epoch : 1459   val_loss : 0.0181\n",
            "epoch : 1459   val_loss : 0.0201\n",
            "epoch : 1460   train_loss : 0.0006\n",
            "epoch : 1460   train_loss : 0.0005\n",
            "epoch : 1460   train_loss : 0.0006\n",
            "epoch : 1460   train_loss : 0.0005\n",
            "epoch : 1460   val_loss : 0.0186\n",
            "epoch : 1460   val_loss : 0.0188\n",
            "epoch : 1461   train_loss : 0.0005\n",
            "epoch : 1461   train_loss : 0.0005\n",
            "epoch : 1461   train_loss : 0.0005\n",
            "epoch : 1461   train_loss : 0.0007\n",
            "epoch : 1461   val_loss : 0.0187\n",
            "epoch : 1461   val_loss : 0.0183\n",
            "epoch : 1462   train_loss : 0.0005\n",
            "epoch : 1462   train_loss : 0.0005\n",
            "epoch : 1462   train_loss : 0.0006\n",
            "epoch : 1462   train_loss : 0.0005\n",
            "epoch : 1462   val_loss : 0.0184\n",
            "epoch : 1462   val_loss : 0.0191\n",
            "epoch : 1463   train_loss : 0.0005\n",
            "epoch : 1463   train_loss : 0.0006\n",
            "epoch : 1463   train_loss : 0.0005\n",
            "epoch : 1463   train_loss : 0.0005\n",
            "epoch : 1463   val_loss : 0.0183\n",
            "epoch : 1463   val_loss : 0.0190\n",
            "epoch : 1464   train_loss : 0.0005\n",
            "epoch : 1464   train_loss : 0.0005\n",
            "epoch : 1464   train_loss : 0.0007\n",
            "epoch : 1464   train_loss : 0.0005\n",
            "epoch : 1464   val_loss : 0.0183\n",
            "epoch : 1464   val_loss : 0.0190\n",
            "epoch : 1465   train_loss : 0.0006\n",
            "epoch : 1465   train_loss : 0.0005\n",
            "epoch : 1465   train_loss : 0.0005\n",
            "epoch : 1465   train_loss : 0.0004\n",
            "epoch : 1465   val_loss : 0.0182\n",
            "epoch : 1465   val_loss : 0.0190\n",
            "epoch : 1466   train_loss : 0.0005\n",
            "epoch : 1466   train_loss : 0.0006\n",
            "epoch : 1466   train_loss : 0.0006\n",
            "epoch : 1466   train_loss : 0.0004\n",
            "epoch : 1466   val_loss : 0.0189\n",
            "epoch : 1466   val_loss : 0.0170\n",
            "new model saved at epoch 1466 with val_loss 0.016992175951600075\n",
            "epoch : 1467   train_loss : 0.0006\n",
            "epoch : 1467   train_loss : 0.0005\n",
            "epoch : 1467   train_loss : 0.0006\n",
            "epoch : 1467   train_loss : 0.0005\n",
            "epoch : 1467   val_loss : 0.0184\n",
            "epoch : 1467   val_loss : 0.0184\n",
            "epoch : 1468   train_loss : 0.0004\n",
            "epoch : 1468   train_loss : 0.0006\n",
            "epoch : 1468   train_loss : 0.0007\n",
            "epoch : 1468   train_loss : 0.0005\n",
            "epoch : 1468   val_loss : 0.0183\n",
            "epoch : 1468   val_loss : 0.0185\n",
            "epoch : 1469   train_loss : 0.0005\n",
            "epoch : 1469   train_loss : 0.0006\n",
            "epoch : 1469   train_loss : 0.0006\n",
            "epoch : 1469   train_loss : 0.0004\n",
            "epoch : 1469   val_loss : 0.0180\n",
            "epoch : 1469   val_loss : 0.0191\n",
            "epoch : 1470   train_loss : 0.0006\n",
            "epoch : 1470   train_loss : 0.0006\n",
            "epoch : 1470   train_loss : 0.0005\n",
            "epoch : 1470   train_loss : 0.0005\n",
            "epoch : 1470   val_loss : 0.0185\n",
            "epoch : 1470   val_loss : 0.0177\n",
            "epoch : 1471   train_loss : 0.0005\n",
            "epoch : 1471   train_loss : 0.0004\n",
            "epoch : 1471   train_loss : 0.0006\n",
            "epoch : 1471   train_loss : 0.0006\n",
            "epoch : 1471   val_loss : 0.0184\n",
            "epoch : 1471   val_loss : 0.0180\n",
            "epoch : 1472   train_loss : 0.0006\n",
            "epoch : 1472   train_loss : 0.0005\n",
            "epoch : 1472   train_loss : 0.0005\n",
            "epoch : 1472   train_loss : 0.0005\n",
            "epoch : 1472   val_loss : 0.0186\n",
            "epoch : 1472   val_loss : 0.0173\n",
            "epoch : 1473   train_loss : 0.0005\n",
            "epoch : 1473   train_loss : 0.0005\n",
            "epoch : 1473   train_loss : 0.0007\n",
            "epoch : 1473   train_loss : 0.0004\n",
            "epoch : 1473   val_loss : 0.0177\n",
            "epoch : 1473   val_loss : 0.0194\n",
            "epoch : 1474   train_loss : 0.0006\n",
            "epoch : 1474   train_loss : 0.0006\n",
            "epoch : 1474   train_loss : 0.0004\n",
            "epoch : 1474   train_loss : 0.0005\n",
            "epoch : 1474   val_loss : 0.0179\n",
            "epoch : 1474   val_loss : 0.0187\n",
            "epoch : 1475   train_loss : 0.0005\n",
            "epoch : 1475   train_loss : 0.0005\n",
            "epoch : 1475   train_loss : 0.0005\n",
            "epoch : 1475   train_loss : 0.0007\n",
            "epoch : 1475   val_loss : 0.0185\n",
            "epoch : 1475   val_loss : 0.0172\n",
            "epoch : 1476   train_loss : 0.0005\n",
            "epoch : 1476   train_loss : 0.0006\n",
            "epoch : 1476   train_loss : 0.0006\n",
            "epoch : 1476   train_loss : 0.0004\n",
            "epoch : 1476   val_loss : 0.0187\n",
            "epoch : 1476   val_loss : 0.0163\n",
            "new model saved at epoch 1476 with val_loss 0.016337791457772255\n",
            "epoch : 1477   train_loss : 0.0004\n",
            "epoch : 1477   train_loss : 0.0007\n",
            "epoch : 1477   train_loss : 0.0005\n",
            "epoch : 1477   train_loss : 0.0005\n",
            "epoch : 1477   val_loss : 0.0180\n",
            "epoch : 1477   val_loss : 0.0180\n",
            "epoch : 1478   train_loss : 0.0005\n",
            "epoch : 1478   train_loss : 0.0005\n",
            "epoch : 1478   train_loss : 0.0006\n",
            "epoch : 1478   train_loss : 0.0005\n",
            "epoch : 1478   val_loss : 0.0182\n",
            "epoch : 1478   val_loss : 0.0175\n",
            "epoch : 1479   train_loss : 0.0006\n",
            "epoch : 1479   train_loss : 0.0005\n",
            "epoch : 1479   train_loss : 0.0005\n",
            "epoch : 1479   train_loss : 0.0005\n",
            "epoch : 1479   val_loss : 0.0180\n",
            "epoch : 1479   val_loss : 0.0179\n",
            "epoch : 1480   train_loss : 0.0006\n",
            "epoch : 1480   train_loss : 0.0005\n",
            "epoch : 1480   train_loss : 0.0004\n",
            "epoch : 1480   train_loss : 0.0006\n",
            "epoch : 1480   val_loss : 0.0180\n",
            "epoch : 1480   val_loss : 0.0177\n",
            "epoch : 1481   train_loss : 0.0006\n",
            "epoch : 1481   train_loss : 0.0005\n",
            "epoch : 1481   train_loss : 0.0005\n",
            "epoch : 1481   train_loss : 0.0006\n",
            "epoch : 1481   val_loss : 0.0185\n",
            "epoch : 1481   val_loss : 0.0162\n",
            "new model saved at epoch 1481 with val_loss 0.01619112491607666\n",
            "epoch : 1482   train_loss : 0.0006\n",
            "epoch : 1482   train_loss : 0.0005\n",
            "epoch : 1482   train_loss : 0.0004\n",
            "epoch : 1482   train_loss : 0.0007\n",
            "epoch : 1482   val_loss : 0.0171\n",
            "epoch : 1482   val_loss : 0.0199\n",
            "epoch : 1483   train_loss : 0.0004\n",
            "epoch : 1483   train_loss : 0.0008\n",
            "epoch : 1483   train_loss : 0.0004\n",
            "epoch : 1483   train_loss : 0.0005\n",
            "epoch : 1483   val_loss : 0.0178\n",
            "epoch : 1483   val_loss : 0.0179\n",
            "epoch : 1484   train_loss : 0.0005\n",
            "epoch : 1484   train_loss : 0.0007\n",
            "epoch : 1484   train_loss : 0.0004\n",
            "epoch : 1484   train_loss : 0.0005\n",
            "epoch : 1484   val_loss : 0.0182\n",
            "epoch : 1484   val_loss : 0.0166\n",
            "epoch : 1485   train_loss : 0.0006\n",
            "epoch : 1485   train_loss : 0.0005\n",
            "epoch : 1485   train_loss : 0.0005\n",
            "epoch : 1485   train_loss : 0.0005\n",
            "epoch : 1485   val_loss : 0.0179\n",
            "epoch : 1485   val_loss : 0.0174\n",
            "epoch : 1486   train_loss : 0.0005\n",
            "epoch : 1486   train_loss : 0.0005\n",
            "epoch : 1486   train_loss : 0.0005\n",
            "epoch : 1486   train_loss : 0.0004\n",
            "epoch : 1486   val_loss : 0.0171\n",
            "epoch : 1486   val_loss : 0.0193\n",
            "epoch : 1487   train_loss : 0.0005\n",
            "epoch : 1487   train_loss : 0.0006\n",
            "epoch : 1487   train_loss : 0.0005\n",
            "epoch : 1487   train_loss : 0.0005\n",
            "epoch : 1487   val_loss : 0.0177\n",
            "epoch : 1487   val_loss : 0.0177\n",
            "epoch : 1488   train_loss : 0.0006\n",
            "epoch : 1488   train_loss : 0.0005\n",
            "epoch : 1488   train_loss : 0.0005\n",
            "epoch : 1488   train_loss : 0.0004\n",
            "epoch : 1488   val_loss : 0.0174\n",
            "epoch : 1488   val_loss : 0.0182\n",
            "epoch : 1489   train_loss : 0.0004\n",
            "epoch : 1489   train_loss : 0.0005\n",
            "epoch : 1489   train_loss : 0.0005\n",
            "epoch : 1489   train_loss : 0.0006\n",
            "epoch : 1489   val_loss : 0.0172\n",
            "epoch : 1489   val_loss : 0.0186\n",
            "epoch : 1490   train_loss : 0.0005\n",
            "epoch : 1490   train_loss : 0.0006\n",
            "epoch : 1490   train_loss : 0.0005\n",
            "epoch : 1490   train_loss : 0.0005\n",
            "epoch : 1490   val_loss : 0.0174\n",
            "epoch : 1490   val_loss : 0.0181\n",
            "epoch : 1491   train_loss : 0.0005\n",
            "epoch : 1491   train_loss : 0.0005\n",
            "epoch : 1491   train_loss : 0.0005\n",
            "epoch : 1491   train_loss : 0.0005\n",
            "epoch : 1491   val_loss : 0.0172\n",
            "epoch : 1491   val_loss : 0.0184\n",
            "epoch : 1492   train_loss : 0.0006\n",
            "epoch : 1492   train_loss : 0.0004\n",
            "epoch : 1492   train_loss : 0.0005\n",
            "epoch : 1492   train_loss : 0.0005\n",
            "epoch : 1492   val_loss : 0.0177\n",
            "epoch : 1492   val_loss : 0.0169\n",
            "epoch : 1493   train_loss : 0.0005\n",
            "epoch : 1493   train_loss : 0.0005\n",
            "epoch : 1493   train_loss : 0.0005\n",
            "epoch : 1493   train_loss : 0.0005\n",
            "epoch : 1493   val_loss : 0.0183\n",
            "epoch : 1493   val_loss : 0.0151\n",
            "new model saved at epoch 1493 with val_loss 0.01506790705025196\n",
            "epoch : 1494   train_loss : 0.0005\n",
            "epoch : 1494   train_loss : 0.0006\n",
            "epoch : 1494   train_loss : 0.0004\n",
            "epoch : 1494   train_loss : 0.0005\n",
            "epoch : 1494   val_loss : 0.0174\n",
            "epoch : 1494   val_loss : 0.0175\n",
            "epoch : 1495   train_loss : 0.0005\n",
            "epoch : 1495   train_loss : 0.0005\n",
            "epoch : 1495   train_loss : 0.0005\n",
            "epoch : 1495   train_loss : 0.0005\n",
            "epoch : 1495   val_loss : 0.0181\n",
            "epoch : 1495   val_loss : 0.0154\n",
            "epoch : 1496   train_loss : 0.0005\n",
            "epoch : 1496   train_loss : 0.0006\n",
            "epoch : 1496   train_loss : 0.0006\n",
            "epoch : 1496   train_loss : 0.0004\n",
            "epoch : 1496   val_loss : 0.0173\n",
            "epoch : 1496   val_loss : 0.0176\n",
            "epoch : 1497   train_loss : 0.0006\n",
            "epoch : 1497   train_loss : 0.0005\n",
            "epoch : 1497   train_loss : 0.0005\n",
            "epoch : 1497   train_loss : 0.0003\n",
            "epoch : 1497   val_loss : 0.0168\n",
            "epoch : 1497   val_loss : 0.0187\n",
            "epoch : 1498   train_loss : 0.0005\n",
            "epoch : 1498   train_loss : 0.0004\n",
            "epoch : 1498   train_loss : 0.0005\n",
            "epoch : 1498   train_loss : 0.0006\n",
            "epoch : 1498   val_loss : 0.0173\n",
            "epoch : 1498   val_loss : 0.0173\n",
            "epoch : 1499   train_loss : 0.0005\n",
            "epoch : 1499   train_loss : 0.0004\n",
            "epoch : 1499   train_loss : 0.0005\n",
            "epoch : 1499   train_loss : 0.0006\n",
            "epoch : 1499   val_loss : 0.0178\n",
            "epoch : 1499   val_loss : 0.0159\n",
            "epoch : 1500   train_loss : 0.0007\n",
            "epoch : 1500   train_loss : 0.0004\n",
            "epoch : 1500   train_loss : 0.0004\n",
            "epoch : 1500   train_loss : 0.0005\n",
            "epoch : 1500   val_loss : 0.0171\n",
            "epoch : 1500   val_loss : 0.0175\n",
            "epoch : 1501   train_loss : 0.0004\n",
            "epoch : 1501   train_loss : 0.0005\n",
            "epoch : 1501   train_loss : 0.0005\n",
            "epoch : 1501   train_loss : 0.0006\n",
            "epoch : 1501   val_loss : 0.0174\n",
            "epoch : 1501   val_loss : 0.0165\n",
            "epoch : 1502   train_loss : 0.0006\n",
            "epoch : 1502   train_loss : 0.0004\n",
            "epoch : 1502   train_loss : 0.0005\n",
            "epoch : 1502   train_loss : 0.0005\n",
            "epoch : 1502   val_loss : 0.0169\n",
            "epoch : 1502   val_loss : 0.0177\n",
            "epoch : 1503   train_loss : 0.0004\n",
            "epoch : 1503   train_loss : 0.0005\n",
            "epoch : 1503   train_loss : 0.0005\n",
            "epoch : 1503   train_loss : 0.0007\n",
            "epoch : 1503   val_loss : 0.0174\n",
            "epoch : 1503   val_loss : 0.0164\n",
            "epoch : 1504   train_loss : 0.0006\n",
            "epoch : 1504   train_loss : 0.0006\n",
            "epoch : 1504   train_loss : 0.0005\n",
            "epoch : 1504   train_loss : 0.0003\n",
            "epoch : 1504   val_loss : 0.0173\n",
            "epoch : 1504   val_loss : 0.0164\n",
            "epoch : 1505   train_loss : 0.0004\n",
            "epoch : 1505   train_loss : 0.0006\n",
            "epoch : 1505   train_loss : 0.0005\n",
            "epoch : 1505   train_loss : 0.0004\n",
            "epoch : 1505   val_loss : 0.0172\n",
            "epoch : 1505   val_loss : 0.0168\n",
            "epoch : 1506   train_loss : 0.0006\n",
            "epoch : 1506   train_loss : 0.0005\n",
            "epoch : 1506   train_loss : 0.0005\n",
            "epoch : 1506   train_loss : 0.0004\n",
            "epoch : 1506   val_loss : 0.0175\n",
            "epoch : 1506   val_loss : 0.0157\n",
            "epoch : 1507   train_loss : 0.0007\n",
            "epoch : 1507   train_loss : 0.0004\n",
            "epoch : 1507   train_loss : 0.0004\n",
            "epoch : 1507   train_loss : 0.0004\n",
            "epoch : 1507   val_loss : 0.0162\n",
            "epoch : 1507   val_loss : 0.0191\n",
            "epoch : 1508   train_loss : 0.0007\n",
            "epoch : 1508   train_loss : 0.0004\n",
            "epoch : 1508   train_loss : 0.0004\n",
            "epoch : 1508   train_loss : 0.0005\n",
            "epoch : 1508   val_loss : 0.0176\n",
            "epoch : 1508   val_loss : 0.0152\n",
            "epoch : 1509   train_loss : 0.0005\n",
            "epoch : 1509   train_loss : 0.0005\n",
            "epoch : 1509   train_loss : 0.0005\n",
            "epoch : 1509   train_loss : 0.0004\n",
            "epoch : 1509   val_loss : 0.0171\n",
            "epoch : 1509   val_loss : 0.0164\n",
            "epoch : 1510   train_loss : 0.0004\n",
            "epoch : 1510   train_loss : 0.0005\n",
            "epoch : 1510   train_loss : 0.0005\n",
            "epoch : 1510   train_loss : 0.0006\n",
            "epoch : 1510   val_loss : 0.0166\n",
            "epoch : 1510   val_loss : 0.0177\n",
            "epoch : 1511   train_loss : 0.0004\n",
            "epoch : 1511   train_loss : 0.0006\n",
            "epoch : 1511   train_loss : 0.0004\n",
            "epoch : 1511   train_loss : 0.0006\n",
            "epoch : 1511   val_loss : 0.0165\n",
            "epoch : 1511   val_loss : 0.0177\n",
            "epoch : 1512   train_loss : 0.0006\n",
            "epoch : 1512   train_loss : 0.0004\n",
            "epoch : 1512   train_loss : 0.0005\n",
            "epoch : 1512   train_loss : 0.0004\n",
            "epoch : 1512   val_loss : 0.0171\n",
            "epoch : 1512   val_loss : 0.0161\n",
            "epoch : 1513   train_loss : 0.0004\n",
            "epoch : 1513   train_loss : 0.0005\n",
            "epoch : 1513   train_loss : 0.0006\n",
            "epoch : 1513   train_loss : 0.0005\n",
            "epoch : 1513   val_loss : 0.0168\n",
            "epoch : 1513   val_loss : 0.0168\n",
            "epoch : 1514   train_loss : 0.0005\n",
            "epoch : 1514   train_loss : 0.0004\n",
            "epoch : 1514   train_loss : 0.0007\n",
            "epoch : 1514   train_loss : 0.0004\n",
            "epoch : 1514   val_loss : 0.0169\n",
            "epoch : 1514   val_loss : 0.0165\n",
            "epoch : 1515   train_loss : 0.0005\n",
            "epoch : 1515   train_loss : 0.0004\n",
            "epoch : 1515   train_loss : 0.0005\n",
            "epoch : 1515   train_loss : 0.0007\n",
            "epoch : 1515   val_loss : 0.0166\n",
            "epoch : 1515   val_loss : 0.0169\n",
            "epoch : 1516   train_loss : 0.0005\n",
            "epoch : 1516   train_loss : 0.0004\n",
            "epoch : 1516   train_loss : 0.0006\n",
            "epoch : 1516   train_loss : 0.0004\n",
            "epoch : 1516   val_loss : 0.0171\n",
            "epoch : 1516   val_loss : 0.0156\n",
            "epoch : 1517   train_loss : 0.0006\n",
            "epoch : 1517   train_loss : 0.0004\n",
            "epoch : 1517   train_loss : 0.0004\n",
            "epoch : 1517   train_loss : 0.0007\n",
            "epoch : 1517   val_loss : 0.0170\n",
            "epoch : 1517   val_loss : 0.0158\n",
            "epoch : 1518   train_loss : 0.0004\n",
            "epoch : 1518   train_loss : 0.0005\n",
            "epoch : 1518   train_loss : 0.0004\n",
            "epoch : 1518   train_loss : 0.0006\n",
            "epoch : 1518   val_loss : 0.0173\n",
            "epoch : 1518   val_loss : 0.0148\n",
            "new model saved at epoch 1518 with val_loss 0.014799700118601322\n",
            "epoch : 1519   train_loss : 0.0006\n",
            "epoch : 1519   train_loss : 0.0004\n",
            "epoch : 1519   train_loss : 0.0005\n",
            "epoch : 1519   train_loss : 0.0004\n",
            "epoch : 1519   val_loss : 0.0167\n",
            "epoch : 1519   val_loss : 0.0165\n",
            "epoch : 1520   train_loss : 0.0005\n",
            "epoch : 1520   train_loss : 0.0005\n",
            "epoch : 1520   train_loss : 0.0005\n",
            "epoch : 1520   train_loss : 0.0004\n",
            "epoch : 1520   val_loss : 0.0167\n",
            "epoch : 1520   val_loss : 0.0161\n",
            "epoch : 1521   train_loss : 0.0004\n",
            "epoch : 1521   train_loss : 0.0005\n",
            "epoch : 1521   train_loss : 0.0006\n",
            "epoch : 1521   train_loss : 0.0004\n",
            "epoch : 1521   val_loss : 0.0170\n",
            "epoch : 1521   val_loss : 0.0154\n",
            "epoch : 1522   train_loss : 0.0005\n",
            "epoch : 1522   train_loss : 0.0004\n",
            "epoch : 1522   train_loss : 0.0005\n",
            "epoch : 1522   train_loss : 0.0006\n",
            "epoch : 1522   val_loss : 0.0167\n",
            "epoch : 1522   val_loss : 0.0159\n",
            "epoch : 1523   train_loss : 0.0004\n",
            "epoch : 1523   train_loss : 0.0006\n",
            "epoch : 1523   train_loss : 0.0005\n",
            "epoch : 1523   train_loss : 0.0004\n",
            "epoch : 1523   val_loss : 0.0166\n",
            "epoch : 1523   val_loss : 0.0160\n",
            "epoch : 1524   train_loss : 0.0007\n",
            "epoch : 1524   train_loss : 0.0004\n",
            "epoch : 1524   train_loss : 0.0004\n",
            "epoch : 1524   train_loss : 0.0004\n",
            "epoch : 1524   val_loss : 0.0167\n",
            "epoch : 1524   val_loss : 0.0157\n",
            "epoch : 1525   train_loss : 0.0005\n",
            "epoch : 1525   train_loss : 0.0004\n",
            "epoch : 1525   train_loss : 0.0005\n",
            "epoch : 1525   train_loss : 0.0007\n",
            "epoch : 1525   val_loss : 0.0166\n",
            "epoch : 1525   val_loss : 0.0160\n",
            "epoch : 1526   train_loss : 0.0005\n",
            "epoch : 1526   train_loss : 0.0004\n",
            "epoch : 1526   train_loss : 0.0006\n",
            "epoch : 1526   train_loss : 0.0004\n",
            "epoch : 1526   val_loss : 0.0166\n",
            "epoch : 1526   val_loss : 0.0157\n",
            "epoch : 1527   train_loss : 0.0005\n",
            "epoch : 1527   train_loss : 0.0004\n",
            "epoch : 1527   train_loss : 0.0005\n",
            "epoch : 1527   train_loss : 0.0006\n",
            "epoch : 1527   val_loss : 0.0166\n",
            "epoch : 1527   val_loss : 0.0156\n",
            "epoch : 1528   train_loss : 0.0005\n",
            "epoch : 1528   train_loss : 0.0004\n",
            "epoch : 1528   train_loss : 0.0005\n",
            "epoch : 1528   train_loss : 0.0004\n",
            "epoch : 1528   val_loss : 0.0165\n",
            "epoch : 1528   val_loss : 0.0158\n",
            "epoch : 1529   train_loss : 0.0006\n",
            "epoch : 1529   train_loss : 0.0004\n",
            "epoch : 1529   train_loss : 0.0005\n",
            "epoch : 1529   train_loss : 0.0005\n",
            "epoch : 1529   val_loss : 0.0168\n",
            "epoch : 1529   val_loss : 0.0149\n",
            "epoch : 1530   train_loss : 0.0004\n",
            "epoch : 1530   train_loss : 0.0006\n",
            "epoch : 1530   train_loss : 0.0005\n",
            "epoch : 1530   train_loss : 0.0004\n",
            "epoch : 1530   val_loss : 0.0162\n",
            "epoch : 1530   val_loss : 0.0164\n",
            "epoch : 1531   train_loss : 0.0004\n",
            "epoch : 1531   train_loss : 0.0005\n",
            "epoch : 1531   train_loss : 0.0005\n",
            "epoch : 1531   train_loss : 0.0004\n",
            "epoch : 1531   val_loss : 0.0160\n",
            "epoch : 1531   val_loss : 0.0167\n",
            "epoch : 1532   train_loss : 0.0004\n",
            "epoch : 1532   train_loss : 0.0006\n",
            "epoch : 1532   train_loss : 0.0005\n",
            "epoch : 1532   train_loss : 0.0003\n",
            "epoch : 1532   val_loss : 0.0164\n",
            "epoch : 1532   val_loss : 0.0156\n",
            "epoch : 1533   train_loss : 0.0007\n",
            "epoch : 1533   train_loss : 0.0004\n",
            "epoch : 1533   train_loss : 0.0004\n",
            "epoch : 1533   train_loss : 0.0004\n",
            "epoch : 1533   val_loss : 0.0164\n",
            "epoch : 1533   val_loss : 0.0155\n",
            "epoch : 1534   train_loss : 0.0004\n",
            "epoch : 1534   train_loss : 0.0005\n",
            "epoch : 1534   train_loss : 0.0004\n",
            "epoch : 1534   train_loss : 0.0006\n",
            "epoch : 1534   val_loss : 0.0169\n",
            "epoch : 1534   val_loss : 0.0139\n",
            "new model saved at epoch 1534 with val_loss 0.01390256267040968\n",
            "epoch : 1535   train_loss : 0.0005\n",
            "epoch : 1535   train_loss : 0.0006\n",
            "epoch : 1535   train_loss : 0.0005\n",
            "epoch : 1535   train_loss : 0.0003\n",
            "epoch : 1535   val_loss : 0.0160\n",
            "epoch : 1535   val_loss : 0.0161\n",
            "epoch : 1536   train_loss : 0.0005\n",
            "epoch : 1536   train_loss : 0.0005\n",
            "epoch : 1536   train_loss : 0.0004\n",
            "epoch : 1536   train_loss : 0.0004\n",
            "epoch : 1536   val_loss : 0.0164\n",
            "epoch : 1536   val_loss : 0.0152\n",
            "epoch : 1537   train_loss : 0.0005\n",
            "epoch : 1537   train_loss : 0.0004\n",
            "epoch : 1537   train_loss : 0.0005\n",
            "epoch : 1537   train_loss : 0.0006\n",
            "epoch : 1537   val_loss : 0.0165\n",
            "epoch : 1537   val_loss : 0.0147\n",
            "epoch : 1538   train_loss : 0.0005\n",
            "epoch : 1538   train_loss : 0.0005\n",
            "epoch : 1538   train_loss : 0.0004\n",
            "epoch : 1538   train_loss : 0.0006\n",
            "epoch : 1538   val_loss : 0.0162\n",
            "epoch : 1538   val_loss : 0.0154\n",
            "epoch : 1539   train_loss : 0.0005\n",
            "epoch : 1539   train_loss : 0.0005\n",
            "epoch : 1539   train_loss : 0.0004\n",
            "epoch : 1539   train_loss : 0.0006\n",
            "epoch : 1539   val_loss : 0.0158\n",
            "epoch : 1539   val_loss : 0.0163\n",
            "epoch : 1540   train_loss : 0.0004\n",
            "epoch : 1540   train_loss : 0.0006\n",
            "epoch : 1540   train_loss : 0.0004\n",
            "epoch : 1540   train_loss : 0.0005\n",
            "epoch : 1540   val_loss : 0.0163\n",
            "epoch : 1540   val_loss : 0.0148\n",
            "epoch : 1541   train_loss : 0.0004\n",
            "epoch : 1541   train_loss : 0.0004\n",
            "epoch : 1541   train_loss : 0.0006\n",
            "epoch : 1541   train_loss : 0.0004\n",
            "epoch : 1541   val_loss : 0.0155\n",
            "epoch : 1541   val_loss : 0.0170\n",
            "epoch : 1542   train_loss : 0.0005\n",
            "epoch : 1542   train_loss : 0.0004\n",
            "epoch : 1542   train_loss : 0.0004\n",
            "epoch : 1542   train_loss : 0.0005\n",
            "epoch : 1542   val_loss : 0.0159\n",
            "epoch : 1542   val_loss : 0.0157\n",
            "epoch : 1543   train_loss : 0.0005\n",
            "epoch : 1543   train_loss : 0.0005\n",
            "epoch : 1543   train_loss : 0.0005\n",
            "epoch : 1543   train_loss : 0.0004\n",
            "epoch : 1543   val_loss : 0.0155\n",
            "epoch : 1543   val_loss : 0.0168\n",
            "epoch : 1544   train_loss : 0.0004\n",
            "epoch : 1544   train_loss : 0.0005\n",
            "epoch : 1544   train_loss : 0.0005\n",
            "epoch : 1544   train_loss : 0.0004\n",
            "epoch : 1544   val_loss : 0.0156\n",
            "epoch : 1544   val_loss : 0.0163\n",
            "epoch : 1545   train_loss : 0.0004\n",
            "epoch : 1545   train_loss : 0.0004\n",
            "epoch : 1545   train_loss : 0.0006\n",
            "epoch : 1545   train_loss : 0.0005\n",
            "epoch : 1545   val_loss : 0.0158\n",
            "epoch : 1545   val_loss : 0.0157\n",
            "epoch : 1546   train_loss : 0.0004\n",
            "epoch : 1546   train_loss : 0.0004\n",
            "epoch : 1546   train_loss : 0.0005\n",
            "epoch : 1546   train_loss : 0.0007\n",
            "epoch : 1546   val_loss : 0.0154\n",
            "epoch : 1546   val_loss : 0.0166\n",
            "epoch : 1547   train_loss : 0.0004\n",
            "epoch : 1547   train_loss : 0.0005\n",
            "epoch : 1547   train_loss : 0.0005\n",
            "epoch : 1547   train_loss : 0.0004\n",
            "epoch : 1547   val_loss : 0.0152\n",
            "epoch : 1547   val_loss : 0.0169\n",
            "epoch : 1548   train_loss : 0.0005\n",
            "epoch : 1548   train_loss : 0.0005\n",
            "epoch : 1548   train_loss : 0.0005\n",
            "epoch : 1548   train_loss : 0.0004\n",
            "epoch : 1548   val_loss : 0.0155\n",
            "epoch : 1548   val_loss : 0.0160\n",
            "epoch : 1549   train_loss : 0.0005\n",
            "epoch : 1549   train_loss : 0.0005\n",
            "epoch : 1549   train_loss : 0.0004\n",
            "epoch : 1549   train_loss : 0.0005\n",
            "epoch : 1549   val_loss : 0.0155\n",
            "epoch : 1549   val_loss : 0.0159\n",
            "epoch : 1550   train_loss : 0.0006\n",
            "epoch : 1550   train_loss : 0.0005\n",
            "epoch : 1550   train_loss : 0.0004\n",
            "epoch : 1550   train_loss : 0.0004\n",
            "epoch : 1550   val_loss : 0.0157\n",
            "epoch : 1550   val_loss : 0.0152\n",
            "epoch : 1551   train_loss : 0.0004\n",
            "epoch : 1551   train_loss : 0.0005\n",
            "epoch : 1551   train_loss : 0.0004\n",
            "epoch : 1551   train_loss : 0.0006\n",
            "epoch : 1551   val_loss : 0.0155\n",
            "epoch : 1551   val_loss : 0.0158\n",
            "epoch : 1552   train_loss : 0.0005\n",
            "epoch : 1552   train_loss : 0.0004\n",
            "epoch : 1552   train_loss : 0.0005\n",
            "epoch : 1552   train_loss : 0.0004\n",
            "epoch : 1552   val_loss : 0.0159\n",
            "epoch : 1552   val_loss : 0.0145\n",
            "epoch : 1553   train_loss : 0.0004\n",
            "epoch : 1553   train_loss : 0.0005\n",
            "epoch : 1553   train_loss : 0.0005\n",
            "epoch : 1553   train_loss : 0.0004\n",
            "epoch : 1553   val_loss : 0.0151\n",
            "epoch : 1553   val_loss : 0.0165\n",
            "epoch : 1554   train_loss : 0.0005\n",
            "epoch : 1554   train_loss : 0.0003\n",
            "epoch : 1554   train_loss : 0.0006\n",
            "epoch : 1554   train_loss : 0.0004\n",
            "epoch : 1554   val_loss : 0.0152\n",
            "epoch : 1554   val_loss : 0.0161\n",
            "epoch : 1555   train_loss : 0.0006\n",
            "epoch : 1555   train_loss : 0.0003\n",
            "epoch : 1555   train_loss : 0.0004\n",
            "epoch : 1555   train_loss : 0.0006\n",
            "epoch : 1555   val_loss : 0.0150\n",
            "epoch : 1555   val_loss : 0.0165\n",
            "epoch : 1556   train_loss : 0.0004\n",
            "epoch : 1556   train_loss : 0.0006\n",
            "epoch : 1556   train_loss : 0.0005\n",
            "epoch : 1556   train_loss : 0.0004\n",
            "epoch : 1556   val_loss : 0.0157\n",
            "epoch : 1556   val_loss : 0.0147\n",
            "epoch : 1557   train_loss : 0.0005\n",
            "epoch : 1557   train_loss : 0.0004\n",
            "epoch : 1557   train_loss : 0.0005\n",
            "epoch : 1557   train_loss : 0.0005\n",
            "epoch : 1557   val_loss : 0.0160\n",
            "epoch : 1557   val_loss : 0.0138\n",
            "new model saved at epoch 1557 with val_loss 0.01376462634652853\n",
            "epoch : 1558   train_loss : 0.0004\n",
            "epoch : 1558   train_loss : 0.0005\n",
            "epoch : 1558   train_loss : 0.0004\n",
            "epoch : 1558   train_loss : 0.0005\n",
            "epoch : 1558   val_loss : 0.0156\n",
            "epoch : 1558   val_loss : 0.0146\n",
            "epoch : 1559   train_loss : 0.0005\n",
            "epoch : 1559   train_loss : 0.0005\n",
            "epoch : 1559   train_loss : 0.0005\n",
            "epoch : 1559   train_loss : 0.0003\n",
            "epoch : 1559   val_loss : 0.0155\n",
            "epoch : 1559   val_loss : 0.0148\n",
            "epoch : 1560   train_loss : 0.0004\n",
            "epoch : 1560   train_loss : 0.0006\n",
            "epoch : 1560   train_loss : 0.0004\n",
            "epoch : 1560   train_loss : 0.0004\n",
            "epoch : 1560   val_loss : 0.0152\n",
            "epoch : 1560   val_loss : 0.0156\n",
            "epoch : 1561   train_loss : 0.0004\n",
            "epoch : 1561   train_loss : 0.0004\n",
            "epoch : 1561   train_loss : 0.0005\n",
            "epoch : 1561   train_loss : 0.0004\n",
            "epoch : 1561   val_loss : 0.0156\n",
            "epoch : 1561   val_loss : 0.0144\n",
            "epoch : 1562   train_loss : 0.0003\n",
            "epoch : 1562   train_loss : 0.0005\n",
            "epoch : 1562   train_loss : 0.0005\n",
            "epoch : 1562   train_loss : 0.0005\n",
            "epoch : 1562   val_loss : 0.0150\n",
            "epoch : 1562   val_loss : 0.0159\n",
            "epoch : 1563   train_loss : 0.0005\n",
            "epoch : 1563   train_loss : 0.0004\n",
            "epoch : 1563   train_loss : 0.0004\n",
            "epoch : 1563   train_loss : 0.0006\n",
            "epoch : 1563   val_loss : 0.0149\n",
            "epoch : 1563   val_loss : 0.0161\n",
            "epoch : 1564   train_loss : 0.0005\n",
            "epoch : 1564   train_loss : 0.0005\n",
            "epoch : 1564   train_loss : 0.0004\n",
            "epoch : 1564   train_loss : 0.0004\n",
            "epoch : 1564   val_loss : 0.0153\n",
            "epoch : 1564   val_loss : 0.0149\n",
            "epoch : 1565   train_loss : 0.0004\n",
            "epoch : 1565   train_loss : 0.0004\n",
            "epoch : 1565   train_loss : 0.0006\n",
            "epoch : 1565   train_loss : 0.0004\n",
            "epoch : 1565   val_loss : 0.0149\n",
            "epoch : 1565   val_loss : 0.0158\n",
            "epoch : 1566   train_loss : 0.0005\n",
            "epoch : 1566   train_loss : 0.0005\n",
            "epoch : 1566   train_loss : 0.0004\n",
            "epoch : 1566   train_loss : 0.0004\n",
            "epoch : 1566   val_loss : 0.0155\n",
            "epoch : 1566   val_loss : 0.0141\n",
            "epoch : 1567   train_loss : 0.0005\n",
            "epoch : 1567   train_loss : 0.0006\n",
            "epoch : 1567   train_loss : 0.0004\n",
            "epoch : 1567   train_loss : 0.0004\n",
            "epoch : 1567   val_loss : 0.0149\n",
            "epoch : 1567   val_loss : 0.0154\n",
            "epoch : 1568   train_loss : 0.0004\n",
            "epoch : 1568   train_loss : 0.0005\n",
            "epoch : 1568   train_loss : 0.0005\n",
            "epoch : 1568   train_loss : 0.0003\n",
            "epoch : 1568   val_loss : 0.0155\n",
            "epoch : 1568   val_loss : 0.0139\n",
            "epoch : 1569   train_loss : 0.0006\n",
            "epoch : 1569   train_loss : 0.0004\n",
            "epoch : 1569   train_loss : 0.0004\n",
            "epoch : 1569   train_loss : 0.0004\n",
            "epoch : 1569   val_loss : 0.0150\n",
            "epoch : 1569   val_loss : 0.0150\n",
            "epoch : 1570   train_loss : 0.0003\n",
            "epoch : 1570   train_loss : 0.0005\n",
            "epoch : 1570   train_loss : 0.0005\n",
            "epoch : 1570   train_loss : 0.0006\n",
            "epoch : 1570   val_loss : 0.0154\n",
            "epoch : 1570   val_loss : 0.0138\n",
            "epoch : 1571   train_loss : 0.0004\n",
            "epoch : 1571   train_loss : 0.0005\n",
            "epoch : 1571   train_loss : 0.0004\n",
            "epoch : 1571   train_loss : 0.0004\n",
            "epoch : 1571   val_loss : 0.0153\n",
            "epoch : 1571   val_loss : 0.0142\n",
            "epoch : 1572   train_loss : 0.0004\n",
            "epoch : 1572   train_loss : 0.0005\n",
            "epoch : 1572   train_loss : 0.0004\n",
            "epoch : 1572   train_loss : 0.0005\n",
            "epoch : 1572   val_loss : 0.0149\n",
            "epoch : 1572   val_loss : 0.0151\n",
            "epoch : 1573   train_loss : 0.0005\n",
            "epoch : 1573   train_loss : 0.0004\n",
            "epoch : 1573   train_loss : 0.0004\n",
            "epoch : 1573   train_loss : 0.0005\n",
            "epoch : 1573   val_loss : 0.0150\n",
            "epoch : 1573   val_loss : 0.0146\n",
            "epoch : 1574   train_loss : 0.0004\n",
            "epoch : 1574   train_loss : 0.0005\n",
            "epoch : 1574   train_loss : 0.0005\n",
            "epoch : 1574   train_loss : 0.0005\n",
            "epoch : 1574   val_loss : 0.0152\n",
            "epoch : 1574   val_loss : 0.0141\n",
            "epoch : 1575   train_loss : 0.0004\n",
            "epoch : 1575   train_loss : 0.0004\n",
            "epoch : 1575   train_loss : 0.0006\n",
            "epoch : 1575   train_loss : 0.0004\n",
            "epoch : 1575   val_loss : 0.0149\n",
            "epoch : 1575   val_loss : 0.0148\n",
            "epoch : 1576   train_loss : 0.0006\n",
            "epoch : 1576   train_loss : 0.0004\n",
            "epoch : 1576   train_loss : 0.0004\n",
            "epoch : 1576   train_loss : 0.0004\n",
            "epoch : 1576   val_loss : 0.0149\n",
            "epoch : 1576   val_loss : 0.0145\n",
            "epoch : 1577   train_loss : 0.0004\n",
            "epoch : 1577   train_loss : 0.0005\n",
            "epoch : 1577   train_loss : 0.0005\n",
            "epoch : 1577   train_loss : 0.0003\n",
            "epoch : 1577   val_loss : 0.0149\n",
            "epoch : 1577   val_loss : 0.0146\n",
            "epoch : 1578   train_loss : 0.0006\n",
            "epoch : 1578   train_loss : 0.0004\n",
            "epoch : 1578   train_loss : 0.0005\n",
            "epoch : 1578   train_loss : 0.0003\n",
            "epoch : 1578   val_loss : 0.0151\n",
            "epoch : 1578   val_loss : 0.0139\n",
            "epoch : 1579   train_loss : 0.0003\n",
            "epoch : 1579   train_loss : 0.0006\n",
            "epoch : 1579   train_loss : 0.0004\n",
            "epoch : 1579   train_loss : 0.0004\n",
            "epoch : 1579   val_loss : 0.0157\n",
            "epoch : 1579   val_loss : 0.0120\n",
            "new model saved at epoch 1579 with val_loss 0.012009275145828724\n",
            "epoch : 1580   train_loss : 0.0005\n",
            "epoch : 1580   train_loss : 0.0005\n",
            "epoch : 1580   train_loss : 0.0004\n",
            "epoch : 1580   train_loss : 0.0004\n",
            "epoch : 1580   val_loss : 0.0140\n",
            "epoch : 1580   val_loss : 0.0167\n",
            "epoch : 1581   train_loss : 0.0004\n",
            "epoch : 1581   train_loss : 0.0004\n",
            "epoch : 1581   train_loss : 0.0005\n",
            "epoch : 1581   train_loss : 0.0004\n",
            "epoch : 1581   val_loss : 0.0137\n",
            "epoch : 1581   val_loss : 0.0172\n",
            "epoch : 1582   train_loss : 0.0004\n",
            "epoch : 1582   train_loss : 0.0005\n",
            "epoch : 1582   train_loss : 0.0004\n",
            "epoch : 1582   train_loss : 0.0004\n",
            "epoch : 1582   val_loss : 0.0146\n",
            "epoch : 1582   val_loss : 0.0148\n",
            "epoch : 1583   train_loss : 0.0004\n",
            "epoch : 1583   train_loss : 0.0004\n",
            "epoch : 1583   train_loss : 0.0004\n",
            "epoch : 1583   train_loss : 0.0005\n",
            "epoch : 1583   val_loss : 0.0151\n",
            "epoch : 1583   val_loss : 0.0134\n",
            "epoch : 1584   train_loss : 0.0005\n",
            "epoch : 1584   train_loss : 0.0004\n",
            "epoch : 1584   train_loss : 0.0005\n",
            "epoch : 1584   train_loss : 0.0003\n",
            "epoch : 1584   val_loss : 0.0151\n",
            "epoch : 1584   val_loss : 0.0132\n",
            "epoch : 1585   train_loss : 0.0003\n",
            "epoch : 1585   train_loss : 0.0004\n",
            "epoch : 1585   train_loss : 0.0006\n",
            "epoch : 1585   train_loss : 0.0005\n",
            "epoch : 1585   val_loss : 0.0140\n",
            "epoch : 1585   val_loss : 0.0161\n",
            "epoch : 1586   train_loss : 0.0004\n",
            "epoch : 1586   train_loss : 0.0004\n",
            "epoch : 1586   train_loss : 0.0004\n",
            "epoch : 1586   train_loss : 0.0007\n",
            "epoch : 1586   val_loss : 0.0145\n",
            "epoch : 1586   val_loss : 0.0145\n",
            "epoch : 1587   train_loss : 0.0004\n",
            "epoch : 1587   train_loss : 0.0005\n",
            "epoch : 1587   train_loss : 0.0005\n",
            "epoch : 1587   train_loss : 0.0003\n",
            "epoch : 1587   val_loss : 0.0144\n",
            "epoch : 1587   val_loss : 0.0146\n",
            "epoch : 1588   train_loss : 0.0006\n",
            "epoch : 1588   train_loss : 0.0004\n",
            "epoch : 1588   train_loss : 0.0003\n",
            "epoch : 1588   train_loss : 0.0004\n",
            "epoch : 1588   val_loss : 0.0145\n",
            "epoch : 1588   val_loss : 0.0142\n",
            "epoch : 1589   train_loss : 0.0003\n",
            "epoch : 1589   train_loss : 0.0004\n",
            "epoch : 1589   train_loss : 0.0005\n",
            "epoch : 1589   train_loss : 0.0004\n",
            "epoch : 1589   val_loss : 0.0141\n",
            "epoch : 1589   val_loss : 0.0153\n",
            "epoch : 1590   train_loss : 0.0005\n",
            "epoch : 1590   train_loss : 0.0004\n",
            "epoch : 1590   train_loss : 0.0005\n",
            "epoch : 1590   train_loss : 0.0004\n",
            "epoch : 1590   val_loss : 0.0140\n",
            "epoch : 1590   val_loss : 0.0155\n",
            "epoch : 1591   train_loss : 0.0005\n",
            "epoch : 1591   train_loss : 0.0003\n",
            "epoch : 1591   train_loss : 0.0005\n",
            "epoch : 1591   train_loss : 0.0004\n",
            "epoch : 1591   val_loss : 0.0148\n",
            "epoch : 1591   val_loss : 0.0132\n",
            "epoch : 1592   train_loss : 0.0004\n",
            "epoch : 1592   train_loss : 0.0004\n",
            "epoch : 1592   train_loss : 0.0004\n",
            "epoch : 1592   train_loss : 0.0005\n",
            "epoch : 1592   val_loss : 0.0141\n",
            "epoch : 1592   val_loss : 0.0150\n",
            "epoch : 1593   train_loss : 0.0004\n",
            "epoch : 1593   train_loss : 0.0005\n",
            "epoch : 1593   train_loss : 0.0004\n",
            "epoch : 1593   train_loss : 0.0004\n",
            "epoch : 1593   val_loss : 0.0141\n",
            "epoch : 1593   val_loss : 0.0148\n",
            "epoch : 1594   train_loss : 0.0006\n",
            "epoch : 1594   train_loss : 0.0004\n",
            "epoch : 1594   train_loss : 0.0004\n",
            "epoch : 1594   train_loss : 0.0004\n",
            "epoch : 1594   val_loss : 0.0139\n",
            "epoch : 1594   val_loss : 0.0153\n",
            "epoch : 1595   train_loss : 0.0004\n",
            "epoch : 1595   train_loss : 0.0004\n",
            "epoch : 1595   train_loss : 0.0004\n",
            "epoch : 1595   train_loss : 0.0005\n",
            "epoch : 1595   val_loss : 0.0142\n",
            "epoch : 1595   val_loss : 0.0142\n",
            "epoch : 1596   train_loss : 0.0004\n",
            "epoch : 1596   train_loss : 0.0004\n",
            "epoch : 1596   train_loss : 0.0005\n",
            "epoch : 1596   train_loss : 0.0006\n",
            "epoch : 1596   val_loss : 0.0143\n",
            "epoch : 1596   val_loss : 0.0140\n",
            "epoch : 1597   train_loss : 0.0004\n",
            "epoch : 1597   train_loss : 0.0004\n",
            "epoch : 1597   train_loss : 0.0005\n",
            "epoch : 1597   train_loss : 0.0004\n",
            "epoch : 1597   val_loss : 0.0140\n",
            "epoch : 1597   val_loss : 0.0147\n",
            "epoch : 1598   train_loss : 0.0003\n",
            "epoch : 1598   train_loss : 0.0006\n",
            "epoch : 1598   train_loss : 0.0004\n",
            "epoch : 1598   train_loss : 0.0004\n",
            "epoch : 1598   val_loss : 0.0142\n",
            "epoch : 1598   val_loss : 0.0140\n",
            "epoch : 1599   train_loss : 0.0004\n",
            "epoch : 1599   train_loss : 0.0005\n",
            "epoch : 1599   train_loss : 0.0004\n",
            "epoch : 1599   train_loss : 0.0003\n",
            "epoch : 1599   val_loss : 0.0149\n",
            "epoch : 1599   val_loss : 0.0119\n",
            "new model saved at epoch 1599 with val_loss 0.011887893080711365\n",
            "epoch : 1600   train_loss : 0.0005\n",
            "epoch : 1600   train_loss : 0.0004\n",
            "epoch : 1600   train_loss : 0.0004\n",
            "epoch : 1600   train_loss : 0.0004\n",
            "epoch : 1600   val_loss : 0.0142\n",
            "epoch : 1600   val_loss : 0.0139\n",
            "epoch : 1601   train_loss : 0.0005\n",
            "epoch : 1601   train_loss : 0.0005\n",
            "epoch : 1601   train_loss : 0.0004\n",
            "epoch : 1601   train_loss : 0.0003\n",
            "epoch : 1601   val_loss : 0.0137\n",
            "epoch : 1601   val_loss : 0.0151\n",
            "epoch : 1602   train_loss : 0.0004\n",
            "epoch : 1602   train_loss : 0.0004\n",
            "epoch : 1602   train_loss : 0.0005\n",
            "epoch : 1602   train_loss : 0.0005\n",
            "epoch : 1602   val_loss : 0.0140\n",
            "epoch : 1602   val_loss : 0.0141\n",
            "epoch : 1603   train_loss : 0.0005\n",
            "epoch : 1603   train_loss : 0.0004\n",
            "epoch : 1603   train_loss : 0.0004\n",
            "epoch : 1603   train_loss : 0.0003\n",
            "epoch : 1603   val_loss : 0.0140\n",
            "epoch : 1603   val_loss : 0.0140\n",
            "epoch : 1604   train_loss : 0.0005\n",
            "epoch : 1604   train_loss : 0.0004\n",
            "epoch : 1604   train_loss : 0.0003\n",
            "epoch : 1604   train_loss : 0.0005\n",
            "epoch : 1604   val_loss : 0.0143\n",
            "epoch : 1604   val_loss : 0.0131\n",
            "epoch : 1605   train_loss : 0.0005\n",
            "epoch : 1605   train_loss : 0.0005\n",
            "epoch : 1605   train_loss : 0.0004\n",
            "epoch : 1605   train_loss : 0.0003\n",
            "epoch : 1605   val_loss : 0.0141\n",
            "epoch : 1605   val_loss : 0.0135\n",
            "epoch : 1606   train_loss : 0.0005\n",
            "epoch : 1606   train_loss : 0.0005\n",
            "epoch : 1606   train_loss : 0.0003\n",
            "epoch : 1606   train_loss : 0.0004\n",
            "epoch : 1606   val_loss : 0.0136\n",
            "epoch : 1606   val_loss : 0.0148\n",
            "epoch : 1607   train_loss : 0.0004\n",
            "epoch : 1607   train_loss : 0.0004\n",
            "epoch : 1607   train_loss : 0.0005\n",
            "epoch : 1607   train_loss : 0.0004\n",
            "epoch : 1607   val_loss : 0.0137\n",
            "epoch : 1607   val_loss : 0.0143\n",
            "epoch : 1608   train_loss : 0.0005\n",
            "epoch : 1608   train_loss : 0.0004\n",
            "epoch : 1608   train_loss : 0.0004\n",
            "epoch : 1608   train_loss : 0.0004\n",
            "epoch : 1608   val_loss : 0.0143\n",
            "epoch : 1608   val_loss : 0.0127\n",
            "epoch : 1609   train_loss : 0.0006\n",
            "epoch : 1609   train_loss : 0.0004\n",
            "epoch : 1609   train_loss : 0.0004\n",
            "epoch : 1609   train_loss : 0.0003\n",
            "epoch : 1609   val_loss : 0.0141\n",
            "epoch : 1609   val_loss : 0.0131\n",
            "epoch : 1610   train_loss : 0.0003\n",
            "epoch : 1610   train_loss : 0.0004\n",
            "epoch : 1610   train_loss : 0.0004\n",
            "epoch : 1610   train_loss : 0.0008\n",
            "epoch : 1610   val_loss : 0.0138\n",
            "epoch : 1610   val_loss : 0.0137\n",
            "epoch : 1611   train_loss : 0.0004\n",
            "epoch : 1611   train_loss : 0.0003\n",
            "epoch : 1611   train_loss : 0.0006\n",
            "epoch : 1611   train_loss : 0.0003\n",
            "epoch : 1611   val_loss : 0.0139\n",
            "epoch : 1611   val_loss : 0.0134\n",
            "epoch : 1612   train_loss : 0.0004\n",
            "epoch : 1612   train_loss : 0.0004\n",
            "epoch : 1612   train_loss : 0.0004\n",
            "epoch : 1612   train_loss : 0.0004\n",
            "epoch : 1612   val_loss : 0.0139\n",
            "epoch : 1612   val_loss : 0.0132\n",
            "epoch : 1613   train_loss : 0.0004\n",
            "epoch : 1613   train_loss : 0.0005\n",
            "epoch : 1613   train_loss : 0.0004\n",
            "epoch : 1613   train_loss : 0.0003\n",
            "epoch : 1613   val_loss : 0.0135\n",
            "epoch : 1613   val_loss : 0.0143\n",
            "epoch : 1614   train_loss : 0.0003\n",
            "epoch : 1614   train_loss : 0.0004\n",
            "epoch : 1614   train_loss : 0.0005\n",
            "epoch : 1614   train_loss : 0.0005\n",
            "epoch : 1614   val_loss : 0.0139\n",
            "epoch : 1614   val_loss : 0.0132\n",
            "epoch : 1615   train_loss : 0.0003\n",
            "epoch : 1615   train_loss : 0.0004\n",
            "epoch : 1615   train_loss : 0.0004\n",
            "epoch : 1615   train_loss : 0.0006\n",
            "epoch : 1615   val_loss : 0.0133\n",
            "epoch : 1615   val_loss : 0.0147\n",
            "epoch : 1616   train_loss : 0.0004\n",
            "epoch : 1616   train_loss : 0.0003\n",
            "epoch : 1616   train_loss : 0.0004\n",
            "epoch : 1616   train_loss : 0.0006\n",
            "epoch : 1616   val_loss : 0.0136\n",
            "epoch : 1616   val_loss : 0.0137\n",
            "epoch : 1617   train_loss : 0.0004\n",
            "epoch : 1617   train_loss : 0.0004\n",
            "epoch : 1617   train_loss : 0.0005\n",
            "epoch : 1617   train_loss : 0.0004\n",
            "epoch : 1617   val_loss : 0.0137\n",
            "epoch : 1617   val_loss : 0.0134\n",
            "epoch : 1618   train_loss : 0.0004\n",
            "epoch : 1618   train_loss : 0.0004\n",
            "epoch : 1618   train_loss : 0.0005\n",
            "epoch : 1618   train_loss : 0.0005\n",
            "epoch : 1618   val_loss : 0.0135\n",
            "epoch : 1618   val_loss : 0.0136\n",
            "epoch : 1619   train_loss : 0.0003\n",
            "epoch : 1619   train_loss : 0.0005\n",
            "epoch : 1619   train_loss : 0.0005\n",
            "epoch : 1619   train_loss : 0.0004\n",
            "epoch : 1619   val_loss : 0.0135\n",
            "epoch : 1619   val_loss : 0.0135\n",
            "epoch : 1620   train_loss : 0.0004\n",
            "epoch : 1620   train_loss : 0.0004\n",
            "epoch : 1620   train_loss : 0.0005\n",
            "epoch : 1620   train_loss : 0.0004\n",
            "epoch : 1620   val_loss : 0.0135\n",
            "epoch : 1620   val_loss : 0.0135\n",
            "epoch : 1621   train_loss : 0.0004\n",
            "epoch : 1621   train_loss : 0.0004\n",
            "epoch : 1621   train_loss : 0.0005\n",
            "epoch : 1621   train_loss : 0.0005\n",
            "epoch : 1621   val_loss : 0.0134\n",
            "epoch : 1621   val_loss : 0.0136\n",
            "epoch : 1622   train_loss : 0.0004\n",
            "epoch : 1622   train_loss : 0.0004\n",
            "epoch : 1622   train_loss : 0.0005\n",
            "epoch : 1622   train_loss : 0.0005\n",
            "epoch : 1622   val_loss : 0.0136\n",
            "epoch : 1622   val_loss : 0.0129\n",
            "epoch : 1623   train_loss : 0.0005\n",
            "epoch : 1623   train_loss : 0.0004\n",
            "epoch : 1623   train_loss : 0.0005\n",
            "epoch : 1623   train_loss : 0.0003\n",
            "epoch : 1623   val_loss : 0.0133\n",
            "epoch : 1623   val_loss : 0.0137\n",
            "epoch : 1624   train_loss : 0.0005\n",
            "epoch : 1624   train_loss : 0.0004\n",
            "epoch : 1624   train_loss : 0.0004\n",
            "epoch : 1624   train_loss : 0.0003\n",
            "epoch : 1624   val_loss : 0.0134\n",
            "epoch : 1624   val_loss : 0.0134\n",
            "epoch : 1625   train_loss : 0.0005\n",
            "epoch : 1625   train_loss : 0.0004\n",
            "epoch : 1625   train_loss : 0.0003\n",
            "epoch : 1625   train_loss : 0.0004\n",
            "epoch : 1625   val_loss : 0.0135\n",
            "epoch : 1625   val_loss : 0.0131\n",
            "epoch : 1626   train_loss : 0.0003\n",
            "epoch : 1626   train_loss : 0.0004\n",
            "epoch : 1626   train_loss : 0.0004\n",
            "epoch : 1626   train_loss : 0.0006\n",
            "epoch : 1626   val_loss : 0.0138\n",
            "epoch : 1626   val_loss : 0.0120\n",
            "epoch : 1627   train_loss : 0.0005\n",
            "epoch : 1627   train_loss : 0.0004\n",
            "epoch : 1627   train_loss : 0.0003\n",
            "epoch : 1627   train_loss : 0.0005\n",
            "epoch : 1627   val_loss : 0.0133\n",
            "epoch : 1627   val_loss : 0.0134\n",
            "epoch : 1628   train_loss : 0.0004\n",
            "epoch : 1628   train_loss : 0.0004\n",
            "epoch : 1628   train_loss : 0.0004\n",
            "epoch : 1628   train_loss : 0.0005\n",
            "epoch : 1628   val_loss : 0.0135\n",
            "epoch : 1628   val_loss : 0.0128\n",
            "epoch : 1629   train_loss : 0.0005\n",
            "epoch : 1629   train_loss : 0.0004\n",
            "epoch : 1629   train_loss : 0.0004\n",
            "epoch : 1629   train_loss : 0.0003\n",
            "epoch : 1629   val_loss : 0.0128\n",
            "epoch : 1629   val_loss : 0.0145\n",
            "epoch : 1630   train_loss : 0.0004\n",
            "epoch : 1630   train_loss : 0.0003\n",
            "epoch : 1630   train_loss : 0.0005\n",
            "epoch : 1630   train_loss : 0.0004\n",
            "epoch : 1630   val_loss : 0.0130\n",
            "epoch : 1630   val_loss : 0.0138\n",
            "epoch : 1631   train_loss : 0.0003\n",
            "epoch : 1631   train_loss : 0.0004\n",
            "epoch : 1631   train_loss : 0.0005\n",
            "epoch : 1631   train_loss : 0.0003\n",
            "epoch : 1631   val_loss : 0.0131\n",
            "epoch : 1631   val_loss : 0.0135\n",
            "epoch : 1632   train_loss : 0.0004\n",
            "epoch : 1632   train_loss : 0.0004\n",
            "epoch : 1632   train_loss : 0.0004\n",
            "epoch : 1632   train_loss : 0.0005\n",
            "epoch : 1632   val_loss : 0.0133\n",
            "epoch : 1632   val_loss : 0.0128\n",
            "epoch : 1633   train_loss : 0.0004\n",
            "epoch : 1633   train_loss : 0.0003\n",
            "epoch : 1633   train_loss : 0.0005\n",
            "epoch : 1633   train_loss : 0.0004\n",
            "epoch : 1633   val_loss : 0.0131\n",
            "epoch : 1633   val_loss : 0.0131\n",
            "epoch : 1634   train_loss : 0.0004\n",
            "epoch : 1634   train_loss : 0.0005\n",
            "epoch : 1634   train_loss : 0.0003\n",
            "epoch : 1634   train_loss : 0.0005\n",
            "epoch : 1634   val_loss : 0.0126\n",
            "epoch : 1634   val_loss : 0.0144\n",
            "epoch : 1635   train_loss : 0.0005\n",
            "epoch : 1635   train_loss : 0.0004\n",
            "epoch : 1635   train_loss : 0.0004\n",
            "epoch : 1635   train_loss : 0.0004\n",
            "epoch : 1635   val_loss : 0.0129\n",
            "epoch : 1635   val_loss : 0.0135\n",
            "epoch : 1636   train_loss : 0.0004\n",
            "epoch : 1636   train_loss : 0.0005\n",
            "epoch : 1636   train_loss : 0.0005\n",
            "epoch : 1636   train_loss : 0.0003\n",
            "epoch : 1636   val_loss : 0.0125\n",
            "epoch : 1636   val_loss : 0.0145\n",
            "epoch : 1637   train_loss : 0.0005\n",
            "epoch : 1637   train_loss : 0.0004\n",
            "epoch : 1637   train_loss : 0.0003\n",
            "epoch : 1637   train_loss : 0.0003\n",
            "epoch : 1637   val_loss : 0.0132\n",
            "epoch : 1637   val_loss : 0.0126\n",
            "epoch : 1638   train_loss : 0.0004\n",
            "epoch : 1638   train_loss : 0.0004\n",
            "epoch : 1638   train_loss : 0.0005\n",
            "epoch : 1638   train_loss : 0.0003\n",
            "epoch : 1638   val_loss : 0.0131\n",
            "epoch : 1638   val_loss : 0.0127\n",
            "epoch : 1639   train_loss : 0.0003\n",
            "epoch : 1639   train_loss : 0.0003\n",
            "epoch : 1639   train_loss : 0.0005\n",
            "epoch : 1639   train_loss : 0.0005\n",
            "epoch : 1639   val_loss : 0.0126\n",
            "epoch : 1639   val_loss : 0.0140\n",
            "epoch : 1640   train_loss : 0.0004\n",
            "epoch : 1640   train_loss : 0.0004\n",
            "epoch : 1640   train_loss : 0.0004\n",
            "epoch : 1640   train_loss : 0.0003\n",
            "epoch : 1640   val_loss : 0.0129\n",
            "epoch : 1640   val_loss : 0.0132\n",
            "epoch : 1641   train_loss : 0.0003\n",
            "epoch : 1641   train_loss : 0.0005\n",
            "epoch : 1641   train_loss : 0.0003\n",
            "epoch : 1641   train_loss : 0.0006\n",
            "epoch : 1641   val_loss : 0.0129\n",
            "epoch : 1641   val_loss : 0.0129\n",
            "epoch : 1642   train_loss : 0.0004\n",
            "epoch : 1642   train_loss : 0.0004\n",
            "epoch : 1642   train_loss : 0.0004\n",
            "epoch : 1642   train_loss : 0.0005\n",
            "epoch : 1642   val_loss : 0.0128\n",
            "epoch : 1642   val_loss : 0.0133\n",
            "epoch : 1643   train_loss : 0.0003\n",
            "epoch : 1643   train_loss : 0.0005\n",
            "epoch : 1643   train_loss : 0.0004\n",
            "epoch : 1643   train_loss : 0.0005\n",
            "epoch : 1643   val_loss : 0.0129\n",
            "epoch : 1643   val_loss : 0.0128\n",
            "epoch : 1644   train_loss : 0.0004\n",
            "epoch : 1644   train_loss : 0.0005\n",
            "epoch : 1644   train_loss : 0.0004\n",
            "epoch : 1644   train_loss : 0.0003\n",
            "epoch : 1644   val_loss : 0.0128\n",
            "epoch : 1644   val_loss : 0.0129\n",
            "epoch : 1645   train_loss : 0.0004\n",
            "epoch : 1645   train_loss : 0.0004\n",
            "epoch : 1645   train_loss : 0.0004\n",
            "epoch : 1645   train_loss : 0.0005\n",
            "epoch : 1645   val_loss : 0.0123\n",
            "epoch : 1645   val_loss : 0.0142\n",
            "epoch : 1646   train_loss : 0.0005\n",
            "epoch : 1646   train_loss : 0.0003\n",
            "epoch : 1646   train_loss : 0.0003\n",
            "epoch : 1646   train_loss : 0.0004\n",
            "epoch : 1646   val_loss : 0.0127\n",
            "epoch : 1646   val_loss : 0.0131\n",
            "epoch : 1647   train_loss : 0.0003\n",
            "epoch : 1647   train_loss : 0.0004\n",
            "epoch : 1647   train_loss : 0.0005\n",
            "epoch : 1647   train_loss : 0.0003\n",
            "epoch : 1647   val_loss : 0.0127\n",
            "epoch : 1647   val_loss : 0.0129\n",
            "epoch : 1648   train_loss : 0.0003\n",
            "epoch : 1648   train_loss : 0.0006\n",
            "epoch : 1648   train_loss : 0.0003\n",
            "epoch : 1648   train_loss : 0.0005\n",
            "epoch : 1648   val_loss : 0.0127\n",
            "epoch : 1648   val_loss : 0.0128\n",
            "epoch : 1649   train_loss : 0.0004\n",
            "epoch : 1649   train_loss : 0.0003\n",
            "epoch : 1649   train_loss : 0.0005\n",
            "epoch : 1649   train_loss : 0.0003\n",
            "epoch : 1649   val_loss : 0.0127\n",
            "epoch : 1649   val_loss : 0.0128\n",
            "epoch : 1650   train_loss : 0.0004\n",
            "epoch : 1650   train_loss : 0.0005\n",
            "epoch : 1650   train_loss : 0.0004\n",
            "epoch : 1650   train_loss : 0.0003\n",
            "epoch : 1650   val_loss : 0.0124\n",
            "epoch : 1650   val_loss : 0.0134\n",
            "epoch : 1651   train_loss : 0.0004\n",
            "epoch : 1651   train_loss : 0.0003\n",
            "epoch : 1651   train_loss : 0.0003\n",
            "epoch : 1651   train_loss : 0.0006\n",
            "epoch : 1651   val_loss : 0.0124\n",
            "epoch : 1651   val_loss : 0.0135\n",
            "epoch : 1652   train_loss : 0.0003\n",
            "epoch : 1652   train_loss : 0.0005\n",
            "epoch : 1652   train_loss : 0.0003\n",
            "epoch : 1652   train_loss : 0.0006\n",
            "epoch : 1652   val_loss : 0.0124\n",
            "epoch : 1652   val_loss : 0.0132\n",
            "epoch : 1653   train_loss : 0.0006\n",
            "epoch : 1653   train_loss : 0.0003\n",
            "epoch : 1653   train_loss : 0.0003\n",
            "epoch : 1653   train_loss : 0.0004\n",
            "epoch : 1653   val_loss : 0.0128\n",
            "epoch : 1653   val_loss : 0.0119\n",
            "epoch : 1654   train_loss : 0.0004\n",
            "epoch : 1654   train_loss : 0.0004\n",
            "epoch : 1654   train_loss : 0.0004\n",
            "epoch : 1654   train_loss : 0.0005\n",
            "epoch : 1654   val_loss : 0.0129\n",
            "epoch : 1654   val_loss : 0.0117\n",
            "new model saved at epoch 1654 with val_loss 0.011724737472832203\n",
            "epoch : 1655   train_loss : 0.0005\n",
            "epoch : 1655   train_loss : 0.0004\n",
            "epoch : 1655   train_loss : 0.0003\n",
            "epoch : 1655   train_loss : 0.0003\n",
            "epoch : 1655   val_loss : 0.0124\n",
            "epoch : 1655   val_loss : 0.0129\n",
            "epoch : 1656   train_loss : 0.0003\n",
            "epoch : 1656   train_loss : 0.0005\n",
            "epoch : 1656   train_loss : 0.0003\n",
            "epoch : 1656   train_loss : 0.0004\n",
            "epoch : 1656   val_loss : 0.0124\n",
            "epoch : 1656   val_loss : 0.0128\n",
            "epoch : 1657   train_loss : 0.0004\n",
            "epoch : 1657   train_loss : 0.0006\n",
            "epoch : 1657   train_loss : 0.0003\n",
            "epoch : 1657   train_loss : 0.0004\n",
            "epoch : 1657   val_loss : 0.0126\n",
            "epoch : 1657   val_loss : 0.0121\n",
            "epoch : 1658   train_loss : 0.0005\n",
            "epoch : 1658   train_loss : 0.0004\n",
            "epoch : 1658   train_loss : 0.0004\n",
            "epoch : 1658   train_loss : 0.0003\n",
            "epoch : 1658   val_loss : 0.0128\n",
            "epoch : 1658   val_loss : 0.0115\n",
            "new model saved at epoch 1658 with val_loss 0.01154369581490755\n",
            "epoch : 1659   train_loss : 0.0003\n",
            "epoch : 1659   train_loss : 0.0004\n",
            "epoch : 1659   train_loss : 0.0004\n",
            "epoch : 1659   train_loss : 0.0005\n",
            "epoch : 1659   val_loss : 0.0121\n",
            "epoch : 1659   val_loss : 0.0133\n",
            "epoch : 1660   train_loss : 0.0003\n",
            "epoch : 1660   train_loss : 0.0003\n",
            "epoch : 1660   train_loss : 0.0004\n",
            "epoch : 1660   train_loss : 0.0006\n",
            "epoch : 1660   val_loss : 0.0123\n",
            "epoch : 1660   val_loss : 0.0127\n",
            "epoch : 1661   train_loss : 0.0004\n",
            "epoch : 1661   train_loss : 0.0004\n",
            "epoch : 1661   train_loss : 0.0004\n",
            "epoch : 1661   train_loss : 0.0004\n",
            "epoch : 1661   val_loss : 0.0123\n",
            "epoch : 1661   val_loss : 0.0125\n",
            "epoch : 1662   train_loss : 0.0004\n",
            "epoch : 1662   train_loss : 0.0004\n",
            "epoch : 1662   train_loss : 0.0005\n",
            "epoch : 1662   train_loss : 0.0003\n",
            "epoch : 1662   val_loss : 0.0124\n",
            "epoch : 1662   val_loss : 0.0122\n",
            "epoch : 1663   train_loss : 0.0004\n",
            "epoch : 1663   train_loss : 0.0004\n",
            "epoch : 1663   train_loss : 0.0004\n",
            "epoch : 1663   train_loss : 0.0005\n",
            "epoch : 1663   val_loss : 0.0123\n",
            "epoch : 1663   val_loss : 0.0125\n",
            "epoch : 1664   train_loss : 0.0003\n",
            "epoch : 1664   train_loss : 0.0004\n",
            "epoch : 1664   train_loss : 0.0004\n",
            "epoch : 1664   train_loss : 0.0004\n",
            "epoch : 1664   val_loss : 0.0131\n",
            "epoch : 1664   val_loss : 0.0101\n",
            "new model saved at epoch 1664 with val_loss 0.010051582008600235\n",
            "epoch : 1665   train_loss : 0.0005\n",
            "epoch : 1665   train_loss : 0.0003\n",
            "epoch : 1665   train_loss : 0.0003\n",
            "epoch : 1665   train_loss : 0.0004\n",
            "epoch : 1665   val_loss : 0.0125\n",
            "epoch : 1665   val_loss : 0.0118\n",
            "epoch : 1666   train_loss : 0.0004\n",
            "epoch : 1666   train_loss : 0.0004\n",
            "epoch : 1666   train_loss : 0.0004\n",
            "epoch : 1666   train_loss : 0.0003\n",
            "epoch : 1666   val_loss : 0.0114\n",
            "epoch : 1666   val_loss : 0.0145\n",
            "epoch : 1667   train_loss : 0.0003\n",
            "epoch : 1667   train_loss : 0.0003\n",
            "epoch : 1667   train_loss : 0.0005\n",
            "epoch : 1667   train_loss : 0.0005\n",
            "epoch : 1667   val_loss : 0.0122\n",
            "epoch : 1667   val_loss : 0.0123\n",
            "epoch : 1668   train_loss : 0.0003\n",
            "epoch : 1668   train_loss : 0.0004\n",
            "epoch : 1668   train_loss : 0.0004\n",
            "epoch : 1668   train_loss : 0.0005\n",
            "epoch : 1668   val_loss : 0.0119\n",
            "epoch : 1668   val_loss : 0.0131\n",
            "epoch : 1669   train_loss : 0.0004\n",
            "epoch : 1669   train_loss : 0.0005\n",
            "epoch : 1669   train_loss : 0.0003\n",
            "epoch : 1669   train_loss : 0.0004\n",
            "epoch : 1669   val_loss : 0.0123\n",
            "epoch : 1669   val_loss : 0.0119\n",
            "epoch : 1670   train_loss : 0.0004\n",
            "epoch : 1670   train_loss : 0.0003\n",
            "epoch : 1670   train_loss : 0.0004\n",
            "epoch : 1670   train_loss : 0.0004\n",
            "epoch : 1670   val_loss : 0.0122\n",
            "epoch : 1670   val_loss : 0.0121\n",
            "epoch : 1671   train_loss : 0.0003\n",
            "epoch : 1671   train_loss : 0.0005\n",
            "epoch : 1671   train_loss : 0.0003\n",
            "epoch : 1671   train_loss : 0.0004\n",
            "epoch : 1671   val_loss : 0.0124\n",
            "epoch : 1671   val_loss : 0.0115\n",
            "epoch : 1672   train_loss : 0.0003\n",
            "epoch : 1672   train_loss : 0.0005\n",
            "epoch : 1672   train_loss : 0.0004\n",
            "epoch : 1672   train_loss : 0.0004\n",
            "epoch : 1672   val_loss : 0.0123\n",
            "epoch : 1672   val_loss : 0.0116\n",
            "epoch : 1673   train_loss : 0.0005\n",
            "epoch : 1673   train_loss : 0.0004\n",
            "epoch : 1673   train_loss : 0.0003\n",
            "epoch : 1673   train_loss : 0.0003\n",
            "epoch : 1673   val_loss : 0.0127\n",
            "epoch : 1673   val_loss : 0.0103\n",
            "epoch : 1674   train_loss : 0.0004\n",
            "epoch : 1674   train_loss : 0.0004\n",
            "epoch : 1674   train_loss : 0.0003\n",
            "epoch : 1674   train_loss : 0.0004\n",
            "epoch : 1674   val_loss : 0.0121\n",
            "epoch : 1674   val_loss : 0.0119\n",
            "epoch : 1675   train_loss : 0.0005\n",
            "epoch : 1675   train_loss : 0.0003\n",
            "epoch : 1675   train_loss : 0.0004\n",
            "epoch : 1675   train_loss : 0.0003\n",
            "epoch : 1675   val_loss : 0.0120\n",
            "epoch : 1675   val_loss : 0.0121\n",
            "epoch : 1676   train_loss : 0.0005\n",
            "epoch : 1676   train_loss : 0.0003\n",
            "epoch : 1676   train_loss : 0.0004\n",
            "epoch : 1676   train_loss : 0.0003\n",
            "epoch : 1676   val_loss : 0.0119\n",
            "epoch : 1676   val_loss : 0.0122\n",
            "epoch : 1677   train_loss : 0.0005\n",
            "epoch : 1677   train_loss : 0.0003\n",
            "epoch : 1677   train_loss : 0.0004\n",
            "epoch : 1677   train_loss : 0.0003\n",
            "epoch : 1677   val_loss : 0.0120\n",
            "epoch : 1677   val_loss : 0.0119\n",
            "epoch : 1678   train_loss : 0.0004\n",
            "epoch : 1678   train_loss : 0.0003\n",
            "epoch : 1678   train_loss : 0.0005\n",
            "epoch : 1678   train_loss : 0.0004\n",
            "epoch : 1678   val_loss : 0.0123\n",
            "epoch : 1678   val_loss : 0.0110\n",
            "epoch : 1679   train_loss : 0.0004\n",
            "epoch : 1679   train_loss : 0.0004\n",
            "epoch : 1679   train_loss : 0.0003\n",
            "epoch : 1679   train_loss : 0.0004\n",
            "epoch : 1679   val_loss : 0.0125\n",
            "epoch : 1679   val_loss : 0.0105\n",
            "epoch : 1680   train_loss : 0.0003\n",
            "epoch : 1680   train_loss : 0.0004\n",
            "epoch : 1680   train_loss : 0.0004\n",
            "epoch : 1680   train_loss : 0.0004\n",
            "epoch : 1680   val_loss : 0.0115\n",
            "epoch : 1680   val_loss : 0.0128\n",
            "epoch : 1681   train_loss : 0.0005\n",
            "epoch : 1681   train_loss : 0.0003\n",
            "epoch : 1681   train_loss : 0.0004\n",
            "epoch : 1681   train_loss : 0.0004\n",
            "epoch : 1681   val_loss : 0.0116\n",
            "epoch : 1681   val_loss : 0.0126\n",
            "epoch : 1682   train_loss : 0.0003\n",
            "epoch : 1682   train_loss : 0.0004\n",
            "epoch : 1682   train_loss : 0.0004\n",
            "epoch : 1682   train_loss : 0.0005\n",
            "epoch : 1682   val_loss : 0.0120\n",
            "epoch : 1682   val_loss : 0.0113\n",
            "epoch : 1683   train_loss : 0.0003\n",
            "epoch : 1683   train_loss : 0.0004\n",
            "epoch : 1683   train_loss : 0.0005\n",
            "epoch : 1683   train_loss : 0.0004\n",
            "epoch : 1683   val_loss : 0.0120\n",
            "epoch : 1683   val_loss : 0.0113\n",
            "epoch : 1684   train_loss : 0.0004\n",
            "epoch : 1684   train_loss : 0.0004\n",
            "epoch : 1684   train_loss : 0.0003\n",
            "epoch : 1684   train_loss : 0.0004\n",
            "epoch : 1684   val_loss : 0.0117\n",
            "epoch : 1684   val_loss : 0.0120\n",
            "epoch : 1685   train_loss : 0.0005\n",
            "epoch : 1685   train_loss : 0.0003\n",
            "epoch : 1685   train_loss : 0.0003\n",
            "epoch : 1685   train_loss : 0.0003\n",
            "epoch : 1685   val_loss : 0.0112\n",
            "epoch : 1685   val_loss : 0.0132\n",
            "epoch : 1686   train_loss : 0.0005\n",
            "epoch : 1686   train_loss : 0.0003\n",
            "epoch : 1686   train_loss : 0.0003\n",
            "epoch : 1686   train_loss : 0.0003\n",
            "epoch : 1686   val_loss : 0.0119\n",
            "epoch : 1686   val_loss : 0.0112\n",
            "epoch : 1687   train_loss : 0.0004\n",
            "epoch : 1687   train_loss : 0.0004\n",
            "epoch : 1687   train_loss : 0.0004\n",
            "epoch : 1687   train_loss : 0.0003\n",
            "epoch : 1687   val_loss : 0.0115\n",
            "epoch : 1687   val_loss : 0.0121\n",
            "epoch : 1688   train_loss : 0.0003\n",
            "epoch : 1688   train_loss : 0.0004\n",
            "epoch : 1688   train_loss : 0.0003\n",
            "epoch : 1688   train_loss : 0.0005\n",
            "epoch : 1688   val_loss : 0.0115\n",
            "epoch : 1688   val_loss : 0.0122\n",
            "epoch : 1689   train_loss : 0.0004\n",
            "epoch : 1689   train_loss : 0.0004\n",
            "epoch : 1689   train_loss : 0.0004\n",
            "epoch : 1689   train_loss : 0.0003\n",
            "epoch : 1689   val_loss : 0.0117\n",
            "epoch : 1689   val_loss : 0.0116\n",
            "epoch : 1690   train_loss : 0.0003\n",
            "epoch : 1690   train_loss : 0.0004\n",
            "epoch : 1690   train_loss : 0.0005\n",
            "epoch : 1690   train_loss : 0.0003\n",
            "epoch : 1690   val_loss : 0.0116\n",
            "epoch : 1690   val_loss : 0.0116\n",
            "epoch : 1691   train_loss : 0.0004\n",
            "epoch : 1691   train_loss : 0.0004\n",
            "epoch : 1691   train_loss : 0.0003\n",
            "epoch : 1691   train_loss : 0.0003\n",
            "epoch : 1691   val_loss : 0.0115\n",
            "epoch : 1691   val_loss : 0.0118\n",
            "epoch : 1692   train_loss : 0.0003\n",
            "epoch : 1692   train_loss : 0.0004\n",
            "epoch : 1692   train_loss : 0.0004\n",
            "epoch : 1692   train_loss : 0.0003\n",
            "epoch : 1692   val_loss : 0.0118\n",
            "epoch : 1692   val_loss : 0.0109\n",
            "epoch : 1693   train_loss : 0.0004\n",
            "epoch : 1693   train_loss : 0.0004\n",
            "epoch : 1693   train_loss : 0.0003\n",
            "epoch : 1693   train_loss : 0.0003\n",
            "epoch : 1693   val_loss : 0.0114\n",
            "epoch : 1693   val_loss : 0.0118\n",
            "epoch : 1694   train_loss : 0.0003\n",
            "epoch : 1694   train_loss : 0.0004\n",
            "epoch : 1694   train_loss : 0.0004\n",
            "epoch : 1694   train_loss : 0.0004\n",
            "epoch : 1694   val_loss : 0.0115\n",
            "epoch : 1694   val_loss : 0.0116\n",
            "epoch : 1695   train_loss : 0.0004\n",
            "epoch : 1695   train_loss : 0.0003\n",
            "epoch : 1695   train_loss : 0.0004\n",
            "epoch : 1695   train_loss : 0.0003\n",
            "epoch : 1695   val_loss : 0.0115\n",
            "epoch : 1695   val_loss : 0.0115\n",
            "epoch : 1696   train_loss : 0.0005\n",
            "epoch : 1696   train_loss : 0.0003\n",
            "epoch : 1696   train_loss : 0.0003\n",
            "epoch : 1696   train_loss : 0.0004\n",
            "epoch : 1696   val_loss : 0.0115\n",
            "epoch : 1696   val_loss : 0.0114\n",
            "epoch : 1697   train_loss : 0.0003\n",
            "epoch : 1697   train_loss : 0.0003\n",
            "epoch : 1697   train_loss : 0.0004\n",
            "epoch : 1697   train_loss : 0.0005\n",
            "epoch : 1697   val_loss : 0.0121\n",
            "epoch : 1697   val_loss : 0.0096\n",
            "new model saved at epoch 1697 with val_loss 0.009578502736985683\n",
            "epoch : 1698   train_loss : 0.0004\n",
            "epoch : 1698   train_loss : 0.0004\n",
            "epoch : 1698   train_loss : 0.0003\n",
            "epoch : 1698   train_loss : 0.0004\n",
            "epoch : 1698   val_loss : 0.0114\n",
            "epoch : 1698   val_loss : 0.0115\n",
            "epoch : 1699   train_loss : 0.0005\n",
            "epoch : 1699   train_loss : 0.0003\n",
            "epoch : 1699   train_loss : 0.0003\n",
            "epoch : 1699   train_loss : 0.0003\n",
            "epoch : 1699   val_loss : 0.0114\n",
            "epoch : 1699   val_loss : 0.0113\n",
            "epoch : 1700   train_loss : 0.0004\n",
            "epoch : 1700   train_loss : 0.0004\n",
            "epoch : 1700   train_loss : 0.0003\n",
            "epoch : 1700   train_loss : 0.0003\n",
            "epoch : 1700   val_loss : 0.0111\n",
            "epoch : 1700   val_loss : 0.0121\n",
            "epoch : 1701   train_loss : 0.0003\n",
            "epoch : 1701   train_loss : 0.0004\n",
            "epoch : 1701   train_loss : 0.0005\n",
            "epoch : 1701   train_loss : 0.0003\n",
            "epoch : 1701   val_loss : 0.0111\n",
            "epoch : 1701   val_loss : 0.0119\n",
            "epoch : 1702   train_loss : 0.0005\n",
            "epoch : 1702   train_loss : 0.0003\n",
            "epoch : 1702   train_loss : 0.0003\n",
            "epoch : 1702   train_loss : 0.0003\n",
            "epoch : 1702   val_loss : 0.0115\n",
            "epoch : 1702   val_loss : 0.0108\n",
            "epoch : 1703   train_loss : 0.0004\n",
            "epoch : 1703   train_loss : 0.0003\n",
            "epoch : 1703   train_loss : 0.0004\n",
            "epoch : 1703   train_loss : 0.0004\n",
            "epoch : 1703   val_loss : 0.0112\n",
            "epoch : 1703   val_loss : 0.0116\n",
            "epoch : 1704   train_loss : 0.0004\n",
            "epoch : 1704   train_loss : 0.0003\n",
            "epoch : 1704   train_loss : 0.0004\n",
            "epoch : 1704   train_loss : 0.0004\n",
            "epoch : 1704   val_loss : 0.0116\n",
            "epoch : 1704   val_loss : 0.0103\n",
            "epoch : 1705   train_loss : 0.0003\n",
            "epoch : 1705   train_loss : 0.0004\n",
            "epoch : 1705   train_loss : 0.0004\n",
            "epoch : 1705   train_loss : 0.0004\n",
            "epoch : 1705   val_loss : 0.0116\n",
            "epoch : 1705   val_loss : 0.0103\n",
            "epoch : 1706   train_loss : 0.0003\n",
            "epoch : 1706   train_loss : 0.0004\n",
            "epoch : 1706   train_loss : 0.0004\n",
            "epoch : 1706   train_loss : 0.0003\n",
            "epoch : 1706   val_loss : 0.0109\n",
            "epoch : 1706   val_loss : 0.0121\n",
            "epoch : 1707   train_loss : 0.0003\n",
            "epoch : 1707   train_loss : 0.0003\n",
            "epoch : 1707   train_loss : 0.0005\n",
            "epoch : 1707   train_loss : 0.0003\n",
            "epoch : 1707   val_loss : 0.0111\n",
            "epoch : 1707   val_loss : 0.0114\n",
            "epoch : 1708   train_loss : 0.0003\n",
            "epoch : 1708   train_loss : 0.0004\n",
            "epoch : 1708   train_loss : 0.0004\n",
            "epoch : 1708   train_loss : 0.0003\n",
            "epoch : 1708   val_loss : 0.0113\n",
            "epoch : 1708   val_loss : 0.0109\n",
            "epoch : 1709   train_loss : 0.0003\n",
            "epoch : 1709   train_loss : 0.0004\n",
            "epoch : 1709   train_loss : 0.0004\n",
            "epoch : 1709   train_loss : 0.0003\n",
            "epoch : 1709   val_loss : 0.0112\n",
            "epoch : 1709   val_loss : 0.0110\n",
            "epoch : 1710   train_loss : 0.0004\n",
            "epoch : 1710   train_loss : 0.0004\n",
            "epoch : 1710   train_loss : 0.0003\n",
            "epoch : 1710   train_loss : 0.0002\n",
            "epoch : 1710   val_loss : 0.0113\n",
            "epoch : 1710   val_loss : 0.0106\n",
            "epoch : 1711   train_loss : 0.0004\n",
            "epoch : 1711   train_loss : 0.0003\n",
            "epoch : 1711   train_loss : 0.0004\n",
            "epoch : 1711   train_loss : 0.0003\n",
            "epoch : 1711   val_loss : 0.0109\n",
            "epoch : 1711   val_loss : 0.0117\n",
            "epoch : 1712   train_loss : 0.0003\n",
            "epoch : 1712   train_loss : 0.0003\n",
            "epoch : 1712   train_loss : 0.0003\n",
            "epoch : 1712   train_loss : 0.0006\n",
            "epoch : 1712   val_loss : 0.0110\n",
            "epoch : 1712   val_loss : 0.0111\n",
            "epoch : 1713   train_loss : 0.0003\n",
            "epoch : 1713   train_loss : 0.0003\n",
            "epoch : 1713   train_loss : 0.0004\n",
            "epoch : 1713   train_loss : 0.0005\n",
            "epoch : 1713   val_loss : 0.0111\n",
            "epoch : 1713   val_loss : 0.0108\n",
            "epoch : 1714   train_loss : 0.0003\n",
            "epoch : 1714   train_loss : 0.0003\n",
            "epoch : 1714   train_loss : 0.0005\n",
            "epoch : 1714   train_loss : 0.0003\n",
            "epoch : 1714   val_loss : 0.0112\n",
            "epoch : 1714   val_loss : 0.0106\n",
            "epoch : 1715   train_loss : 0.0004\n",
            "epoch : 1715   train_loss : 0.0004\n",
            "epoch : 1715   train_loss : 0.0003\n",
            "epoch : 1715   train_loss : 0.0003\n",
            "epoch : 1715   val_loss : 0.0110\n",
            "epoch : 1715   val_loss : 0.0111\n",
            "epoch : 1716   train_loss : 0.0004\n",
            "epoch : 1716   train_loss : 0.0004\n",
            "epoch : 1716   train_loss : 0.0003\n",
            "epoch : 1716   train_loss : 0.0003\n",
            "epoch : 1716   val_loss : 0.0114\n",
            "epoch : 1716   val_loss : 0.0097\n",
            "epoch : 1717   train_loss : 0.0004\n",
            "epoch : 1717   train_loss : 0.0004\n",
            "epoch : 1717   train_loss : 0.0004\n",
            "epoch : 1717   train_loss : 0.0003\n",
            "epoch : 1717   val_loss : 0.0110\n",
            "epoch : 1717   val_loss : 0.0108\n",
            "epoch : 1718   train_loss : 0.0003\n",
            "epoch : 1718   train_loss : 0.0004\n",
            "epoch : 1718   train_loss : 0.0004\n",
            "epoch : 1718   train_loss : 0.0003\n",
            "epoch : 1718   val_loss : 0.0107\n",
            "epoch : 1718   val_loss : 0.0114\n",
            "epoch : 1719   train_loss : 0.0004\n",
            "epoch : 1719   train_loss : 0.0003\n",
            "epoch : 1719   train_loss : 0.0004\n",
            "epoch : 1719   train_loss : 0.0003\n",
            "epoch : 1719   val_loss : 0.0110\n",
            "epoch : 1719   val_loss : 0.0105\n",
            "epoch : 1720   train_loss : 0.0003\n",
            "epoch : 1720   train_loss : 0.0005\n",
            "epoch : 1720   train_loss : 0.0003\n",
            "epoch : 1720   train_loss : 0.0002\n",
            "epoch : 1720   val_loss : 0.0107\n",
            "epoch : 1720   val_loss : 0.0113\n",
            "epoch : 1721   train_loss : 0.0003\n",
            "epoch : 1721   train_loss : 0.0002\n",
            "epoch : 1721   train_loss : 0.0004\n",
            "epoch : 1721   train_loss : 0.0006\n",
            "epoch : 1721   val_loss : 0.0109\n",
            "epoch : 1721   val_loss : 0.0106\n",
            "epoch : 1722   train_loss : 0.0005\n",
            "epoch : 1722   train_loss : 0.0003\n",
            "epoch : 1722   train_loss : 0.0003\n",
            "epoch : 1722   train_loss : 0.0003\n",
            "epoch : 1722   val_loss : 0.0106\n",
            "epoch : 1722   val_loss : 0.0115\n",
            "epoch : 1723   train_loss : 0.0002\n",
            "epoch : 1723   train_loss : 0.0003\n",
            "epoch : 1723   train_loss : 0.0005\n",
            "epoch : 1723   train_loss : 0.0004\n",
            "epoch : 1723   val_loss : 0.0103\n",
            "epoch : 1723   val_loss : 0.0123\n",
            "epoch : 1724   train_loss : 0.0003\n",
            "epoch : 1724   train_loss : 0.0003\n",
            "epoch : 1724   train_loss : 0.0004\n",
            "epoch : 1724   train_loss : 0.0005\n",
            "epoch : 1724   val_loss : 0.0106\n",
            "epoch : 1724   val_loss : 0.0112\n",
            "epoch : 1725   train_loss : 0.0003\n",
            "epoch : 1725   train_loss : 0.0004\n",
            "epoch : 1725   train_loss : 0.0003\n",
            "epoch : 1725   train_loss : 0.0004\n",
            "epoch : 1725   val_loss : 0.0105\n",
            "epoch : 1725   val_loss : 0.0113\n",
            "epoch : 1726   train_loss : 0.0003\n",
            "epoch : 1726   train_loss : 0.0003\n",
            "epoch : 1726   train_loss : 0.0006\n",
            "epoch : 1726   train_loss : 0.0003\n",
            "epoch : 1726   val_loss : 0.0108\n",
            "epoch : 1726   val_loss : 0.0106\n",
            "epoch : 1727   train_loss : 0.0003\n",
            "epoch : 1727   train_loss : 0.0003\n",
            "epoch : 1727   train_loss : 0.0004\n",
            "epoch : 1727   train_loss : 0.0005\n",
            "epoch : 1727   val_loss : 0.0104\n",
            "epoch : 1727   val_loss : 0.0116\n",
            "epoch : 1728   train_loss : 0.0003\n",
            "epoch : 1728   train_loss : 0.0004\n",
            "epoch : 1728   train_loss : 0.0003\n",
            "epoch : 1728   train_loss : 0.0004\n",
            "epoch : 1728   val_loss : 0.0107\n",
            "epoch : 1728   val_loss : 0.0106\n",
            "epoch : 1729   train_loss : 0.0004\n",
            "epoch : 1729   train_loss : 0.0003\n",
            "epoch : 1729   train_loss : 0.0003\n",
            "epoch : 1729   train_loss : 0.0004\n",
            "epoch : 1729   val_loss : 0.0108\n",
            "epoch : 1729   val_loss : 0.0102\n",
            "epoch : 1730   train_loss : 0.0003\n",
            "epoch : 1730   train_loss : 0.0004\n",
            "epoch : 1730   train_loss : 0.0004\n",
            "epoch : 1730   train_loss : 0.0003\n",
            "epoch : 1730   val_loss : 0.0113\n",
            "epoch : 1730   val_loss : 0.0088\n",
            "new model saved at epoch 1730 with val_loss 0.00877399742603302\n",
            "epoch : 1731   train_loss : 0.0003\n",
            "epoch : 1731   train_loss : 0.0003\n",
            "epoch : 1731   train_loss : 0.0004\n",
            "epoch : 1731   train_loss : 0.0004\n",
            "epoch : 1731   val_loss : 0.0103\n",
            "epoch : 1731   val_loss : 0.0115\n",
            "epoch : 1732   train_loss : 0.0003\n",
            "epoch : 1732   train_loss : 0.0004\n",
            "epoch : 1732   train_loss : 0.0004\n",
            "epoch : 1732   train_loss : 0.0002\n",
            "epoch : 1732   val_loss : 0.0108\n",
            "epoch : 1732   val_loss : 0.0101\n",
            "epoch : 1733   train_loss : 0.0004\n",
            "epoch : 1733   train_loss : 0.0003\n",
            "epoch : 1733   train_loss : 0.0003\n",
            "epoch : 1733   train_loss : 0.0004\n",
            "epoch : 1733   val_loss : 0.0103\n",
            "epoch : 1733   val_loss : 0.0113\n",
            "epoch : 1734   train_loss : 0.0004\n",
            "epoch : 1734   train_loss : 0.0003\n",
            "epoch : 1734   train_loss : 0.0003\n",
            "epoch : 1734   train_loss : 0.0003\n",
            "epoch : 1734   val_loss : 0.0105\n",
            "epoch : 1734   val_loss : 0.0107\n",
            "epoch : 1735   train_loss : 0.0003\n",
            "epoch : 1735   train_loss : 0.0003\n",
            "epoch : 1735   train_loss : 0.0004\n",
            "epoch : 1735   train_loss : 0.0003\n",
            "epoch : 1735   val_loss : 0.0102\n",
            "epoch : 1735   val_loss : 0.0115\n",
            "epoch : 1736   train_loss : 0.0003\n",
            "epoch : 1736   train_loss : 0.0003\n",
            "epoch : 1736   train_loss : 0.0003\n",
            "epoch : 1736   train_loss : 0.0005\n",
            "epoch : 1736   val_loss : 0.0106\n",
            "epoch : 1736   val_loss : 0.0103\n",
            "epoch : 1737   train_loss : 0.0003\n",
            "epoch : 1737   train_loss : 0.0004\n",
            "epoch : 1737   train_loss : 0.0004\n",
            "epoch : 1737   train_loss : 0.0003\n",
            "epoch : 1737   val_loss : 0.0103\n",
            "epoch : 1737   val_loss : 0.0110\n",
            "epoch : 1738   train_loss : 0.0005\n",
            "epoch : 1738   train_loss : 0.0003\n",
            "epoch : 1738   train_loss : 0.0003\n",
            "epoch : 1738   train_loss : 0.0003\n",
            "epoch : 1738   val_loss : 0.0101\n",
            "epoch : 1738   val_loss : 0.0114\n",
            "epoch : 1739   train_loss : 0.0003\n",
            "epoch : 1739   train_loss : 0.0004\n",
            "epoch : 1739   train_loss : 0.0003\n",
            "epoch : 1739   train_loss : 0.0003\n",
            "epoch : 1739   val_loss : 0.0104\n",
            "epoch : 1739   val_loss : 0.0105\n",
            "epoch : 1740   train_loss : 0.0004\n",
            "epoch : 1740   train_loss : 0.0003\n",
            "epoch : 1740   train_loss : 0.0003\n",
            "epoch : 1740   train_loss : 0.0004\n",
            "epoch : 1740   val_loss : 0.0103\n",
            "epoch : 1740   val_loss : 0.0107\n",
            "epoch : 1741   train_loss : 0.0003\n",
            "epoch : 1741   train_loss : 0.0004\n",
            "epoch : 1741   train_loss : 0.0004\n",
            "epoch : 1741   train_loss : 0.0003\n",
            "epoch : 1741   val_loss : 0.0101\n",
            "epoch : 1741   val_loss : 0.0111\n",
            "epoch : 1742   train_loss : 0.0003\n",
            "epoch : 1742   train_loss : 0.0004\n",
            "epoch : 1742   train_loss : 0.0003\n",
            "epoch : 1742   train_loss : 0.0004\n",
            "epoch : 1742   val_loss : 0.0103\n",
            "epoch : 1742   val_loss : 0.0105\n",
            "epoch : 1743   train_loss : 0.0003\n",
            "epoch : 1743   train_loss : 0.0003\n",
            "epoch : 1743   train_loss : 0.0004\n",
            "epoch : 1743   train_loss : 0.0004\n",
            "epoch : 1743   val_loss : 0.0101\n",
            "epoch : 1743   val_loss : 0.0111\n",
            "epoch : 1744   train_loss : 0.0003\n",
            "epoch : 1744   train_loss : 0.0004\n",
            "epoch : 1744   train_loss : 0.0003\n",
            "epoch : 1744   train_loss : 0.0005\n",
            "epoch : 1744   val_loss : 0.0103\n",
            "epoch : 1744   val_loss : 0.0103\n",
            "epoch : 1745   train_loss : 0.0003\n",
            "epoch : 1745   train_loss : 0.0004\n",
            "epoch : 1745   train_loss : 0.0003\n",
            "epoch : 1745   train_loss : 0.0003\n",
            "epoch : 1745   val_loss : 0.0105\n",
            "epoch : 1745   val_loss : 0.0099\n",
            "epoch : 1746   train_loss : 0.0004\n",
            "epoch : 1746   train_loss : 0.0004\n",
            "epoch : 1746   train_loss : 0.0003\n",
            "epoch : 1746   train_loss : 0.0003\n",
            "epoch : 1746   val_loss : 0.0107\n",
            "epoch : 1746   val_loss : 0.0092\n",
            "epoch : 1747   train_loss : 0.0004\n",
            "epoch : 1747   train_loss : 0.0004\n",
            "epoch : 1747   train_loss : 0.0003\n",
            "epoch : 1747   train_loss : 0.0003\n",
            "epoch : 1747   val_loss : 0.0102\n",
            "epoch : 1747   val_loss : 0.0105\n",
            "epoch : 1748   train_loss : 0.0004\n",
            "epoch : 1748   train_loss : 0.0003\n",
            "epoch : 1748   train_loss : 0.0003\n",
            "epoch : 1748   train_loss : 0.0004\n",
            "epoch : 1748   val_loss : 0.0110\n",
            "epoch : 1748   val_loss : 0.0081\n",
            "new model saved at epoch 1748 with val_loss 0.008128534071147442\n",
            "epoch : 1749   train_loss : 0.0003\n",
            "epoch : 1749   train_loss : 0.0003\n",
            "epoch : 1749   train_loss : 0.0004\n",
            "epoch : 1749   train_loss : 0.0004\n",
            "epoch : 1749   val_loss : 0.0102\n",
            "epoch : 1749   val_loss : 0.0102\n",
            "epoch : 1750   train_loss : 0.0003\n",
            "epoch : 1750   train_loss : 0.0004\n",
            "epoch : 1750   train_loss : 0.0004\n",
            "epoch : 1750   train_loss : 0.0003\n",
            "epoch : 1750   val_loss : 0.0096\n",
            "epoch : 1750   val_loss : 0.0118\n",
            "epoch : 1751   train_loss : 0.0004\n",
            "epoch : 1751   train_loss : 0.0003\n",
            "epoch : 1751   train_loss : 0.0003\n",
            "epoch : 1751   train_loss : 0.0003\n",
            "epoch : 1751   val_loss : 0.0101\n",
            "epoch : 1751   val_loss : 0.0103\n",
            "epoch : 1752   train_loss : 0.0003\n",
            "epoch : 1752   train_loss : 0.0003\n",
            "epoch : 1752   train_loss : 0.0005\n",
            "epoch : 1752   train_loss : 0.0003\n",
            "epoch : 1752   val_loss : 0.0105\n",
            "epoch : 1752   val_loss : 0.0092\n",
            "epoch : 1753   train_loss : 0.0004\n",
            "epoch : 1753   train_loss : 0.0003\n",
            "epoch : 1753   train_loss : 0.0003\n",
            "epoch : 1753   train_loss : 0.0003\n",
            "epoch : 1753   val_loss : 0.0102\n",
            "epoch : 1753   val_loss : 0.0101\n",
            "epoch : 1754   train_loss : 0.0003\n",
            "epoch : 1754   train_loss : 0.0004\n",
            "epoch : 1754   train_loss : 0.0003\n",
            "epoch : 1754   train_loss : 0.0004\n",
            "epoch : 1754   val_loss : 0.0098\n",
            "epoch : 1754   val_loss : 0.0109\n",
            "epoch : 1755   train_loss : 0.0004\n",
            "epoch : 1755   train_loss : 0.0003\n",
            "epoch : 1755   train_loss : 0.0003\n",
            "epoch : 1755   train_loss : 0.0004\n",
            "epoch : 1755   val_loss : 0.0102\n",
            "epoch : 1755   val_loss : 0.0098\n",
            "epoch : 1756   train_loss : 0.0003\n",
            "epoch : 1756   train_loss : 0.0003\n",
            "epoch : 1756   train_loss : 0.0004\n",
            "epoch : 1756   train_loss : 0.0003\n",
            "epoch : 1756   val_loss : 0.0099\n",
            "epoch : 1756   val_loss : 0.0106\n",
            "epoch : 1757   train_loss : 0.0004\n",
            "epoch : 1757   train_loss : 0.0003\n",
            "epoch : 1757   train_loss : 0.0003\n",
            "epoch : 1757   train_loss : 0.0003\n",
            "epoch : 1757   val_loss : 0.0100\n",
            "epoch : 1757   val_loss : 0.0101\n",
            "epoch : 1758   train_loss : 0.0003\n",
            "epoch : 1758   train_loss : 0.0004\n",
            "epoch : 1758   train_loss : 0.0003\n",
            "epoch : 1758   train_loss : 0.0003\n",
            "epoch : 1758   val_loss : 0.0099\n",
            "epoch : 1758   val_loss : 0.0104\n",
            "epoch : 1759   train_loss : 0.0003\n",
            "epoch : 1759   train_loss : 0.0004\n",
            "epoch : 1759   train_loss : 0.0003\n",
            "epoch : 1759   train_loss : 0.0002\n",
            "epoch : 1759   val_loss : 0.0099\n",
            "epoch : 1759   val_loss : 0.0105\n",
            "epoch : 1760   train_loss : 0.0004\n",
            "epoch : 1760   train_loss : 0.0003\n",
            "epoch : 1760   train_loss : 0.0003\n",
            "epoch : 1760   train_loss : 0.0004\n",
            "epoch : 1760   val_loss : 0.0105\n",
            "epoch : 1760   val_loss : 0.0087\n",
            "epoch : 1761   train_loss : 0.0003\n",
            "epoch : 1761   train_loss : 0.0004\n",
            "epoch : 1761   train_loss : 0.0003\n",
            "epoch : 1761   train_loss : 0.0003\n",
            "epoch : 1761   val_loss : 0.0099\n",
            "epoch : 1761   val_loss : 0.0101\n",
            "epoch : 1762   train_loss : 0.0003\n",
            "epoch : 1762   train_loss : 0.0004\n",
            "epoch : 1762   train_loss : 0.0003\n",
            "epoch : 1762   train_loss : 0.0004\n",
            "epoch : 1762   val_loss : 0.0094\n",
            "epoch : 1762   val_loss : 0.0114\n",
            "epoch : 1763   train_loss : 0.0003\n",
            "epoch : 1763   train_loss : 0.0004\n",
            "epoch : 1763   train_loss : 0.0003\n",
            "epoch : 1763   train_loss : 0.0004\n",
            "epoch : 1763   val_loss : 0.0097\n",
            "epoch : 1763   val_loss : 0.0106\n",
            "epoch : 1764   train_loss : 0.0002\n",
            "epoch : 1764   train_loss : 0.0003\n",
            "epoch : 1764   train_loss : 0.0004\n",
            "epoch : 1764   train_loss : 0.0004\n",
            "epoch : 1764   val_loss : 0.0099\n",
            "epoch : 1764   val_loss : 0.0100\n",
            "epoch : 1765   train_loss : 0.0003\n",
            "epoch : 1765   train_loss : 0.0003\n",
            "epoch : 1765   train_loss : 0.0004\n",
            "epoch : 1765   train_loss : 0.0004\n",
            "epoch : 1765   val_loss : 0.0098\n",
            "epoch : 1765   val_loss : 0.0100\n",
            "epoch : 1766   train_loss : 0.0003\n",
            "epoch : 1766   train_loss : 0.0004\n",
            "epoch : 1766   train_loss : 0.0004\n",
            "epoch : 1766   train_loss : 0.0003\n",
            "epoch : 1766   val_loss : 0.0100\n",
            "epoch : 1766   val_loss : 0.0094\n",
            "epoch : 1767   train_loss : 0.0004\n",
            "epoch : 1767   train_loss : 0.0003\n",
            "epoch : 1767   train_loss : 0.0003\n",
            "epoch : 1767   train_loss : 0.0003\n",
            "epoch : 1767   val_loss : 0.0098\n",
            "epoch : 1767   val_loss : 0.0100\n",
            "epoch : 1768   train_loss : 0.0004\n",
            "epoch : 1768   train_loss : 0.0003\n",
            "epoch : 1768   train_loss : 0.0002\n",
            "epoch : 1768   train_loss : 0.0004\n",
            "epoch : 1768   val_loss : 0.0100\n",
            "epoch : 1768   val_loss : 0.0092\n",
            "epoch : 1769   train_loss : 0.0003\n",
            "epoch : 1769   train_loss : 0.0003\n",
            "epoch : 1769   train_loss : 0.0004\n",
            "epoch : 1769   train_loss : 0.0004\n",
            "epoch : 1769   val_loss : 0.0097\n",
            "epoch : 1769   val_loss : 0.0101\n",
            "epoch : 1770   train_loss : 0.0003\n",
            "epoch : 1770   train_loss : 0.0003\n",
            "epoch : 1770   train_loss : 0.0003\n",
            "epoch : 1770   train_loss : 0.0002\n",
            "epoch : 1770   val_loss : 0.0093\n",
            "epoch : 1770   val_loss : 0.0109\n",
            "epoch : 1771   train_loss : 0.0004\n",
            "epoch : 1771   train_loss : 0.0003\n",
            "epoch : 1771   train_loss : 0.0004\n",
            "epoch : 1771   train_loss : 0.0002\n",
            "epoch : 1771   val_loss : 0.0098\n",
            "epoch : 1771   val_loss : 0.0097\n",
            "epoch : 1772   train_loss : 0.0003\n",
            "epoch : 1772   train_loss : 0.0003\n",
            "epoch : 1772   train_loss : 0.0003\n",
            "epoch : 1772   train_loss : 0.0003\n",
            "epoch : 1772   val_loss : 0.0099\n",
            "epoch : 1772   val_loss : 0.0093\n",
            "epoch : 1773   train_loss : 0.0002\n",
            "epoch : 1773   train_loss : 0.0004\n",
            "epoch : 1773   train_loss : 0.0003\n",
            "epoch : 1773   train_loss : 0.0003\n",
            "epoch : 1773   val_loss : 0.0097\n",
            "epoch : 1773   val_loss : 0.0096\n",
            "epoch : 1774   train_loss : 0.0002\n",
            "epoch : 1774   train_loss : 0.0004\n",
            "epoch : 1774   train_loss : 0.0004\n",
            "epoch : 1774   train_loss : 0.0003\n",
            "epoch : 1774   val_loss : 0.0098\n",
            "epoch : 1774   val_loss : 0.0094\n",
            "epoch : 1775   train_loss : 0.0004\n",
            "epoch : 1775   train_loss : 0.0003\n",
            "epoch : 1775   train_loss : 0.0003\n",
            "epoch : 1775   train_loss : 0.0004\n",
            "epoch : 1775   val_loss : 0.0095\n",
            "epoch : 1775   val_loss : 0.0100\n",
            "epoch : 1776   train_loss : 0.0002\n",
            "epoch : 1776   train_loss : 0.0005\n",
            "epoch : 1776   train_loss : 0.0003\n",
            "epoch : 1776   train_loss : 0.0003\n",
            "epoch : 1776   val_loss : 0.0099\n",
            "epoch : 1776   val_loss : 0.0089\n",
            "epoch : 1777   train_loss : 0.0003\n",
            "epoch : 1777   train_loss : 0.0003\n",
            "epoch : 1777   train_loss : 0.0003\n",
            "epoch : 1777   train_loss : 0.0004\n",
            "epoch : 1777   val_loss : 0.0099\n",
            "epoch : 1777   val_loss : 0.0088\n",
            "epoch : 1778   train_loss : 0.0004\n",
            "epoch : 1778   train_loss : 0.0004\n",
            "epoch : 1778   train_loss : 0.0002\n",
            "epoch : 1778   train_loss : 0.0003\n",
            "epoch : 1778   val_loss : 0.0098\n",
            "epoch : 1778   val_loss : 0.0089\n",
            "epoch : 1779   train_loss : 0.0003\n",
            "epoch : 1779   train_loss : 0.0004\n",
            "epoch : 1779   train_loss : 0.0003\n",
            "epoch : 1779   train_loss : 0.0004\n",
            "epoch : 1779   val_loss : 0.0094\n",
            "epoch : 1779   val_loss : 0.0101\n",
            "epoch : 1780   train_loss : 0.0004\n",
            "epoch : 1780   train_loss : 0.0003\n",
            "epoch : 1780   train_loss : 0.0003\n",
            "epoch : 1780   train_loss : 0.0004\n",
            "epoch : 1780   val_loss : 0.0093\n",
            "epoch : 1780   val_loss : 0.0103\n",
            "epoch : 1781   train_loss : 0.0003\n",
            "epoch : 1781   train_loss : 0.0003\n",
            "epoch : 1781   train_loss : 0.0004\n",
            "epoch : 1781   train_loss : 0.0003\n",
            "epoch : 1781   val_loss : 0.0096\n",
            "epoch : 1781   val_loss : 0.0094\n",
            "epoch : 1782   train_loss : 0.0002\n",
            "epoch : 1782   train_loss : 0.0004\n",
            "epoch : 1782   train_loss : 0.0003\n",
            "epoch : 1782   train_loss : 0.0002\n",
            "epoch : 1782   val_loss : 0.0095\n",
            "epoch : 1782   val_loss : 0.0094\n",
            "epoch : 1783   train_loss : 0.0004\n",
            "epoch : 1783   train_loss : 0.0004\n",
            "epoch : 1783   train_loss : 0.0002\n",
            "epoch : 1783   train_loss : 0.0004\n",
            "epoch : 1783   val_loss : 0.0094\n",
            "epoch : 1783   val_loss : 0.0098\n",
            "epoch : 1784   train_loss : 0.0003\n",
            "epoch : 1784   train_loss : 0.0005\n",
            "epoch : 1784   train_loss : 0.0002\n",
            "epoch : 1784   train_loss : 0.0002\n",
            "epoch : 1784   val_loss : 0.0098\n",
            "epoch : 1784   val_loss : 0.0085\n",
            "epoch : 1785   train_loss : 0.0003\n",
            "epoch : 1785   train_loss : 0.0003\n",
            "epoch : 1785   train_loss : 0.0003\n",
            "epoch : 1785   train_loss : 0.0004\n",
            "epoch : 1785   val_loss : 0.0091\n",
            "epoch : 1785   val_loss : 0.0103\n",
            "epoch : 1786   train_loss : 0.0004\n",
            "epoch : 1786   train_loss : 0.0003\n",
            "epoch : 1786   train_loss : 0.0003\n",
            "epoch : 1786   train_loss : 0.0003\n",
            "epoch : 1786   val_loss : 0.0096\n",
            "epoch : 1786   val_loss : 0.0089\n",
            "epoch : 1787   train_loss : 0.0003\n",
            "epoch : 1787   train_loss : 0.0003\n",
            "epoch : 1787   train_loss : 0.0004\n",
            "epoch : 1787   train_loss : 0.0002\n",
            "epoch : 1787   val_loss : 0.0099\n",
            "epoch : 1787   val_loss : 0.0081\n",
            "new model saved at epoch 1787 with val_loss 0.008057606406509876\n",
            "epoch : 1788   train_loss : 0.0004\n",
            "epoch : 1788   train_loss : 0.0003\n",
            "epoch : 1788   train_loss : 0.0003\n",
            "epoch : 1788   train_loss : 0.0003\n",
            "epoch : 1788   val_loss : 0.0092\n",
            "epoch : 1788   val_loss : 0.0099\n",
            "epoch : 1789   train_loss : 0.0003\n",
            "epoch : 1789   train_loss : 0.0003\n",
            "epoch : 1789   train_loss : 0.0003\n",
            "epoch : 1789   train_loss : 0.0003\n",
            "epoch : 1789   val_loss : 0.0096\n",
            "epoch : 1789   val_loss : 0.0087\n",
            "epoch : 1790   train_loss : 0.0003\n",
            "epoch : 1790   train_loss : 0.0003\n",
            "epoch : 1790   train_loss : 0.0003\n",
            "epoch : 1790   train_loss : 0.0003\n",
            "epoch : 1790   val_loss : 0.0090\n",
            "epoch : 1790   val_loss : 0.0101\n",
            "epoch : 1791   train_loss : 0.0003\n",
            "epoch : 1791   train_loss : 0.0003\n",
            "epoch : 1791   train_loss : 0.0004\n",
            "epoch : 1791   train_loss : 0.0003\n",
            "epoch : 1791   val_loss : 0.0096\n",
            "epoch : 1791   val_loss : 0.0086\n",
            "epoch : 1792   train_loss : 0.0003\n",
            "epoch : 1792   train_loss : 0.0004\n",
            "epoch : 1792   train_loss : 0.0003\n",
            "epoch : 1792   train_loss : 0.0002\n",
            "epoch : 1792   val_loss : 0.0093\n",
            "epoch : 1792   val_loss : 0.0094\n",
            "epoch : 1793   train_loss : 0.0003\n",
            "epoch : 1793   train_loss : 0.0003\n",
            "epoch : 1793   train_loss : 0.0003\n",
            "epoch : 1793   train_loss : 0.0003\n",
            "epoch : 1793   val_loss : 0.0090\n",
            "epoch : 1793   val_loss : 0.0100\n",
            "epoch : 1794   train_loss : 0.0004\n",
            "epoch : 1794   train_loss : 0.0003\n",
            "epoch : 1794   train_loss : 0.0003\n",
            "epoch : 1794   train_loss : 0.0002\n",
            "epoch : 1794   val_loss : 0.0091\n",
            "epoch : 1794   val_loss : 0.0096\n",
            "epoch : 1795   train_loss : 0.0004\n",
            "epoch : 1795   train_loss : 0.0003\n",
            "epoch : 1795   train_loss : 0.0002\n",
            "epoch : 1795   train_loss : 0.0003\n",
            "epoch : 1795   val_loss : 0.0087\n",
            "epoch : 1795   val_loss : 0.0107\n",
            "epoch : 1796   train_loss : 0.0003\n",
            "epoch : 1796   train_loss : 0.0003\n",
            "epoch : 1796   train_loss : 0.0004\n",
            "epoch : 1796   train_loss : 0.0004\n",
            "epoch : 1796   val_loss : 0.0090\n",
            "epoch : 1796   val_loss : 0.0098\n",
            "epoch : 1797   train_loss : 0.0004\n",
            "epoch : 1797   train_loss : 0.0003\n",
            "epoch : 1797   train_loss : 0.0002\n",
            "epoch : 1797   train_loss : 0.0003\n",
            "epoch : 1797   val_loss : 0.0093\n",
            "epoch : 1797   val_loss : 0.0088\n",
            "epoch : 1798   train_loss : 0.0003\n",
            "epoch : 1798   train_loss : 0.0003\n",
            "epoch : 1798   train_loss : 0.0003\n",
            "epoch : 1798   train_loss : 0.0004\n",
            "epoch : 1798   val_loss : 0.0094\n",
            "epoch : 1798   val_loss : 0.0087\n",
            "epoch : 1799   train_loss : 0.0004\n",
            "epoch : 1799   train_loss : 0.0003\n",
            "epoch : 1799   train_loss : 0.0003\n",
            "epoch : 1799   train_loss : 0.0003\n",
            "epoch : 1799   val_loss : 0.0092\n",
            "epoch : 1799   val_loss : 0.0090\n",
            "epoch : 1800   train_loss : 0.0003\n",
            "epoch : 1800   train_loss : 0.0003\n",
            "epoch : 1800   train_loss : 0.0003\n",
            "epoch : 1800   train_loss : 0.0003\n",
            "epoch : 1800   val_loss : 0.0087\n",
            "epoch : 1800   val_loss : 0.0103\n",
            "epoch : 1801   train_loss : 0.0003\n",
            "epoch : 1801   train_loss : 0.0002\n",
            "epoch : 1801   train_loss : 0.0004\n",
            "epoch : 1801   train_loss : 0.0003\n",
            "epoch : 1801   val_loss : 0.0089\n",
            "epoch : 1801   val_loss : 0.0098\n",
            "epoch : 1802   train_loss : 0.0003\n",
            "epoch : 1802   train_loss : 0.0003\n",
            "epoch : 1802   train_loss : 0.0004\n",
            "epoch : 1802   train_loss : 0.0003\n",
            "epoch : 1802   val_loss : 0.0093\n",
            "epoch : 1802   val_loss : 0.0086\n",
            "epoch : 1803   train_loss : 0.0004\n",
            "epoch : 1803   train_loss : 0.0002\n",
            "epoch : 1803   train_loss : 0.0004\n",
            "epoch : 1803   train_loss : 0.0003\n",
            "epoch : 1803   val_loss : 0.0092\n",
            "epoch : 1803   val_loss : 0.0089\n",
            "epoch : 1804   train_loss : 0.0003\n",
            "epoch : 1804   train_loss : 0.0003\n",
            "epoch : 1804   train_loss : 0.0003\n",
            "epoch : 1804   train_loss : 0.0003\n",
            "epoch : 1804   val_loss : 0.0094\n",
            "epoch : 1804   val_loss : 0.0081\n",
            "epoch : 1805   train_loss : 0.0004\n",
            "epoch : 1805   train_loss : 0.0002\n",
            "epoch : 1805   train_loss : 0.0003\n",
            "epoch : 1805   train_loss : 0.0002\n",
            "epoch : 1805   val_loss : 0.0091\n",
            "epoch : 1805   val_loss : 0.0088\n",
            "epoch : 1806   train_loss : 0.0002\n",
            "epoch : 1806   train_loss : 0.0003\n",
            "epoch : 1806   train_loss : 0.0005\n",
            "epoch : 1806   train_loss : 0.0002\n",
            "epoch : 1806   val_loss : 0.0090\n",
            "epoch : 1806   val_loss : 0.0090\n",
            "epoch : 1807   train_loss : 0.0003\n",
            "epoch : 1807   train_loss : 0.0003\n",
            "epoch : 1807   train_loss : 0.0003\n",
            "epoch : 1807   train_loss : 0.0004\n",
            "epoch : 1807   val_loss : 0.0093\n",
            "epoch : 1807   val_loss : 0.0083\n",
            "epoch : 1808   train_loss : 0.0003\n",
            "epoch : 1808   train_loss : 0.0002\n",
            "epoch : 1808   train_loss : 0.0004\n",
            "epoch : 1808   train_loss : 0.0003\n",
            "epoch : 1808   val_loss : 0.0087\n",
            "epoch : 1808   val_loss : 0.0096\n",
            "epoch : 1809   train_loss : 0.0004\n",
            "epoch : 1809   train_loss : 0.0003\n",
            "epoch : 1809   train_loss : 0.0003\n",
            "epoch : 1809   train_loss : 0.0003\n",
            "epoch : 1809   val_loss : 0.0090\n",
            "epoch : 1809   val_loss : 0.0089\n",
            "epoch : 1810   train_loss : 0.0003\n",
            "epoch : 1810   train_loss : 0.0003\n",
            "epoch : 1810   train_loss : 0.0004\n",
            "epoch : 1810   train_loss : 0.0003\n",
            "epoch : 1810   val_loss : 0.0091\n",
            "epoch : 1810   val_loss : 0.0084\n",
            "epoch : 1811   train_loss : 0.0003\n",
            "epoch : 1811   train_loss : 0.0002\n",
            "epoch : 1811   train_loss : 0.0004\n",
            "epoch : 1811   train_loss : 0.0002\n",
            "epoch : 1811   val_loss : 0.0090\n",
            "epoch : 1811   val_loss : 0.0087\n",
            "epoch : 1812   train_loss : 0.0004\n",
            "epoch : 1812   train_loss : 0.0003\n",
            "epoch : 1812   train_loss : 0.0002\n",
            "epoch : 1812   train_loss : 0.0003\n",
            "epoch : 1812   val_loss : 0.0086\n",
            "epoch : 1812   val_loss : 0.0097\n",
            "epoch : 1813   train_loss : 0.0003\n",
            "epoch : 1813   train_loss : 0.0003\n",
            "epoch : 1813   train_loss : 0.0002\n",
            "epoch : 1813   train_loss : 0.0003\n",
            "epoch : 1813   val_loss : 0.0090\n",
            "epoch : 1813   val_loss : 0.0087\n",
            "epoch : 1814   train_loss : 0.0003\n",
            "epoch : 1814   train_loss : 0.0003\n",
            "epoch : 1814   train_loss : 0.0003\n",
            "epoch : 1814   train_loss : 0.0002\n",
            "epoch : 1814   val_loss : 0.0087\n",
            "epoch : 1814   val_loss : 0.0093\n",
            "epoch : 1815   train_loss : 0.0002\n",
            "epoch : 1815   train_loss : 0.0003\n",
            "epoch : 1815   train_loss : 0.0003\n",
            "epoch : 1815   train_loss : 0.0004\n",
            "epoch : 1815   val_loss : 0.0087\n",
            "epoch : 1815   val_loss : 0.0092\n",
            "epoch : 1816   train_loss : 0.0003\n",
            "epoch : 1816   train_loss : 0.0003\n",
            "epoch : 1816   train_loss : 0.0004\n",
            "epoch : 1816   train_loss : 0.0003\n",
            "epoch : 1816   val_loss : 0.0087\n",
            "epoch : 1816   val_loss : 0.0090\n",
            "epoch : 1817   train_loss : 0.0003\n",
            "epoch : 1817   train_loss : 0.0003\n",
            "epoch : 1817   train_loss : 0.0003\n",
            "epoch : 1817   train_loss : 0.0003\n",
            "epoch : 1817   val_loss : 0.0089\n",
            "epoch : 1817   val_loss : 0.0086\n",
            "epoch : 1818   train_loss : 0.0003\n",
            "epoch : 1818   train_loss : 0.0003\n",
            "epoch : 1818   train_loss : 0.0003\n",
            "epoch : 1818   train_loss : 0.0002\n",
            "epoch : 1818   val_loss : 0.0086\n",
            "epoch : 1818   val_loss : 0.0093\n",
            "epoch : 1819   train_loss : 0.0003\n",
            "epoch : 1819   train_loss : 0.0004\n",
            "epoch : 1819   train_loss : 0.0002\n",
            "epoch : 1819   train_loss : 0.0003\n",
            "epoch : 1819   val_loss : 0.0090\n",
            "epoch : 1819   val_loss : 0.0082\n",
            "epoch : 1820   train_loss : 0.0003\n",
            "epoch : 1820   train_loss : 0.0003\n",
            "epoch : 1820   train_loss : 0.0003\n",
            "epoch : 1820   train_loss : 0.0003\n",
            "epoch : 1820   val_loss : 0.0085\n",
            "epoch : 1820   val_loss : 0.0095\n",
            "epoch : 1821   train_loss : 0.0003\n",
            "epoch : 1821   train_loss : 0.0003\n",
            "epoch : 1821   train_loss : 0.0003\n",
            "epoch : 1821   train_loss : 0.0002\n",
            "epoch : 1821   val_loss : 0.0090\n",
            "epoch : 1821   val_loss : 0.0080\n",
            "new model saved at epoch 1821 with val_loss 0.00802051555365324\n",
            "epoch : 1822   train_loss : 0.0004\n",
            "epoch : 1822   train_loss : 0.0002\n",
            "epoch : 1822   train_loss : 0.0002\n",
            "epoch : 1822   train_loss : 0.0003\n",
            "epoch : 1822   val_loss : 0.0087\n",
            "epoch : 1822   val_loss : 0.0087\n",
            "epoch : 1823   train_loss : 0.0003\n",
            "epoch : 1823   train_loss : 0.0003\n",
            "epoch : 1823   train_loss : 0.0004\n",
            "epoch : 1823   train_loss : 0.0002\n",
            "epoch : 1823   val_loss : 0.0084\n",
            "epoch : 1823   val_loss : 0.0093\n",
            "epoch : 1824   train_loss : 0.0003\n",
            "epoch : 1824   train_loss : 0.0003\n",
            "epoch : 1824   train_loss : 0.0003\n",
            "epoch : 1824   train_loss : 0.0003\n",
            "epoch : 1824   val_loss : 0.0086\n",
            "epoch : 1824   val_loss : 0.0088\n",
            "epoch : 1825   train_loss : 0.0003\n",
            "epoch : 1825   train_loss : 0.0003\n",
            "epoch : 1825   train_loss : 0.0003\n",
            "epoch : 1825   train_loss : 0.0002\n",
            "epoch : 1825   val_loss : 0.0089\n",
            "epoch : 1825   val_loss : 0.0080\n",
            "epoch : 1826   train_loss : 0.0002\n",
            "epoch : 1826   train_loss : 0.0003\n",
            "epoch : 1826   train_loss : 0.0004\n",
            "epoch : 1826   train_loss : 0.0002\n",
            "epoch : 1826   val_loss : 0.0087\n",
            "epoch : 1826   val_loss : 0.0083\n",
            "epoch : 1827   train_loss : 0.0002\n",
            "epoch : 1827   train_loss : 0.0003\n",
            "epoch : 1827   train_loss : 0.0004\n",
            "epoch : 1827   train_loss : 0.0003\n",
            "epoch : 1827   val_loss : 0.0086\n",
            "epoch : 1827   val_loss : 0.0087\n",
            "epoch : 1828   train_loss : 0.0002\n",
            "epoch : 1828   train_loss : 0.0003\n",
            "epoch : 1828   train_loss : 0.0003\n",
            "epoch : 1828   train_loss : 0.0002\n",
            "epoch : 1828   val_loss : 0.0084\n",
            "epoch : 1828   val_loss : 0.0091\n",
            "epoch : 1829   train_loss : 0.0003\n",
            "epoch : 1829   train_loss : 0.0003\n",
            "epoch : 1829   train_loss : 0.0003\n",
            "epoch : 1829   train_loss : 0.0003\n",
            "epoch : 1829   val_loss : 0.0087\n",
            "epoch : 1829   val_loss : 0.0082\n",
            "epoch : 1830   train_loss : 0.0002\n",
            "epoch : 1830   train_loss : 0.0002\n",
            "epoch : 1830   train_loss : 0.0004\n",
            "epoch : 1830   train_loss : 0.0004\n",
            "epoch : 1830   val_loss : 0.0087\n",
            "epoch : 1830   val_loss : 0.0081\n",
            "epoch : 1831   train_loss : 0.0003\n",
            "epoch : 1831   train_loss : 0.0002\n",
            "epoch : 1831   train_loss : 0.0003\n",
            "epoch : 1831   train_loss : 0.0004\n",
            "epoch : 1831   val_loss : 0.0086\n",
            "epoch : 1831   val_loss : 0.0083\n",
            "epoch : 1832   train_loss : 0.0003\n",
            "epoch : 1832   train_loss : 0.0003\n",
            "epoch : 1832   train_loss : 0.0003\n",
            "epoch : 1832   train_loss : 0.0003\n",
            "epoch : 1832   val_loss : 0.0084\n",
            "epoch : 1832   val_loss : 0.0086\n",
            "epoch : 1833   train_loss : 0.0003\n",
            "epoch : 1833   train_loss : 0.0003\n",
            "epoch : 1833   train_loss : 0.0002\n",
            "epoch : 1833   train_loss : 0.0002\n",
            "epoch : 1833   val_loss : 0.0082\n",
            "epoch : 1833   val_loss : 0.0093\n",
            "epoch : 1834   train_loss : 0.0003\n",
            "epoch : 1834   train_loss : 0.0004\n",
            "epoch : 1834   train_loss : 0.0002\n",
            "epoch : 1834   train_loss : 0.0002\n",
            "epoch : 1834   val_loss : 0.0081\n",
            "epoch : 1834   val_loss : 0.0093\n",
            "epoch : 1835   train_loss : 0.0002\n",
            "epoch : 1835   train_loss : 0.0004\n",
            "epoch : 1835   train_loss : 0.0003\n",
            "epoch : 1835   train_loss : 0.0002\n",
            "epoch : 1835   val_loss : 0.0082\n",
            "epoch : 1835   val_loss : 0.0091\n",
            "epoch : 1836   train_loss : 0.0003\n",
            "epoch : 1836   train_loss : 0.0004\n",
            "epoch : 1836   train_loss : 0.0002\n",
            "epoch : 1836   train_loss : 0.0003\n",
            "epoch : 1836   val_loss : 0.0079\n",
            "epoch : 1836   val_loss : 0.0098\n",
            "epoch : 1837   train_loss : 0.0002\n",
            "epoch : 1837   train_loss : 0.0003\n",
            "epoch : 1837   train_loss : 0.0003\n",
            "epoch : 1837   train_loss : 0.0002\n",
            "epoch : 1837   val_loss : 0.0083\n",
            "epoch : 1837   val_loss : 0.0087\n",
            "epoch : 1838   train_loss : 0.0003\n",
            "epoch : 1838   train_loss : 0.0003\n",
            "epoch : 1838   train_loss : 0.0003\n",
            "epoch : 1838   train_loss : 0.0002\n",
            "epoch : 1838   val_loss : 0.0084\n",
            "epoch : 1838   val_loss : 0.0083\n",
            "epoch : 1839   train_loss : 0.0003\n",
            "epoch : 1839   train_loss : 0.0004\n",
            "epoch : 1839   train_loss : 0.0003\n",
            "epoch : 1839   train_loss : 0.0002\n",
            "epoch : 1839   val_loss : 0.0083\n",
            "epoch : 1839   val_loss : 0.0085\n",
            "epoch : 1840   train_loss : 0.0003\n",
            "epoch : 1840   train_loss : 0.0002\n",
            "epoch : 1840   train_loss : 0.0003\n",
            "epoch : 1840   train_loss : 0.0004\n",
            "epoch : 1840   val_loss : 0.0084\n",
            "epoch : 1840   val_loss : 0.0081\n",
            "epoch : 1841   train_loss : 0.0002\n",
            "epoch : 1841   train_loss : 0.0002\n",
            "epoch : 1841   train_loss : 0.0004\n",
            "epoch : 1841   train_loss : 0.0004\n",
            "epoch : 1841   val_loss : 0.0083\n",
            "epoch : 1841   val_loss : 0.0083\n",
            "epoch : 1842   train_loss : 0.0003\n",
            "epoch : 1842   train_loss : 0.0003\n",
            "epoch : 1842   train_loss : 0.0003\n",
            "epoch : 1842   train_loss : 0.0003\n",
            "epoch : 1842   val_loss : 0.0085\n",
            "epoch : 1842   val_loss : 0.0077\n",
            "new model saved at epoch 1842 with val_loss 0.007691396400332451\n",
            "epoch : 1843   train_loss : 0.0003\n",
            "epoch : 1843   train_loss : 0.0002\n",
            "epoch : 1843   train_loss : 0.0003\n",
            "epoch : 1843   train_loss : 0.0003\n",
            "epoch : 1843   val_loss : 0.0084\n",
            "epoch : 1843   val_loss : 0.0081\n",
            "epoch : 1844   train_loss : 0.0003\n",
            "epoch : 1844   train_loss : 0.0002\n",
            "epoch : 1844   train_loss : 0.0004\n",
            "epoch : 1844   train_loss : 0.0003\n",
            "epoch : 1844   val_loss : 0.0085\n",
            "epoch : 1844   val_loss : 0.0076\n",
            "new model saved at epoch 1844 with val_loss 0.007640609052032232\n",
            "epoch : 1845   train_loss : 0.0003\n",
            "epoch : 1845   train_loss : 0.0003\n",
            "epoch : 1845   train_loss : 0.0002\n",
            "epoch : 1845   train_loss : 0.0004\n",
            "epoch : 1845   val_loss : 0.0081\n",
            "epoch : 1845   val_loss : 0.0086\n",
            "epoch : 1846   train_loss : 0.0004\n",
            "epoch : 1846   train_loss : 0.0002\n",
            "epoch : 1846   train_loss : 0.0003\n",
            "epoch : 1846   train_loss : 0.0003\n",
            "epoch : 1846   val_loss : 0.0083\n",
            "epoch : 1846   val_loss : 0.0081\n",
            "epoch : 1847   train_loss : 0.0002\n",
            "epoch : 1847   train_loss : 0.0003\n",
            "epoch : 1847   train_loss : 0.0003\n",
            "epoch : 1847   train_loss : 0.0002\n",
            "epoch : 1847   val_loss : 0.0080\n",
            "epoch : 1847   val_loss : 0.0088\n",
            "epoch : 1848   train_loss : 0.0003\n",
            "epoch : 1848   train_loss : 0.0003\n",
            "epoch : 1848   train_loss : 0.0003\n",
            "epoch : 1848   train_loss : 0.0002\n",
            "epoch : 1848   val_loss : 0.0084\n",
            "epoch : 1848   val_loss : 0.0076\n",
            "new model saved at epoch 1848 with val_loss 0.007555069401860237\n",
            "epoch : 1849   train_loss : 0.0003\n",
            "epoch : 1849   train_loss : 0.0003\n",
            "epoch : 1849   train_loss : 0.0002\n",
            "epoch : 1849   train_loss : 0.0003\n",
            "epoch : 1849   val_loss : 0.0080\n",
            "epoch : 1849   val_loss : 0.0086\n",
            "epoch : 1850   train_loss : 0.0002\n",
            "epoch : 1850   train_loss : 0.0003\n",
            "epoch : 1850   train_loss : 0.0002\n",
            "epoch : 1850   train_loss : 0.0004\n",
            "epoch : 1850   val_loss : 0.0084\n",
            "epoch : 1850   val_loss : 0.0076\n",
            "epoch : 1851   train_loss : 0.0004\n",
            "epoch : 1851   train_loss : 0.0002\n",
            "epoch : 1851   train_loss : 0.0002\n",
            "epoch : 1851   train_loss : 0.0002\n",
            "epoch : 1851   val_loss : 0.0078\n",
            "epoch : 1851   val_loss : 0.0090\n",
            "epoch : 1852   train_loss : 0.0003\n",
            "epoch : 1852   train_loss : 0.0002\n",
            "epoch : 1852   train_loss : 0.0003\n",
            "epoch : 1852   train_loss : 0.0003\n",
            "epoch : 1852   val_loss : 0.0078\n",
            "epoch : 1852   val_loss : 0.0089\n",
            "epoch : 1853   train_loss : 0.0003\n",
            "epoch : 1853   train_loss : 0.0003\n",
            "epoch : 1853   train_loss : 0.0003\n",
            "epoch : 1853   train_loss : 0.0003\n",
            "epoch : 1853   val_loss : 0.0081\n",
            "epoch : 1853   val_loss : 0.0081\n",
            "epoch : 1854   train_loss : 0.0003\n",
            "epoch : 1854   train_loss : 0.0003\n",
            "epoch : 1854   train_loss : 0.0003\n",
            "epoch : 1854   train_loss : 0.0003\n",
            "epoch : 1854   val_loss : 0.0080\n",
            "epoch : 1854   val_loss : 0.0083\n",
            "epoch : 1855   train_loss : 0.0003\n",
            "epoch : 1855   train_loss : 0.0003\n",
            "epoch : 1855   train_loss : 0.0002\n",
            "epoch : 1855   train_loss : 0.0003\n",
            "epoch : 1855   val_loss : 0.0079\n",
            "epoch : 1855   val_loss : 0.0084\n",
            "epoch : 1856   train_loss : 0.0003\n",
            "epoch : 1856   train_loss : 0.0002\n",
            "epoch : 1856   train_loss : 0.0003\n",
            "epoch : 1856   train_loss : 0.0004\n",
            "epoch : 1856   val_loss : 0.0083\n",
            "epoch : 1856   val_loss : 0.0075\n",
            "new model saved at epoch 1856 with val_loss 0.007496231235563755\n",
            "epoch : 1857   train_loss : 0.0003\n",
            "epoch : 1857   train_loss : 0.0003\n",
            "epoch : 1857   train_loss : 0.0003\n",
            "epoch : 1857   train_loss : 0.0003\n",
            "epoch : 1857   val_loss : 0.0083\n",
            "epoch : 1857   val_loss : 0.0072\n",
            "new model saved at epoch 1857 with val_loss 0.007202954031527042\n",
            "epoch : 1858   train_loss : 0.0003\n",
            "epoch : 1858   train_loss : 0.0002\n",
            "epoch : 1858   train_loss : 0.0003\n",
            "epoch : 1858   train_loss : 0.0003\n",
            "epoch : 1858   val_loss : 0.0082\n",
            "epoch : 1858   val_loss : 0.0076\n",
            "epoch : 1859   train_loss : 0.0003\n",
            "epoch : 1859   train_loss : 0.0003\n",
            "epoch : 1859   train_loss : 0.0003\n",
            "epoch : 1859   train_loss : 0.0002\n",
            "epoch : 1859   val_loss : 0.0079\n",
            "epoch : 1859   val_loss : 0.0083\n",
            "epoch : 1860   train_loss : 0.0003\n",
            "epoch : 1860   train_loss : 0.0002\n",
            "epoch : 1860   train_loss : 0.0003\n",
            "epoch : 1860   train_loss : 0.0003\n",
            "epoch : 1860   val_loss : 0.0076\n",
            "epoch : 1860   val_loss : 0.0091\n",
            "epoch : 1861   train_loss : 0.0002\n",
            "epoch : 1861   train_loss : 0.0003\n",
            "epoch : 1861   train_loss : 0.0003\n",
            "epoch : 1861   train_loss : 0.0003\n",
            "epoch : 1861   val_loss : 0.0078\n",
            "epoch : 1861   val_loss : 0.0084\n",
            "epoch : 1862   train_loss : 0.0002\n",
            "epoch : 1862   train_loss : 0.0003\n",
            "epoch : 1862   train_loss : 0.0003\n",
            "epoch : 1862   train_loss : 0.0003\n",
            "epoch : 1862   val_loss : 0.0080\n",
            "epoch : 1862   val_loss : 0.0078\n",
            "epoch : 1863   train_loss : 0.0003\n",
            "epoch : 1863   train_loss : 0.0003\n",
            "epoch : 1863   train_loss : 0.0002\n",
            "epoch : 1863   train_loss : 0.0004\n",
            "epoch : 1863   val_loss : 0.0081\n",
            "epoch : 1863   val_loss : 0.0075\n",
            "epoch : 1864   train_loss : 0.0003\n",
            "epoch : 1864   train_loss : 0.0003\n",
            "epoch : 1864   train_loss : 0.0003\n",
            "epoch : 1864   train_loss : 0.0002\n",
            "epoch : 1864   val_loss : 0.0081\n",
            "epoch : 1864   val_loss : 0.0074\n",
            "epoch : 1865   train_loss : 0.0002\n",
            "epoch : 1865   train_loss : 0.0003\n",
            "epoch : 1865   train_loss : 0.0003\n",
            "epoch : 1865   train_loss : 0.0003\n",
            "epoch : 1865   val_loss : 0.0075\n",
            "epoch : 1865   val_loss : 0.0091\n",
            "epoch : 1866   train_loss : 0.0003\n",
            "epoch : 1866   train_loss : 0.0003\n",
            "epoch : 1866   train_loss : 0.0002\n",
            "epoch : 1866   train_loss : 0.0003\n",
            "epoch : 1866   val_loss : 0.0078\n",
            "epoch : 1866   val_loss : 0.0081\n",
            "epoch : 1867   train_loss : 0.0003\n",
            "epoch : 1867   train_loss : 0.0002\n",
            "epoch : 1867   train_loss : 0.0002\n",
            "epoch : 1867   train_loss : 0.0004\n",
            "epoch : 1867   val_loss : 0.0077\n",
            "epoch : 1867   val_loss : 0.0084\n",
            "epoch : 1868   train_loss : 0.0003\n",
            "epoch : 1868   train_loss : 0.0002\n",
            "epoch : 1868   train_loss : 0.0002\n",
            "epoch : 1868   train_loss : 0.0004\n",
            "epoch : 1868   val_loss : 0.0081\n",
            "epoch : 1868   val_loss : 0.0073\n",
            "epoch : 1869   train_loss : 0.0003\n",
            "epoch : 1869   train_loss : 0.0002\n",
            "epoch : 1869   train_loss : 0.0003\n",
            "epoch : 1869   train_loss : 0.0003\n",
            "epoch : 1869   val_loss : 0.0076\n",
            "epoch : 1869   val_loss : 0.0085\n",
            "epoch : 1870   train_loss : 0.0003\n",
            "epoch : 1870   train_loss : 0.0002\n",
            "epoch : 1870   train_loss : 0.0003\n",
            "epoch : 1870   train_loss : 0.0002\n",
            "epoch : 1870   val_loss : 0.0076\n",
            "epoch : 1870   val_loss : 0.0084\n",
            "epoch : 1871   train_loss : 0.0003\n",
            "epoch : 1871   train_loss : 0.0003\n",
            "epoch : 1871   train_loss : 0.0002\n",
            "epoch : 1871   train_loss : 0.0003\n",
            "epoch : 1871   val_loss : 0.0080\n",
            "epoch : 1871   val_loss : 0.0073\n",
            "epoch : 1872   train_loss : 0.0003\n",
            "epoch : 1872   train_loss : 0.0003\n",
            "epoch : 1872   train_loss : 0.0003\n",
            "epoch : 1872   train_loss : 0.0002\n",
            "epoch : 1872   val_loss : 0.0078\n",
            "epoch : 1872   val_loss : 0.0076\n",
            "epoch : 1873   train_loss : 0.0003\n",
            "epoch : 1873   train_loss : 0.0003\n",
            "epoch : 1873   train_loss : 0.0003\n",
            "epoch : 1873   train_loss : 0.0003\n",
            "epoch : 1873   val_loss : 0.0079\n",
            "epoch : 1873   val_loss : 0.0075\n",
            "epoch : 1874   train_loss : 0.0002\n",
            "epoch : 1874   train_loss : 0.0003\n",
            "epoch : 1874   train_loss : 0.0003\n",
            "epoch : 1874   train_loss : 0.0003\n",
            "epoch : 1874   val_loss : 0.0078\n",
            "epoch : 1874   val_loss : 0.0077\n",
            "epoch : 1875   train_loss : 0.0002\n",
            "epoch : 1875   train_loss : 0.0002\n",
            "epoch : 1875   train_loss : 0.0003\n",
            "epoch : 1875   train_loss : 0.0004\n",
            "epoch : 1875   val_loss : 0.0083\n",
            "epoch : 1875   val_loss : 0.0063\n",
            "new model saved at epoch 1875 with val_loss 0.006250393111258745\n",
            "epoch : 1876   train_loss : 0.0003\n",
            "epoch : 1876   train_loss : 0.0003\n",
            "epoch : 1876   train_loss : 0.0002\n",
            "epoch : 1876   train_loss : 0.0002\n",
            "epoch : 1876   val_loss : 0.0078\n",
            "epoch : 1876   val_loss : 0.0074\n",
            "epoch : 1877   train_loss : 0.0003\n",
            "epoch : 1877   train_loss : 0.0003\n",
            "epoch : 1877   train_loss : 0.0002\n",
            "epoch : 1877   train_loss : 0.0004\n",
            "epoch : 1877   val_loss : 0.0074\n",
            "epoch : 1877   val_loss : 0.0084\n",
            "epoch : 1878   train_loss : 0.0004\n",
            "epoch : 1878   train_loss : 0.0002\n",
            "epoch : 1878   train_loss : 0.0002\n",
            "epoch : 1878   train_loss : 0.0003\n",
            "epoch : 1878   val_loss : 0.0076\n",
            "epoch : 1878   val_loss : 0.0080\n",
            "epoch : 1879   train_loss : 0.0003\n",
            "epoch : 1879   train_loss : 0.0003\n",
            "epoch : 1879   train_loss : 0.0002\n",
            "epoch : 1879   train_loss : 0.0003\n",
            "epoch : 1879   val_loss : 0.0078\n",
            "epoch : 1879   val_loss : 0.0072\n",
            "epoch : 1880   train_loss : 0.0003\n",
            "epoch : 1880   train_loss : 0.0004\n",
            "epoch : 1880   train_loss : 0.0002\n",
            "epoch : 1880   train_loss : 0.0002\n",
            "epoch : 1880   val_loss : 0.0075\n",
            "epoch : 1880   val_loss : 0.0081\n",
            "epoch : 1881   train_loss : 0.0002\n",
            "epoch : 1881   train_loss : 0.0003\n",
            "epoch : 1881   train_loss : 0.0003\n",
            "epoch : 1881   train_loss : 0.0002\n",
            "epoch : 1881   val_loss : 0.0082\n",
            "epoch : 1881   val_loss : 0.0061\n",
            "new model saved at epoch 1881 with val_loss 0.0060727475211024284\n",
            "epoch : 1882   train_loss : 0.0003\n",
            "epoch : 1882   train_loss : 0.0003\n",
            "epoch : 1882   train_loss : 0.0002\n",
            "epoch : 1882   train_loss : 0.0003\n",
            "epoch : 1882   val_loss : 0.0074\n",
            "epoch : 1882   val_loss : 0.0081\n",
            "epoch : 1883   train_loss : 0.0003\n",
            "epoch : 1883   train_loss : 0.0002\n",
            "epoch : 1883   train_loss : 0.0003\n",
            "epoch : 1883   train_loss : 0.0003\n",
            "epoch : 1883   val_loss : 0.0074\n",
            "epoch : 1883   val_loss : 0.0081\n",
            "epoch : 1884   train_loss : 0.0002\n",
            "epoch : 1884   train_loss : 0.0002\n",
            "epoch : 1884   train_loss : 0.0003\n",
            "epoch : 1884   train_loss : 0.0002\n",
            "epoch : 1884   val_loss : 0.0073\n",
            "epoch : 1884   val_loss : 0.0082\n",
            "epoch : 1885   train_loss : 0.0002\n",
            "epoch : 1885   train_loss : 0.0002\n",
            "epoch : 1885   train_loss : 0.0003\n",
            "epoch : 1885   train_loss : 0.0003\n",
            "epoch : 1885   val_loss : 0.0076\n",
            "epoch : 1885   val_loss : 0.0075\n",
            "epoch : 1886   train_loss : 0.0002\n",
            "epoch : 1886   train_loss : 0.0003\n",
            "epoch : 1886   train_loss : 0.0003\n",
            "epoch : 1886   train_loss : 0.0003\n",
            "epoch : 1886   val_loss : 0.0076\n",
            "epoch : 1886   val_loss : 0.0073\n",
            "epoch : 1887   train_loss : 0.0002\n",
            "epoch : 1887   train_loss : 0.0003\n",
            "epoch : 1887   train_loss : 0.0002\n",
            "epoch : 1887   train_loss : 0.0003\n",
            "epoch : 1887   val_loss : 0.0074\n",
            "epoch : 1887   val_loss : 0.0078\n",
            "epoch : 1888   train_loss : 0.0003\n",
            "epoch : 1888   train_loss : 0.0003\n",
            "epoch : 1888   train_loss : 0.0003\n",
            "epoch : 1888   train_loss : 0.0003\n",
            "epoch : 1888   val_loss : 0.0079\n",
            "epoch : 1888   val_loss : 0.0065\n",
            "epoch : 1889   train_loss : 0.0002\n",
            "epoch : 1889   train_loss : 0.0002\n",
            "epoch : 1889   train_loss : 0.0003\n",
            "epoch : 1889   train_loss : 0.0003\n",
            "epoch : 1889   val_loss : 0.0076\n",
            "epoch : 1889   val_loss : 0.0073\n",
            "epoch : 1890   train_loss : 0.0003\n",
            "epoch : 1890   train_loss : 0.0003\n",
            "epoch : 1890   train_loss : 0.0003\n",
            "epoch : 1890   train_loss : 0.0003\n",
            "epoch : 1890   val_loss : 0.0072\n",
            "epoch : 1890   val_loss : 0.0083\n",
            "epoch : 1891   train_loss : 0.0003\n",
            "epoch : 1891   train_loss : 0.0002\n",
            "epoch : 1891   train_loss : 0.0003\n",
            "epoch : 1891   train_loss : 0.0002\n",
            "epoch : 1891   val_loss : 0.0075\n",
            "epoch : 1891   val_loss : 0.0072\n",
            "epoch : 1892   train_loss : 0.0002\n",
            "epoch : 1892   train_loss : 0.0003\n",
            "epoch : 1892   train_loss : 0.0003\n",
            "epoch : 1892   train_loss : 0.0002\n",
            "epoch : 1892   val_loss : 0.0073\n",
            "epoch : 1892   val_loss : 0.0077\n",
            "epoch : 1893   train_loss : 0.0002\n",
            "epoch : 1893   train_loss : 0.0003\n",
            "epoch : 1893   train_loss : 0.0004\n",
            "epoch : 1893   train_loss : 0.0002\n",
            "epoch : 1893   val_loss : 0.0076\n",
            "epoch : 1893   val_loss : 0.0070\n",
            "epoch : 1894   train_loss : 0.0002\n",
            "epoch : 1894   train_loss : 0.0002\n",
            "epoch : 1894   train_loss : 0.0004\n",
            "epoch : 1894   train_loss : 0.0002\n",
            "epoch : 1894   val_loss : 0.0071\n",
            "epoch : 1894   val_loss : 0.0081\n",
            "epoch : 1895   train_loss : 0.0002\n",
            "epoch : 1895   train_loss : 0.0003\n",
            "epoch : 1895   train_loss : 0.0003\n",
            "epoch : 1895   train_loss : 0.0003\n",
            "epoch : 1895   val_loss : 0.0073\n",
            "epoch : 1895   val_loss : 0.0077\n",
            "epoch : 1896   train_loss : 0.0003\n",
            "epoch : 1896   train_loss : 0.0003\n",
            "epoch : 1896   train_loss : 0.0002\n",
            "epoch : 1896   train_loss : 0.0002\n",
            "epoch : 1896   val_loss : 0.0073\n",
            "epoch : 1896   val_loss : 0.0075\n",
            "epoch : 1897   train_loss : 0.0002\n",
            "epoch : 1897   train_loss : 0.0003\n",
            "epoch : 1897   train_loss : 0.0003\n",
            "epoch : 1897   train_loss : 0.0003\n",
            "epoch : 1897   val_loss : 0.0071\n",
            "epoch : 1897   val_loss : 0.0079\n",
            "epoch : 1898   train_loss : 0.0003\n",
            "epoch : 1898   train_loss : 0.0003\n",
            "epoch : 1898   train_loss : 0.0002\n",
            "epoch : 1898   train_loss : 0.0002\n",
            "epoch : 1898   val_loss : 0.0073\n",
            "epoch : 1898   val_loss : 0.0076\n",
            "epoch : 1899   train_loss : 0.0003\n",
            "epoch : 1899   train_loss : 0.0002\n",
            "epoch : 1899   train_loss : 0.0003\n",
            "epoch : 1899   train_loss : 0.0003\n",
            "epoch : 1899   val_loss : 0.0071\n",
            "epoch : 1899   val_loss : 0.0079\n",
            "epoch : 1900   train_loss : 0.0003\n",
            "epoch : 1900   train_loss : 0.0002\n",
            "epoch : 1900   train_loss : 0.0003\n",
            "epoch : 1900   train_loss : 0.0002\n",
            "epoch : 1900   val_loss : 0.0072\n",
            "epoch : 1900   val_loss : 0.0076\n",
            "epoch : 1901   train_loss : 0.0003\n",
            "epoch : 1901   train_loss : 0.0003\n",
            "epoch : 1901   train_loss : 0.0002\n",
            "epoch : 1901   train_loss : 0.0003\n",
            "epoch : 1901   val_loss : 0.0072\n",
            "epoch : 1901   val_loss : 0.0075\n",
            "epoch : 1902   train_loss : 0.0002\n",
            "epoch : 1902   train_loss : 0.0003\n",
            "epoch : 1902   train_loss : 0.0003\n",
            "epoch : 1902   train_loss : 0.0003\n",
            "epoch : 1902   val_loss : 0.0071\n",
            "epoch : 1902   val_loss : 0.0077\n",
            "epoch : 1903   train_loss : 0.0003\n",
            "epoch : 1903   train_loss : 0.0002\n",
            "epoch : 1903   train_loss : 0.0003\n",
            "epoch : 1903   train_loss : 0.0002\n",
            "epoch : 1903   val_loss : 0.0072\n",
            "epoch : 1903   val_loss : 0.0073\n",
            "epoch : 1904   train_loss : 0.0002\n",
            "epoch : 1904   train_loss : 0.0003\n",
            "epoch : 1904   train_loss : 0.0002\n",
            "epoch : 1904   train_loss : 0.0002\n",
            "epoch : 1904   val_loss : 0.0069\n",
            "epoch : 1904   val_loss : 0.0081\n",
            "epoch : 1905   train_loss : 0.0002\n",
            "epoch : 1905   train_loss : 0.0003\n",
            "epoch : 1905   train_loss : 0.0002\n",
            "epoch : 1905   train_loss : 0.0003\n",
            "epoch : 1905   val_loss : 0.0068\n",
            "epoch : 1905   val_loss : 0.0084\n",
            "epoch : 1906   train_loss : 0.0003\n",
            "epoch : 1906   train_loss : 0.0003\n",
            "epoch : 1906   train_loss : 0.0002\n",
            "epoch : 1906   train_loss : 0.0002\n",
            "epoch : 1906   val_loss : 0.0074\n",
            "epoch : 1906   val_loss : 0.0068\n",
            "epoch : 1907   train_loss : 0.0002\n",
            "epoch : 1907   train_loss : 0.0003\n",
            "epoch : 1907   train_loss : 0.0003\n",
            "epoch : 1907   train_loss : 0.0002\n",
            "epoch : 1907   val_loss : 0.0076\n",
            "epoch : 1907   val_loss : 0.0061\n",
            "epoch : 1908   train_loss : 0.0003\n",
            "epoch : 1908   train_loss : 0.0003\n",
            "epoch : 1908   train_loss : 0.0002\n",
            "epoch : 1908   train_loss : 0.0002\n",
            "epoch : 1908   val_loss : 0.0069\n",
            "epoch : 1908   val_loss : 0.0080\n",
            "epoch : 1909   train_loss : 0.0002\n",
            "epoch : 1909   train_loss : 0.0003\n",
            "epoch : 1909   train_loss : 0.0002\n",
            "epoch : 1909   train_loss : 0.0002\n",
            "epoch : 1909   val_loss : 0.0074\n",
            "epoch : 1909   val_loss : 0.0065\n",
            "epoch : 1910   train_loss : 0.0003\n",
            "epoch : 1910   train_loss : 0.0003\n",
            "epoch : 1910   train_loss : 0.0003\n",
            "epoch : 1910   train_loss : 0.0002\n",
            "epoch : 1910   val_loss : 0.0072\n",
            "epoch : 1910   val_loss : 0.0071\n",
            "epoch : 1911   train_loss : 0.0003\n",
            "epoch : 1911   train_loss : 0.0003\n",
            "epoch : 1911   train_loss : 0.0002\n",
            "epoch : 1911   train_loss : 0.0002\n",
            "epoch : 1911   val_loss : 0.0073\n",
            "epoch : 1911   val_loss : 0.0066\n",
            "epoch : 1912   train_loss : 0.0002\n",
            "epoch : 1912   train_loss : 0.0002\n",
            "epoch : 1912   train_loss : 0.0003\n",
            "epoch : 1912   train_loss : 0.0003\n",
            "epoch : 1912   val_loss : 0.0072\n",
            "epoch : 1912   val_loss : 0.0069\n",
            "epoch : 1913   train_loss : 0.0003\n",
            "epoch : 1913   train_loss : 0.0003\n",
            "epoch : 1913   train_loss : 0.0002\n",
            "epoch : 1913   train_loss : 0.0002\n",
            "epoch : 1913   val_loss : 0.0073\n",
            "epoch : 1913   val_loss : 0.0065\n",
            "epoch : 1914   train_loss : 0.0003\n",
            "epoch : 1914   train_loss : 0.0002\n",
            "epoch : 1914   train_loss : 0.0002\n",
            "epoch : 1914   train_loss : 0.0002\n",
            "epoch : 1914   val_loss : 0.0070\n",
            "epoch : 1914   val_loss : 0.0073\n",
            "epoch : 1915   train_loss : 0.0002\n",
            "epoch : 1915   train_loss : 0.0002\n",
            "epoch : 1915   train_loss : 0.0002\n",
            "epoch : 1915   train_loss : 0.0004\n",
            "epoch : 1915   val_loss : 0.0070\n",
            "epoch : 1915   val_loss : 0.0072\n",
            "epoch : 1916   train_loss : 0.0003\n",
            "epoch : 1916   train_loss : 0.0002\n",
            "epoch : 1916   train_loss : 0.0002\n",
            "epoch : 1916   train_loss : 0.0003\n",
            "epoch : 1916   val_loss : 0.0075\n",
            "epoch : 1916   val_loss : 0.0060\n",
            "new model saved at epoch 1916 with val_loss 0.006015365943312645\n",
            "epoch : 1917   train_loss : 0.0003\n",
            "epoch : 1917   train_loss : 0.0002\n",
            "epoch : 1917   train_loss : 0.0003\n",
            "epoch : 1917   train_loss : 0.0002\n",
            "epoch : 1917   val_loss : 0.0073\n",
            "epoch : 1917   val_loss : 0.0064\n",
            "epoch : 1918   train_loss : 0.0003\n",
            "epoch : 1918   train_loss : 0.0003\n",
            "epoch : 1918   train_loss : 0.0002\n",
            "epoch : 1918   train_loss : 0.0002\n",
            "epoch : 1918   val_loss : 0.0070\n",
            "epoch : 1918   val_loss : 0.0072\n",
            "epoch : 1919   train_loss : 0.0003\n",
            "epoch : 1919   train_loss : 0.0002\n",
            "epoch : 1919   train_loss : 0.0003\n",
            "epoch : 1919   train_loss : 0.0002\n",
            "epoch : 1919   val_loss : 0.0072\n",
            "epoch : 1919   val_loss : 0.0065\n",
            "epoch : 1920   train_loss : 0.0002\n",
            "epoch : 1920   train_loss : 0.0003\n",
            "epoch : 1920   train_loss : 0.0003\n",
            "epoch : 1920   train_loss : 0.0002\n",
            "epoch : 1920   val_loss : 0.0073\n",
            "epoch : 1920   val_loss : 0.0062\n",
            "epoch : 1921   train_loss : 0.0003\n",
            "epoch : 1921   train_loss : 0.0003\n",
            "epoch : 1921   train_loss : 0.0002\n",
            "epoch : 1921   train_loss : 0.0002\n",
            "epoch : 1921   val_loss : 0.0069\n",
            "epoch : 1921   val_loss : 0.0073\n",
            "epoch : 1922   train_loss : 0.0003\n",
            "epoch : 1922   train_loss : 0.0002\n",
            "epoch : 1922   train_loss : 0.0002\n",
            "epoch : 1922   train_loss : 0.0002\n",
            "epoch : 1922   val_loss : 0.0068\n",
            "epoch : 1922   val_loss : 0.0074\n",
            "epoch : 1923   train_loss : 0.0003\n",
            "epoch : 1923   train_loss : 0.0002\n",
            "epoch : 1923   train_loss : 0.0003\n",
            "epoch : 1923   train_loss : 0.0002\n",
            "epoch : 1923   val_loss : 0.0071\n",
            "epoch : 1923   val_loss : 0.0066\n",
            "epoch : 1924   train_loss : 0.0003\n",
            "epoch : 1924   train_loss : 0.0002\n",
            "epoch : 1924   train_loss : 0.0002\n",
            "epoch : 1924   train_loss : 0.0003\n",
            "epoch : 1924   val_loss : 0.0070\n",
            "epoch : 1924   val_loss : 0.0068\n",
            "epoch : 1925   train_loss : 0.0002\n",
            "epoch : 1925   train_loss : 0.0004\n",
            "epoch : 1925   train_loss : 0.0002\n",
            "epoch : 1925   train_loss : 0.0002\n",
            "epoch : 1925   val_loss : 0.0068\n",
            "epoch : 1925   val_loss : 0.0073\n",
            "epoch : 1926   train_loss : 0.0002\n",
            "epoch : 1926   train_loss : 0.0003\n",
            "epoch : 1926   train_loss : 0.0003\n",
            "epoch : 1926   train_loss : 0.0002\n",
            "epoch : 1926   val_loss : 0.0069\n",
            "epoch : 1926   val_loss : 0.0068\n",
            "epoch : 1927   train_loss : 0.0003\n",
            "epoch : 1927   train_loss : 0.0002\n",
            "epoch : 1927   train_loss : 0.0003\n",
            "epoch : 1927   train_loss : 0.0002\n",
            "epoch : 1927   val_loss : 0.0071\n",
            "epoch : 1927   val_loss : 0.0062\n",
            "epoch : 1928   train_loss : 0.0002\n",
            "epoch : 1928   train_loss : 0.0003\n",
            "epoch : 1928   train_loss : 0.0002\n",
            "epoch : 1928   train_loss : 0.0002\n",
            "epoch : 1928   val_loss : 0.0069\n",
            "epoch : 1928   val_loss : 0.0069\n",
            "epoch : 1929   train_loss : 0.0002\n",
            "epoch : 1929   train_loss : 0.0003\n",
            "epoch : 1929   train_loss : 0.0002\n",
            "epoch : 1929   train_loss : 0.0002\n",
            "epoch : 1929   val_loss : 0.0070\n",
            "epoch : 1929   val_loss : 0.0064\n",
            "epoch : 1930   train_loss : 0.0002\n",
            "epoch : 1930   train_loss : 0.0002\n",
            "epoch : 1930   train_loss : 0.0002\n",
            "epoch : 1930   train_loss : 0.0004\n",
            "epoch : 1930   val_loss : 0.0068\n",
            "epoch : 1930   val_loss : 0.0070\n",
            "epoch : 1931   train_loss : 0.0002\n",
            "epoch : 1931   train_loss : 0.0002\n",
            "epoch : 1931   train_loss : 0.0002\n",
            "epoch : 1931   train_loss : 0.0003\n",
            "epoch : 1931   val_loss : 0.0070\n",
            "epoch : 1931   val_loss : 0.0064\n",
            "epoch : 1932   train_loss : 0.0003\n",
            "epoch : 1932   train_loss : 0.0002\n",
            "epoch : 1932   train_loss : 0.0002\n",
            "epoch : 1932   train_loss : 0.0004\n",
            "epoch : 1932   val_loss : 0.0070\n",
            "epoch : 1932   val_loss : 0.0064\n",
            "epoch : 1933   train_loss : 0.0002\n",
            "epoch : 1933   train_loss : 0.0003\n",
            "epoch : 1933   train_loss : 0.0003\n",
            "epoch : 1933   train_loss : 0.0002\n",
            "epoch : 1933   val_loss : 0.0066\n",
            "epoch : 1933   val_loss : 0.0073\n",
            "epoch : 1934   train_loss : 0.0002\n",
            "epoch : 1934   train_loss : 0.0002\n",
            "epoch : 1934   train_loss : 0.0003\n",
            "epoch : 1934   train_loss : 0.0003\n",
            "epoch : 1934   val_loss : 0.0069\n",
            "epoch : 1934   val_loss : 0.0066\n",
            "epoch : 1935   train_loss : 0.0003\n",
            "epoch : 1935   train_loss : 0.0003\n",
            "epoch : 1935   train_loss : 0.0002\n",
            "epoch : 1935   train_loss : 0.0002\n",
            "epoch : 1935   val_loss : 0.0068\n",
            "epoch : 1935   val_loss : 0.0066\n",
            "epoch : 1936   train_loss : 0.0002\n",
            "epoch : 1936   train_loss : 0.0003\n",
            "epoch : 1936   train_loss : 0.0003\n",
            "epoch : 1936   train_loss : 0.0002\n",
            "epoch : 1936   val_loss : 0.0066\n",
            "epoch : 1936   val_loss : 0.0071\n",
            "epoch : 1937   train_loss : 0.0002\n",
            "epoch : 1937   train_loss : 0.0003\n",
            "epoch : 1937   train_loss : 0.0002\n",
            "epoch : 1937   train_loss : 0.0003\n",
            "epoch : 1937   val_loss : 0.0071\n",
            "epoch : 1937   val_loss : 0.0057\n",
            "new model saved at epoch 1937 with val_loss 0.005735537502914667\n",
            "epoch : 1938   train_loss : 0.0003\n",
            "epoch : 1938   train_loss : 0.0002\n",
            "epoch : 1938   train_loss : 0.0002\n",
            "epoch : 1938   train_loss : 0.0003\n",
            "epoch : 1938   val_loss : 0.0068\n",
            "epoch : 1938   val_loss : 0.0066\n",
            "epoch : 1939   train_loss : 0.0003\n",
            "epoch : 1939   train_loss : 0.0002\n",
            "epoch : 1939   train_loss : 0.0002\n",
            "epoch : 1939   train_loss : 0.0003\n",
            "epoch : 1939   val_loss : 0.0066\n",
            "epoch : 1939   val_loss : 0.0070\n",
            "epoch : 1940   train_loss : 0.0002\n",
            "epoch : 1940   train_loss : 0.0003\n",
            "epoch : 1940   train_loss : 0.0002\n",
            "epoch : 1940   train_loss : 0.0003\n",
            "epoch : 1940   val_loss : 0.0066\n",
            "epoch : 1940   val_loss : 0.0069\n",
            "epoch : 1941   train_loss : 0.0002\n",
            "epoch : 1941   train_loss : 0.0003\n",
            "epoch : 1941   train_loss : 0.0003\n",
            "epoch : 1941   train_loss : 0.0002\n",
            "epoch : 1941   val_loss : 0.0063\n",
            "epoch : 1941   val_loss : 0.0079\n",
            "epoch : 1942   train_loss : 0.0003\n",
            "epoch : 1942   train_loss : 0.0002\n",
            "epoch : 1942   train_loss : 0.0002\n",
            "epoch : 1942   train_loss : 0.0003\n",
            "epoch : 1942   val_loss : 0.0065\n",
            "epoch : 1942   val_loss : 0.0070\n",
            "epoch : 1943   train_loss : 0.0003\n",
            "epoch : 1943   train_loss : 0.0003\n",
            "epoch : 1943   train_loss : 0.0002\n",
            "epoch : 1943   train_loss : 0.0002\n",
            "epoch : 1943   val_loss : 0.0069\n",
            "epoch : 1943   val_loss : 0.0061\n",
            "epoch : 1944   train_loss : 0.0002\n",
            "epoch : 1944   train_loss : 0.0003\n",
            "epoch : 1944   train_loss : 0.0002\n",
            "epoch : 1944   train_loss : 0.0002\n",
            "epoch : 1944   val_loss : 0.0065\n",
            "epoch : 1944   val_loss : 0.0070\n",
            "epoch : 1945   train_loss : 0.0003\n",
            "epoch : 1945   train_loss : 0.0002\n",
            "epoch : 1945   train_loss : 0.0003\n",
            "epoch : 1945   train_loss : 0.0003\n",
            "epoch : 1945   val_loss : 0.0065\n",
            "epoch : 1945   val_loss : 0.0069\n",
            "epoch : 1946   train_loss : 0.0002\n",
            "epoch : 1946   train_loss : 0.0003\n",
            "epoch : 1946   train_loss : 0.0003\n",
            "epoch : 1946   train_loss : 0.0002\n",
            "epoch : 1946   val_loss : 0.0064\n",
            "epoch : 1946   val_loss : 0.0071\n",
            "epoch : 1947   train_loss : 0.0002\n",
            "epoch : 1947   train_loss : 0.0003\n",
            "epoch : 1947   train_loss : 0.0002\n",
            "epoch : 1947   train_loss : 0.0002\n",
            "epoch : 1947   val_loss : 0.0067\n",
            "epoch : 1947   val_loss : 0.0063\n",
            "epoch : 1948   train_loss : 0.0003\n",
            "epoch : 1948   train_loss : 0.0002\n",
            "epoch : 1948   train_loss : 0.0002\n",
            "epoch : 1948   train_loss : 0.0002\n",
            "epoch : 1948   val_loss : 0.0068\n",
            "epoch : 1948   val_loss : 0.0060\n",
            "epoch : 1949   train_loss : 0.0003\n",
            "epoch : 1949   train_loss : 0.0002\n",
            "epoch : 1949   train_loss : 0.0002\n",
            "epoch : 1949   train_loss : 0.0002\n",
            "epoch : 1949   val_loss : 0.0065\n",
            "epoch : 1949   val_loss : 0.0068\n",
            "epoch : 1950   train_loss : 0.0002\n",
            "epoch : 1950   train_loss : 0.0002\n",
            "epoch : 1950   train_loss : 0.0003\n",
            "epoch : 1950   train_loss : 0.0003\n",
            "epoch : 1950   val_loss : 0.0065\n",
            "epoch : 1950   val_loss : 0.0068\n",
            "epoch : 1951   train_loss : 0.0002\n",
            "epoch : 1951   train_loss : 0.0002\n",
            "epoch : 1951   train_loss : 0.0003\n",
            "epoch : 1951   train_loss : 0.0002\n",
            "epoch : 1951   val_loss : 0.0070\n",
            "epoch : 1951   val_loss : 0.0055\n",
            "new model saved at epoch 1951 with val_loss 0.0054869079031050205\n",
            "epoch : 1952   train_loss : 0.0003\n",
            "epoch : 1952   train_loss : 0.0002\n",
            "epoch : 1952   train_loss : 0.0002\n",
            "epoch : 1952   train_loss : 0.0002\n",
            "epoch : 1952   val_loss : 0.0067\n",
            "epoch : 1952   val_loss : 0.0062\n",
            "epoch : 1953   train_loss : 0.0002\n",
            "epoch : 1953   train_loss : 0.0003\n",
            "epoch : 1953   train_loss : 0.0002\n",
            "epoch : 1953   train_loss : 0.0002\n",
            "epoch : 1953   val_loss : 0.0063\n",
            "epoch : 1953   val_loss : 0.0071\n",
            "epoch : 1954   train_loss : 0.0002\n",
            "epoch : 1954   train_loss : 0.0003\n",
            "epoch : 1954   train_loss : 0.0003\n",
            "epoch : 1954   train_loss : 0.0002\n",
            "epoch : 1954   val_loss : 0.0064\n",
            "epoch : 1954   val_loss : 0.0068\n",
            "epoch : 1955   train_loss : 0.0002\n",
            "epoch : 1955   train_loss : 0.0003\n",
            "epoch : 1955   train_loss : 0.0002\n",
            "epoch : 1955   train_loss : 0.0002\n",
            "epoch : 1955   val_loss : 0.0067\n",
            "epoch : 1955   val_loss : 0.0061\n",
            "epoch : 1956   train_loss : 0.0003\n",
            "epoch : 1956   train_loss : 0.0002\n",
            "epoch : 1956   train_loss : 0.0002\n",
            "epoch : 1956   train_loss : 0.0003\n",
            "epoch : 1956   val_loss : 0.0063\n",
            "epoch : 1956   val_loss : 0.0069\n",
            "epoch : 1957   train_loss : 0.0002\n",
            "epoch : 1957   train_loss : 0.0002\n",
            "epoch : 1957   train_loss : 0.0003\n",
            "epoch : 1957   train_loss : 0.0003\n",
            "epoch : 1957   val_loss : 0.0068\n",
            "epoch : 1957   val_loss : 0.0055\n",
            "new model saved at epoch 1957 with val_loss 0.005451183300465345\n",
            "epoch : 1958   train_loss : 0.0002\n",
            "epoch : 1958   train_loss : 0.0003\n",
            "epoch : 1958   train_loss : 0.0003\n",
            "epoch : 1958   train_loss : 0.0002\n",
            "epoch : 1958   val_loss : 0.0065\n",
            "epoch : 1958   val_loss : 0.0065\n",
            "epoch : 1959   train_loss : 0.0002\n",
            "epoch : 1959   train_loss : 0.0002\n",
            "epoch : 1959   train_loss : 0.0003\n",
            "epoch : 1959   train_loss : 0.0002\n",
            "epoch : 1959   val_loss : 0.0062\n",
            "epoch : 1959   val_loss : 0.0072\n",
            "epoch : 1960   train_loss : 0.0002\n",
            "epoch : 1960   train_loss : 0.0003\n",
            "epoch : 1960   train_loss : 0.0002\n",
            "epoch : 1960   train_loss : 0.0002\n",
            "epoch : 1960   val_loss : 0.0064\n",
            "epoch : 1960   val_loss : 0.0065\n",
            "epoch : 1961   train_loss : 0.0003\n",
            "epoch : 1961   train_loss : 0.0002\n",
            "epoch : 1961   train_loss : 0.0002\n",
            "epoch : 1961   train_loss : 0.0002\n",
            "epoch : 1961   val_loss : 0.0064\n",
            "epoch : 1961   val_loss : 0.0065\n",
            "epoch : 1962   train_loss : 0.0002\n",
            "epoch : 1962   train_loss : 0.0002\n",
            "epoch : 1962   train_loss : 0.0002\n",
            "epoch : 1962   train_loss : 0.0004\n",
            "epoch : 1962   val_loss : 0.0066\n",
            "epoch : 1962   val_loss : 0.0059\n",
            "epoch : 1963   train_loss : 0.0002\n",
            "epoch : 1963   train_loss : 0.0002\n",
            "epoch : 1963   train_loss : 0.0002\n",
            "epoch : 1963   train_loss : 0.0002\n",
            "epoch : 1963   val_loss : 0.0062\n",
            "epoch : 1963   val_loss : 0.0070\n",
            "epoch : 1964   train_loss : 0.0002\n",
            "epoch : 1964   train_loss : 0.0003\n",
            "epoch : 1964   train_loss : 0.0002\n",
            "epoch : 1964   train_loss : 0.0002\n",
            "epoch : 1964   val_loss : 0.0064\n",
            "epoch : 1964   val_loss : 0.0063\n",
            "epoch : 1965   train_loss : 0.0002\n",
            "epoch : 1965   train_loss : 0.0002\n",
            "epoch : 1965   train_loss : 0.0003\n",
            "epoch : 1965   train_loss : 0.0002\n",
            "epoch : 1965   val_loss : 0.0062\n",
            "epoch : 1965   val_loss : 0.0067\n",
            "epoch : 1966   train_loss : 0.0002\n",
            "epoch : 1966   train_loss : 0.0002\n",
            "epoch : 1966   train_loss : 0.0003\n",
            "epoch : 1966   train_loss : 0.0003\n",
            "epoch : 1966   val_loss : 0.0062\n",
            "epoch : 1966   val_loss : 0.0068\n",
            "epoch : 1967   train_loss : 0.0003\n",
            "epoch : 1967   train_loss : 0.0002\n",
            "epoch : 1967   train_loss : 0.0002\n",
            "epoch : 1967   train_loss : 0.0002\n",
            "epoch : 1967   val_loss : 0.0064\n",
            "epoch : 1967   val_loss : 0.0062\n",
            "epoch : 1968   train_loss : 0.0003\n",
            "epoch : 1968   train_loss : 0.0002\n",
            "epoch : 1968   train_loss : 0.0002\n",
            "epoch : 1968   train_loss : 0.0002\n",
            "epoch : 1968   val_loss : 0.0064\n",
            "epoch : 1968   val_loss : 0.0062\n",
            "epoch : 1969   train_loss : 0.0003\n",
            "epoch : 1969   train_loss : 0.0003\n",
            "epoch : 1969   train_loss : 0.0002\n",
            "epoch : 1969   train_loss : 0.0002\n",
            "epoch : 1969   val_loss : 0.0066\n",
            "epoch : 1969   val_loss : 0.0055\n",
            "epoch : 1970   train_loss : 0.0003\n",
            "epoch : 1970   train_loss : 0.0003\n",
            "epoch : 1970   train_loss : 0.0002\n",
            "epoch : 1970   train_loss : 0.0002\n",
            "epoch : 1970   val_loss : 0.0064\n",
            "epoch : 1970   val_loss : 0.0060\n",
            "epoch : 1971   train_loss : 0.0002\n",
            "epoch : 1971   train_loss : 0.0002\n",
            "epoch : 1971   train_loss : 0.0003\n",
            "epoch : 1971   train_loss : 0.0002\n",
            "epoch : 1971   val_loss : 0.0061\n",
            "epoch : 1971   val_loss : 0.0067\n",
            "epoch : 1972   train_loss : 0.0003\n",
            "epoch : 1972   train_loss : 0.0002\n",
            "epoch : 1972   train_loss : 0.0002\n",
            "epoch : 1972   train_loss : 0.0002\n",
            "epoch : 1972   val_loss : 0.0060\n",
            "epoch : 1972   val_loss : 0.0070\n",
            "epoch : 1973   train_loss : 0.0003\n",
            "epoch : 1973   train_loss : 0.0002\n",
            "epoch : 1973   train_loss : 0.0002\n",
            "epoch : 1973   train_loss : 0.0002\n",
            "epoch : 1973   val_loss : 0.0063\n",
            "epoch : 1973   val_loss : 0.0060\n",
            "epoch : 1974   train_loss : 0.0002\n",
            "epoch : 1974   train_loss : 0.0003\n",
            "epoch : 1974   train_loss : 0.0002\n",
            "epoch : 1974   train_loss : 0.0003\n",
            "epoch : 1974   val_loss : 0.0063\n",
            "epoch : 1974   val_loss : 0.0061\n",
            "epoch : 1975   train_loss : 0.0002\n",
            "epoch : 1975   train_loss : 0.0003\n",
            "epoch : 1975   train_loss : 0.0002\n",
            "epoch : 1975   train_loss : 0.0003\n",
            "epoch : 1975   val_loss : 0.0059\n",
            "epoch : 1975   val_loss : 0.0071\n",
            "epoch : 1976   train_loss : 0.0002\n",
            "epoch : 1976   train_loss : 0.0002\n",
            "epoch : 1976   train_loss : 0.0003\n",
            "epoch : 1976   train_loss : 0.0002\n",
            "epoch : 1976   val_loss : 0.0064\n",
            "epoch : 1976   val_loss : 0.0058\n",
            "epoch : 1977   train_loss : 0.0003\n",
            "epoch : 1977   train_loss : 0.0002\n",
            "epoch : 1977   train_loss : 0.0002\n",
            "epoch : 1977   train_loss : 0.0002\n",
            "epoch : 1977   val_loss : 0.0065\n",
            "epoch : 1977   val_loss : 0.0053\n",
            "new model saved at epoch 1977 with val_loss 0.005282444879412651\n",
            "epoch : 1978   train_loss : 0.0002\n",
            "epoch : 1978   train_loss : 0.0002\n",
            "epoch : 1978   train_loss : 0.0003\n",
            "epoch : 1978   train_loss : 0.0001\n",
            "epoch : 1978   val_loss : 0.0063\n",
            "epoch : 1978   val_loss : 0.0057\n",
            "epoch : 1979   train_loss : 0.0002\n",
            "epoch : 1979   train_loss : 0.0002\n",
            "epoch : 1979   train_loss : 0.0002\n",
            "epoch : 1979   train_loss : 0.0003\n",
            "epoch : 1979   val_loss : 0.0062\n",
            "epoch : 1979   val_loss : 0.0060\n",
            "epoch : 1980   train_loss : 0.0002\n",
            "epoch : 1980   train_loss : 0.0002\n",
            "epoch : 1980   train_loss : 0.0002\n",
            "epoch : 1980   train_loss : 0.0002\n",
            "epoch : 1980   val_loss : 0.0059\n",
            "epoch : 1980   val_loss : 0.0068\n",
            "epoch : 1981   train_loss : 0.0003\n",
            "epoch : 1981   train_loss : 0.0003\n",
            "epoch : 1981   train_loss : 0.0002\n",
            "epoch : 1981   train_loss : 0.0002\n",
            "epoch : 1981   val_loss : 0.0058\n",
            "epoch : 1981   val_loss : 0.0070\n",
            "epoch : 1982   train_loss : 0.0002\n",
            "epoch : 1982   train_loss : 0.0002\n",
            "epoch : 1982   train_loss : 0.0002\n",
            "epoch : 1982   train_loss : 0.0003\n",
            "epoch : 1982   val_loss : 0.0060\n",
            "epoch : 1982   val_loss : 0.0064\n",
            "epoch : 1983   train_loss : 0.0003\n",
            "epoch : 1983   train_loss : 0.0002\n",
            "epoch : 1983   train_loss : 0.0002\n",
            "epoch : 1983   train_loss : 0.0002\n",
            "epoch : 1983   val_loss : 0.0057\n",
            "epoch : 1983   val_loss : 0.0072\n",
            "epoch : 1984   train_loss : 0.0002\n",
            "epoch : 1984   train_loss : 0.0002\n",
            "epoch : 1984   train_loss : 0.0003\n",
            "epoch : 1984   train_loss : 0.0002\n",
            "epoch : 1984   val_loss : 0.0061\n",
            "epoch : 1984   val_loss : 0.0060\n",
            "epoch : 1985   train_loss : 0.0003\n",
            "epoch : 1985   train_loss : 0.0002\n",
            "epoch : 1985   train_loss : 0.0002\n",
            "epoch : 1985   train_loss : 0.0002\n",
            "epoch : 1985   val_loss : 0.0060\n",
            "epoch : 1985   val_loss : 0.0063\n",
            "epoch : 1986   train_loss : 0.0002\n",
            "epoch : 1986   train_loss : 0.0003\n",
            "epoch : 1986   train_loss : 0.0002\n",
            "epoch : 1986   train_loss : 0.0003\n",
            "epoch : 1986   val_loss : 0.0061\n",
            "epoch : 1986   val_loss : 0.0060\n",
            "epoch : 1987   train_loss : 0.0002\n",
            "epoch : 1987   train_loss : 0.0002\n",
            "epoch : 1987   train_loss : 0.0003\n",
            "epoch : 1987   train_loss : 0.0002\n",
            "epoch : 1987   val_loss : 0.0060\n",
            "epoch : 1987   val_loss : 0.0063\n",
            "epoch : 1988   train_loss : 0.0002\n",
            "epoch : 1988   train_loss : 0.0002\n",
            "epoch : 1988   train_loss : 0.0003\n",
            "epoch : 1988   train_loss : 0.0002\n",
            "epoch : 1988   val_loss : 0.0059\n",
            "epoch : 1988   val_loss : 0.0064\n",
            "epoch : 1989   train_loss : 0.0002\n",
            "epoch : 1989   train_loss : 0.0002\n",
            "epoch : 1989   train_loss : 0.0002\n",
            "epoch : 1989   train_loss : 0.0002\n",
            "epoch : 1989   val_loss : 0.0062\n",
            "epoch : 1989   val_loss : 0.0056\n",
            "epoch : 1990   train_loss : 0.0002\n",
            "epoch : 1990   train_loss : 0.0003\n",
            "epoch : 1990   train_loss : 0.0002\n",
            "epoch : 1990   train_loss : 0.0002\n",
            "epoch : 1990   val_loss : 0.0058\n",
            "epoch : 1990   val_loss : 0.0067\n",
            "epoch : 1991   train_loss : 0.0002\n",
            "epoch : 1991   train_loss : 0.0003\n",
            "epoch : 1991   train_loss : 0.0002\n",
            "epoch : 1991   train_loss : 0.0002\n",
            "epoch : 1991   val_loss : 0.0060\n",
            "epoch : 1991   val_loss : 0.0061\n",
            "epoch : 1992   train_loss : 0.0002\n",
            "epoch : 1992   train_loss : 0.0002\n",
            "epoch : 1992   train_loss : 0.0003\n",
            "epoch : 1992   train_loss : 0.0002\n",
            "epoch : 1992   val_loss : 0.0061\n",
            "epoch : 1992   val_loss : 0.0056\n",
            "epoch : 1993   train_loss : 0.0002\n",
            "epoch : 1993   train_loss : 0.0002\n",
            "epoch : 1993   train_loss : 0.0003\n",
            "epoch : 1993   train_loss : 0.0002\n",
            "epoch : 1993   val_loss : 0.0060\n",
            "epoch : 1993   val_loss : 0.0060\n",
            "epoch : 1994   train_loss : 0.0002\n",
            "epoch : 1994   train_loss : 0.0002\n",
            "epoch : 1994   train_loss : 0.0003\n",
            "epoch : 1994   train_loss : 0.0002\n",
            "epoch : 1994   val_loss : 0.0058\n",
            "epoch : 1994   val_loss : 0.0065\n",
            "epoch : 1995   train_loss : 0.0002\n",
            "epoch : 1995   train_loss : 0.0002\n",
            "epoch : 1995   train_loss : 0.0002\n",
            "epoch : 1995   train_loss : 0.0002\n",
            "epoch : 1995   val_loss : 0.0060\n",
            "epoch : 1995   val_loss : 0.0059\n",
            "epoch : 1996   train_loss : 0.0002\n",
            "epoch : 1996   train_loss : 0.0002\n",
            "epoch : 1996   train_loss : 0.0003\n",
            "epoch : 1996   train_loss : 0.0002\n",
            "epoch : 1996   val_loss : 0.0060\n",
            "epoch : 1996   val_loss : 0.0058\n",
            "epoch : 1997   train_loss : 0.0003\n",
            "epoch : 1997   train_loss : 0.0002\n",
            "epoch : 1997   train_loss : 0.0002\n",
            "epoch : 1997   train_loss : 0.0002\n",
            "epoch : 1997   val_loss : 0.0058\n",
            "epoch : 1997   val_loss : 0.0062\n",
            "epoch : 1998   train_loss : 0.0002\n",
            "epoch : 1998   train_loss : 0.0002\n",
            "epoch : 1998   train_loss : 0.0003\n",
            "epoch : 1998   train_loss : 0.0002\n",
            "epoch : 1998   val_loss : 0.0058\n",
            "epoch : 1998   val_loss : 0.0063\n",
            "epoch : 1999   train_loss : 0.0002\n",
            "epoch : 1999   train_loss : 0.0002\n",
            "epoch : 1999   train_loss : 0.0002\n",
            "epoch : 1999   train_loss : 0.0002\n",
            "epoch : 1999   val_loss : 0.0060\n",
            "epoch : 1999   val_loss : 0.0055\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s83ixR1AY-3f",
        "outputId": "e61f48e6-a9c6-4a7e-d468-1d8564c7ec6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "Loss=np.array(Loss)\n",
        "plt.plot(Step,Loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f4cbb704d68>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAPS9pHkZxif"
      },
      "source": [
        "rnn = LSTM()\n",
        "\n",
        "rnn = torch.load('weights/rnn.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAgb22HMSt-K",
        "outputId": "13955326-850c-41b0-a946-45e031424b80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        }
      },
      "source": [
        "generate_data_train0 = []\n",
        "generate_data_train1 = []\n",
        "generate_data_train2 = []\n",
        "generate_data_train3 = []\n",
        "generate_data_train4 = []\n",
        "generate_data_test = []\n",
        "\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "    x = X_train[i]\n",
        "    x = torch.unsqueeze(x, dim=0)\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cuda()\n",
        "\n",
        "    y = torch.squeeze(rnn(x))\n",
        "    generate_data_train0.append(torch.squeeze(y.cpu()).detach().numpy()[0][0] * np.std(np.array(df['Open'])) + (np.mean(np.array(df['Open']))))\n",
        "    generate_data_train1.append(torch.squeeze(y.cpu()).detach().numpy()[0][1] * np.std(np.array(df['High'])) + (np.mean(np.array(df['High']))))\n",
        "    generate_data_train2.append(torch.squeeze(y.cpu()).detach().numpy()[0][2] * np.std(np.array(df['Low'])) + (np.mean(np.array(df['Low']))))\n",
        "    generate_data_train3.append(torch.squeeze(y.cpu()).detach().numpy()[0][3] * np.std(np.array(df['Close'])) + (np.mean(np.array(df['Close']))))\n",
        "    generate_data_train4.append(torch.squeeze(y.cpu()).detach().numpy()[0][0] * np.std(np.array(df['Volume'])) + (np.mean(np.array(df['Volume']))))       \n",
        "generate_data_train = np.array(generate_data_train)\n",
        "print(generate_data_train.shape)\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(df.index[7:907], generate_data_train0, 'b', label='generate_train0')\n",
        "plt.plot(df.index[7:907], generate_data_train1, 'b', label='generate_train1')\n",
        "plt.plot(df.index[7:907], generate_data_train2, 'b', label='generate_train2')\n",
        "plt.plot(df.index[7:907], generate_data_train3, 'b', label='generate_train3')\n",
        "plt.plot(df.index[7:907], df[7:907].drop(['Volume'],axis=1), 'r', label='real_data')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(900,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAswAAAHSCAYAAAD45Z1sAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzde1hVZdr48e8STdyAguCpRDDNwBNyGvAwdLBEmzygVOo7k8xM6ojVNO+vJp3XKU28ppQpx97eKU3USavxkCSMTkTiIc2EzYApmFuTUjygKMoWNsf1+2Ptvdmcj8rB+3NdXmvtZz1r7WdBwc297/U8iqqqCCGEEEIIIWrWqbUHIIQQQgghRFsmAbMQQgghhBB1kIBZCCGEEEKIOkjALIQQQgghRB0kYBZCCCGEEKIOEjALIYQQQghRh86tPYC6uLm5qZ6enq09DCGEEEII0cHp9fqrqqr2qulYmw6YPT09SUlJae1hCCGEEEKIDk5RlB9rOyYlGUIIIYQQQtRBAmYhhBBCCCHqIAGzEEIIIYQQdWjTNcxCCCGEEG1BSUkJ58+fx2QytfZQRDPZ29vTv39/unTp0uBzJGAWQgghhKjH+fPncXJywtPTE0VRWns4oolUVSU3N5fz588zcODABp8nJRlCCCGEEPUwmUy4urpKsNzOKYqCq6troz8pkIBZCCGEEKIBJFjuGJryfZSAWQghhBBCiDpIwCyEEEIIIRpl9erVFBQU3NFzX3vtNRITE+vso6oqL774IoMHD2bkyJGkpqY2aYxVScAshBBCCCEqUVWV8vLyWo/froC5rKys1vPeeOMNHnvssTqvvWfPHgwGAwaDgbVr17JgwYImjbEqmSVDCCGEEKIRXnoJ0tJa9pqjRsHq1fX3W758OZs3b6ZXr164u7vj7+9PWFgYCxcu5MqVK+h0OtatW4eXlxcRERF0796dlJQULl26xMqVKwkPDwdg1apVbN26laKiIsLCwli2bBlZWVmEhoYSFBSEXq9n9+7dvPnmmyQnJ1NYWEh4eDjLli1jzZo1XLhwgUceeQQ3NzeSkpJISEjg9ddfp6ioiEGDBrFhwwYcHR2rjb+mcx0dHZk/fz6JiYm899577N27l7i4OAoLCxkzZgwffPABiqIQERHBk08+SXh4OJ6ensyZM4e4uDhKSkrYtm0bXl5efP755zz77LMoikJwcDB5eXlcvHiRfv36Nev7IxlmIYQQQoh2IDk5mR07dpCens6ePXtISUkBYN68ebz77rvo9Xqio6OJjIy0nnPx4kW+/vpr4uPjWbRoEQAJCQkYDAaOHj1KWloaer2eAwcOAGAwGIiMjOTEiRN4eHiwYsUKUlJSOHbsGPv37+fYsWO8+OKL3HvvvSQlJZGUlMTVq1eJiooiMTGR1NRUAgICePvtt2u8h6rnAty6dYugoCDS09MZN24czz//PMnJyRw/fpzCwkLi4+NrvJabmxupqaksWLCA6OhoALKzs3F3d7f26d+/P9nZ2c38ykuGWQghhBCiURqSCb4dDh06xNSpU7G3t8fe3p7JkydjMpk4fPgwTz31lLVfUVGRdX/atGl06tSJoUOHcvnyZUALmBMSEvD19QXAaDRiMBgYMGAAHh4eBAcHW8/funUra9eupbS0lIsXL5KRkcHIkSMrjevIkSNkZGQwduxYAIqLixk9enSD78vOzo4ZM2ZYXyclJbFy5UoKCgq4du0aw4YNY/LkydXOmz59OgD+/v589tlnDX6/ppCAWQghhBCinSovL8fZ2Zm0WmpEunbtat1XVdW6Xbx4MfPnz6/UNysrCwcHB+vrs2fPEh0dTXJyMi4uLkRERNQ4f7Gqqjz++ON88sknTboHe3t77OzsAG2+68jISFJSUnB3d2fp0qW1zplsuTc7OztKS0sBuO+++zh37py1z/nz57nvvvuaNC5bUpIhhBBCCNEOjB07lri4OEwmE0ajkfj4eHQ6HQMHDmTbtm2AFrymp6fXeZ3Q0FBiYmIwGo2AVsaQk5NTrd/NmzdxcHCgR48eXL58mT179liPOTk5kZ+fD0BwcDCHDh3i9OnTgFZicerUqVrf3/bcqizBsZubG0ajke3bt9d5L1VNmTKFf/zjH6iqypEjR+jRo0ez65dBMsxCCCGEEO1CYGAgU6ZMYeTIkfTp04cRI0bQo0cPtmzZwoIFC4iKiqKkpISZM2fi4+NT63UmTJhAZmamtWzC0dGRzZs3W7O8Fj4+Pvj6+uLl5YW7u7u15AK0uumJEyda65E3btzIrFmzrOUgUVFRDBkypMb3r3quLWdnZ+bOncvw4cPp27cvgYGBjfoaPfHEE+zevZvBgwej0+nYsGFDo86vjWJJz7dFAQEBqqWgXQghhABAVaGsDDpLzkfcOZmZmXh7e7f2MDAajTg6OlJQUEBISAhr167Fz8+vtYfV7tT0/VQURa+qakBN/aUkQwghRPsybhy4usLhw1rwvGQJvPFGa49KiDti3rx5jBo1Cj8/P2bMmCHB8h0if54LIYRoPwoKtEAZIDoaXn4ZVqzQXi9ZAp0kDyQ6to8//ri1h9BgYWFhnD17tlLbW2+9RWhoaCuNqOkkYBZCCNF+mB8qws0N/v1vsH2Y5+uvISSkdcYlhKhm586drT2EFiN/igshhGj7rl3Tyi8yM7XXzz8PhYXw8cfg6QkDBlA+eQrv/lqPdTXf//xHq3UWQohmqjdgVhQlRlGUHEVRjtu0rVIU5aSiKMcURdmpKIqzzbHFiqKcVhTle0VRQm3aJ5rbTiuKsqjlb0UIIUSHdOgQ9O4NX30FO3Zo9csLFmjlF3l5lHsP48NnD5Bf0JleG1fx7bdogbWfH/zP/7T26IUQHUBDMswbgYlV2r4EhquqOhI4BSwGUBRlKDATGGY+5/8URbFTFMUOeA+YBAwFZpn7CiGEELVTVa1OuawM9u6Fzz+HX/6S/2T3ptRHe9jp+7w+zI3y4D+lw5nJP+mcehSuXtXO37+/FQcvhOgo6g2YVVU9AFyr0pagqmqp+eURoL95fyrwqaqqRaqqngVOAz8z/zutquoPqqoWA5+a+wohhBC1274djhzR9v/yFygupnDmr/Hzgx3XHgXglmMfAK7iBkDg80FgXvWLwsI7PmQhRMfTEjXMvwEsS7/cB5yzOXbe3FZbuxBCCFGz0lJYvBhGjKho8/UluVhbkOGjC1rAnGffF4Bu2ATHN24AYLphIjVVazpwALKzb/+whbgbrF69moKCgjt67muvvUZiYmKdfU6ePMno0aPp2rUr0dHRTRpfTZoVMCuK8j9AKbClZYYDiqLMUxQlRVGUlCtXrrTUZYUQQrQ3X3wBZ87A669r8yyPHg3x8VjWs8rs9RA8+yzJrlrVYHduVpxrDphvXCxk1iwoL4eHHgKbhcqEEHVQVZVy6xO01d2ugLmsjgd133jjDR577LE6r92zZ0/WrFnDyy+/3KSx1abJ08opihIBPAmMVyuWC8wG3G269Te3UUd7JaqqrgXWgrbSX1PHJ4QQop07eBC6dIHJk2HGDPjznwE4eVI7fCXfnvz/3cSfumuvX+UtDjOWS84P0tccMPcp+onsU0aSkx0B+PHHO34XogN66SVIS2vZa44aBatX199v+fLlbN68mV69euHu7o6/vz9hYWEsXLiQK1euoNPpWLduHV5eXkRERNC9e3dSUlK4dOkSK1euJDw8HIBVq1axdetWioqKCAsLY9myZWRlZREaGkpQUBB6vZ7du3fz5ptvkpycTGFhIeHh4Sxbtow1a9Zw4cIFHnnkEdzc3EhKSiIhIYHXX3+doqIiBg0axIYNG3B0dKw2/prOdXR0ZP78+SQmJvLee++xd+9e4uLiKCwsZMyYMXzwwQcoikJERARPPvkk4eHheHp6MmfOHOLi4igpKWHbtm14eXnRu3dvevfuzb/+9a8W/f40KcOsKMpE4I/AFFVVbf9E2AXMVBSlq6IoA4EHgKNAMvCAoigDFUW5B+3BwF3NG7oQQogOLSdHmx3jnnsqNZ86pW3z88H8ux+AbxjDx8yipLDUmmEGeJS9rF+v7cu6JqI9S05OZseOHaSnp7Nnzx5SzB+3zJs3j3fffRe9Xk90dDSRkZHWcy5evMjXX39NfHw8ixZpk5QlJCRgMBg4evQoaWlp6PV6Dhw4AIDBYCAyMpITJ07g4eHBihUrSElJ4dixY+zfv59jx47x4osvcu+995KUlERSUhJXr14lKiqKxMREUlNTCQgI4O23367xHqqeC3Dr1i2CgoJIT09n3LhxPP/88yQnJ3P8+HEKCwuJj4+v8Vpubm6kpqayYMGCFi2/qEm9GWZFUT4BHgbcFEU5D7yONitGV+BLRVEAjqiq+jtVVU8oirIVyEAr1VioqmqZ+TrPA18AdkCMqqonbsP9CCGE6ChycqBXr2rNBgNERMCxY5CQAE5OsHs3/PzncB0XdEXXKbx8g27m/r3JYd06bd/ZudrlhGi0hmSCb4dDhw4xdepU7O3tsbe3Z/LkyZhMJg4fPsxTTz1l7VdUVGTdnzZtGp06dWLo0KFcvnwZ0ALmhIQEfH19ATAajRgMBgYMGICHhwfBwcHW87du3cratWspLS3l4sWLZGRkMHLkyErjOnLkCBkZGYw11zwVFxczevToBt+XnZ0dM2bMsL5OSkpi5cqVFBQUcO3aNYYNG8bkyZOrnTd9+nQA/P39+eyzzxr8fk1Rb8CsquqsGprX19F/BbCihvbdwO5GjU4IIcTdKTcX/vUvmDDB2nThgpYhvnJFW+Bv6lQIC4OZMyEgAOzsIF91xrk8j707r+BHT1y5Ri+uWq+h02nTOn//PfzmN61xY0K0rPLycpydnUmrpUaka9eu1n1LBa2qqixevJj58+dX6puVlYWDg4P19dmzZ4mOjiY5ORkXFxciIiIwmUzV3kNVVR5//HE++eSTJt2Dvb09dnZ2AJhMJiIjI0lJScHd3Z2lS5fW+J6292ZnZ0epZWac20Q+nBJCCNH2vPqqtjUXHRcUwLRp8MQTUFICPXtqAfPWrfDXv4K9PYwbB0OCXLCjHPcL35KKH7fQMfK+ioBZVbV+v/1ta9yUEM0zduxY4uLiMJlMGI1G4uPj0el0DBw4kG3btgFa8Jqenl7ndUJDQ4mJicFoNAKQnZ1NTk5OtX43b97EwcGBHj16cPnyZfbs2WM95uTkRH5+PgDBwcEcOnSI0+al62/dusUpS+1UDWzPrcoSHLu5uWE0Gtm+fXud93KnNPmhPyGEEOK20cr9tMgYbfVr24mTevbUuth8Ck1SEijrneEb8OJ79jCJBzAwqPsV62PmeXl3ZvhC3A6BgYFMmTKFkSNH0qdPH0aMGEGPHj3YsmULCxYsICoqipKSEmbOnImPj0+t15kwYQKZmZnWsglHR0c2b95szfJa+Pj44Ovri5eXF+7u7taSC9DqpidOnGitR964cSOzZs2yloNERUUxZMiQGt+/6rm2nJ2dmTt3LsOHD6dv374EBgY26mt06dIlAgICuHnzJp06dWL16tVkZGTQvXv3Rl2nKqVigou2JyAgQLUUtAshhLiLLFwI//d/8NNP4O5ujZ8tdu7UMs7V7NhhfRLwt3zIAv6O58/60Oto9SfmS0qgs6SNRANlZmbi7e3d2sPAaDTi6OhIQUEBISEhrF27Fj8/v9YeVrtT0/dTURS9qqoBNfWXkgwhhBCt67vvYMOGym15eTBoELi713iKOfFcnc1TfcOfGU5h9z70KLxobXvrLZg0Sduv5RNhIdq0efPmMWrUKPz8/JgxY4YEy3eI/G0thBCida1fD++/r019oShgNML163VOaVFrwOziYt39w7qhsHgg6pbD1rY//hFcXWHPHi1gtukuRLvw8ccft/YQGiwsLIyzZ89WanvrrbcIDQ1tpRE1nQTMQgghWldRkfavsFCbK27cOHB0hBpqFydO1GqZBw6s5VqWINvTU5tv7v77UfLycOY6eWjRsZOT1kUyzELcXjt37mztIbQYKckQQgjRuoqLte21a9pS2GVlcOMGZU7VM8w/+xmkpIDNzFeVWVLGw4dr2/vvB+C1//qBjz7SmizP/kjALIRoKAmYhRBCtC7LIgvXrmmlGEBJZ3ve3XkfN25oU8FZ2EwpW7Pu3bVo2t9fe21ORf9h6g/88pdakyXDfPNmC41fCNHhSUmGEEKI1mWbYTYHzI/0ySQt240pudocyxYvvVTPtezsQK+veFjQUrvxww/WLpaA2Wb1bCGEqJNkmIUQQrSqG1dsAua8PHBwILuLJ7dwRFXh1i3t8OrV2kp99XrwwYqO3buDm1ulgHngQK15x46WvQ8hRMclAbMQQog7p6wMfvlL+H//D4CLF+HQvioZZhcX67zLxcXapBmgPQfYJPffXy3DPGEC1LKSsBCiAVavXk1BQcEdPfe1114jMTGxzj5btmxh5MiRjBgxgjFjxtS76mFDScAshBDiztmzB7Zsgbff5sg3Krt2QVe0GuZ9n12j+HLl6eQKC+GqeWXrWh/0q0+VgBm0DLMlEBdCVKeqKuXl5bUev10Bc1lZWa3nvfHGGzz22GN1XnvgwIHs37+f7777jj//+c/MmzevSWOsSmqYhRBC3Dlbt1p3oyJO4/n4AwxFyzAf2XMNb+88+ri5gLkMw2SCDz7Q6phDQpr4nvffD59+Crm52iTMaMG3pdRDiMZ66aWW/4Ri1Cit7Kg+y5cvZ/PmzfTq1Qt3d3f8/f0JCwtj4cKFXLlyBZ1Ox7p16/Dy8iIiIoLu3buTkpLCpUuXWLlyJeHmlTBXrVrF1q1bKSoqIiwsjGXLlpGVlUVoaChBQUHo9Xp2797Nm2++SXJyMoWFhYSHh7Ns2TLWrFnDhQsXeOSRR3BzcyMpKYmEhARef/11ioqKGDRoEBs2bMCxho+FajrX0dGR+fPnk5iYyHvvvcfevXuJi4ujsLCQMWPG8MEHH6AoChERETz55JOEh4fj6enJnDlziIuLo6SkhG3btuHl5cWYMWOs7xUcHMz58+db5PsjGWYhhBC3n8kEP/4IsbHg4QHAjVOX+OILuMccMPfkGnb5lUsyUlNh82Z4/nm4994mvvegQdr2+eetTY6OkmEW7U9ycjI7duwgPT2dPXv2kJKSAmir/7377rvo9Xqio6OJjIy0nnPx4kW+/vpr4uPjWbRoEQAJCQkYDAaOHj1KWloaer2eAwcOAGAwGIiMjOTEiRN4eHiwYsUKUlJSOHbsGPv37+fYsWO8+OKL3HvvvSQlJZGUlMTVq1eJiooiMTGR1NRUAgICePvtt2u8h6rnAty6dYugoCDS09MZN24czz//PMnJyRw/fpzCwkLi4+NrvJabmxupqaksWLCA6OjoasfXr1/PJMvSns0kGWYhhBC33+zZYFnE4KWXYPlyulLE6dPg0LkYSrWA+R7jdXD2sZ72yitaNvjVV5vx3s88A7/9rVYjbebgAKWlWo30Pfc049rirtSQTPDtcOjQIaZOnYq9vT329vZMnjwZk8nE4cOHeeqpp6z9iixTNQLTpk2jU6dODB06lMuXLwNawJyQkICvry8ARqMRg8HAgAED8PDwIDg42Hr+1q1bWbt2LaWlpVy8eJGMjAxGjhxZaVxHjhwhIyODsWPHAlBcXMzo0aMbfF92dnbMmDHD+jopKYmVK1dSUFDAtWvXGDZsGJMnT6523vTp0wHw9/fns88+q3QsKSmJ9evX8/XXXzd4HHWRgFkIIcTtZ7vi1xNPwPLl2GMCwK1HMeSaA+bCvEoZZpMJFi3SJrpoMgcHCAiAThUfqlo+KTYa61hmW4h2oLy8HGdnZ9JqqRHpajN5uWqe1FxVVRYvXsz8+fMr9c3KysLB5mGBs2fPEh0dTXJyMi4uLkRERGAymaq9h6qqPP7443zyySdNugd7e3vs7OwAMJlMREZGkpKSgru7O0uXLq3xPW3vzc7OjtLSUmv7sWPHeO6559izZw+u5jKs5pKSDCGEELeXTe1DofsQynVatOrmqP0S7KnTsmG9lSvYF92sWK3P7OWXW2AMDg5g85CRJSaQOmbRnowdO5a4uDhMJhNGo5H4+Hh0Oh0DBw5k27ZtgBa81jczRGhoKDExMRjN/29mZ2eTk5NTrd/NmzdxcHCgR48eXL58mT179liPOTk5kW9eLjM4OJhDhw5x+vRpQCuxOHXqVK3vb3tuVZbg2M3NDaPRyPbt2+u8l6p++uknpk+fzkcffcSQIUMadW5dJMMshBDi9nr/fevuWw/t5o+o6IAZT5hY+S7cE6DVMHuSpXVydrZmmKEZs2PYcnAA88fRUDnDLER7ERgYyJQpUxg5ciR9+vRhxIgR9OjRgy1btrBgwQKioqIoKSlh5syZ+Pj41HqdCRMmkJmZaS2bcHR0ZPPmzdYsr4WPjw++vr54eXnh7u5uLbkArW564sSJ1nrkjRs3MmvWLGs5SFRUVK0Ba9VzbTk7OzN37lyGDx9O3759CQwMbNTX6I033iA3N9dax925c2drrXdzKKrtmqNtTEBAgNoSNymEEKIVLVoEb72FK1ex6+XKsX+do+/PBrB31joe/fg56Nu3UjDLpk08sPxZzMkqWuTX1FNPwYkTkJEBQHw8TJ4MR49CI38fi7tUZmYm3t7erT0MjEYjjo6OFBQUEBISwtq1a/Hz82vtYbU7NX0/FUXRq6oaUFN/yTALIYS4vQoLudWlB9dKXOEKfLHfnjmArpO5LtGyNLZFlQxzi6gyj5yUZIj2at68eWRkZGAymZgzZ44Ey3eIBMxCCCFa3tGj2swYSUlw6xaFnRwYMQJ++gliPtYCZnvF/CR/UZG2lLWlxvj++1smq2yrSsDcrZu2LSxs4fcR4jb7+OOPW3sIDRYWFsbZs2crtb311luEhoa20oiaTgJmIYQQLe/ZZ+HMGfjmG7h1C1MnHc7O2mQVH22wB7DOkkFxsTY385kz2uuhQykp0Xa7d2+h8dgG5FRMmFHHQmZCiGbaaTs7Tjsns2QIIYRoeZbU7bVrUFBAgeJAt25aHF1KZ8rohF2pSYtYS0u1OmbAqDhyq7ATubng7g71POzfcA4O2pjMEbKl5KMNP8YjhGhDJGAWQgjR8nQ6bXvhAty6xS20gPnhh+Hvf1cooiu9HE0V9ctOTgBkqN7WVfheew08PVt4POZAXjLMQojGkIBZCCFEy7OUP+zapQXMqoM1Zv3d76Cbiz3O9qaKfg89RLrHZH7FR9ZLjBrVguOp8pSfZJiFEI0hAbMQQoiWZ1mUID0dUlMxqg7WB+0AFHt7bRk/S8Dcqxcfhe/iFA8CYGcHw4e34Hgs88uaU8qWDLMEzEKIhpCAWQghRMtSVS1gnj8fevSA4mJullcOmKkaMOt01sVEALy9tS63ZWxUZJilJEOIplm9ejUFNg/S3olzX3vtNRITE+vs8/nnnzNy5EhGjRpFQEAAX3/9dZPGWJUEzEIIIVqWyaQ9yOfhAX/6EwD5ZbrqAXNREXz3nfa6SsDs69vCY6oysbNkmIWom6qqlNfxF+XtCpjLyspqPe+NN97gscceq/Pa48ePJz09nbS0NGJiYnjuueeaNMaqZFo5IYQQLSM9naL+g+haZv5F6OQEv/oV6sqV/JR7X+WAuWdPbXW/8HDttU5Hly4Vh1u0ftmWZJhFC3jpJUhLa9lrjhoFq1fX32/58uVs3ryZXr164e7ujr+/P2FhYSxcuJArV66g0+lYt24dXl5eRERE0L17d1JSUrh06RIrV64k3Pz/3KpVq9i6dStFRUWEhYWxbNkysrKyCA0NJSgoCL1ez+7du3nzzTdJTk6msLCQ8PBwli1bxpo1a7hw4QKPPPIIbm5uJCUlkZCQwOuvv05RURGDBg1iw4YNONr+FWxW07mOjo7Mnz+fxMRE3nvvPfbu3UtcXByFhYWMGTOGDz74AEVRiIiI4MknnyQ8PBxPT0/mzJlDXFwcJSUlbNu2DS8vr0rveevWLZQWWgVJMsxCCCGa59gxOH8eRo3iWO/xvBCh1S+fveqE2r0HCX87yV9YjLOzzTkPPAAnT1a87tatUvDa4ouXSYZZdADJycns2LGD9PR09uzZQ0pKCqCt/vfuu++i1+uJjo4mMjLSes7Fixf5+uuviY+PZ9GiRQAkJCRgMBg4evQoaWlp6PV6Dhw4AIDBYCAyMpITJ07g4eHBihUrSElJ4dixY+zfv59jx47x4osvcu+995KUlERSUhJXr14lKiqKxMREUlNTCQgI4O23367xHqqeC1pgGxQURHp6OuPGjeP5558nOTmZ48ePU1hYSHx8fI3XcnNzIzU1lQULFhAdHW1t37lzJ15eXvziF78gJiam+V94JMMshBCiOQ4ehJAQ6/xvgeVHmb/nBgD/b5kTny+H8nI3AFxdbc4bMgQ2bqx4rdNZA+b+/eGhh27TeCXDLFpAQzLBt8OhQ4eYOnUq9vb22NvbM3nyZEwmE4cPH+app56y9isqKrLuT5s2jU6dOjF06FAuX74MaAFzQkICvubaJ6PRiMFgYMCAAXh4eBAcHGw9f+vWraxdu5bS0lIuXrxIRkYGI0eOrDSuI0eOkJGRwdixYwEoLi5m9OjRDb4vOzs7ZsyYYX2dlJTEypUrKSgo4Nq1awwbNozJkydXO2/69OkA+Pv789lnn1nbw8LCCAsL48CBA/z5z3+ut+65ISRgFkII0TQvvgjvvqvtZ2VZm3/JZgC+Y0SlgNTNzebcIUMqX8smYH7qqWoJ4eaTDLPooMrLy3F2diatlhqRrl27WvdV83/wqqqyePFi5s+fX6lvVlYWDpYpGIGzZ88SHR1NcnIyLi4uREREYDKZqr2Hqqo8/vjjfPLJJ026B3t7e+zMM9mYTCYiIyNJSUnB3d2dpUuX1vietvdmZ2dHaWlpteMhISH88MMPXL16FbdKP4AaT0oyhBBCNJ6qVgTLjzxS6dB/8w5ZeHCawZXaK/2+euCBytfT6Rg/XtudOrWFx2pLMsyiHRs7dixxcXGYTCaMRiPx8fHodDoGDhzItm3bAC14Ta9niczQ0FBiYmIwGo0AZGdnk5OTU63fzZs3cXBwoEePHly+fJk9e/ZYjzk5OZFvnj4yODiYQ4cOcQcXxVQAACAASURBVPr0aUArsTh16lSt7297blWW4NjNzQ2j0cj27dvrvJeqTp8+bf3DIDU1laKiIlwrfbzVNJJhFkII0XhnzlTsf/gh/OY3sH+/telLHgcqZ3Ur/c4aXDmYRqfDz+82Znwlwyw6gMDAQKZMmcLIkSPp06cPI0aMoEePHmzZsoUFCxYQFRVFSUkJM2fOxMfHp9brTJgwgczMTGvZhKOjI5s3b7ZmeS18fHzw9fXFy8sLd3d3a8kFaHXTEydOtNYjb9y4kVmzZlnLQaKiohhS9ZOkWs615ezszNy5cxk+fDh9+/YlMDCwUV+jHTt28I9//IMuXbrQrVs3/vnPf7bIg3+K2oZ/WgQEBKiWgnYhhBBtyKZNEBEBx4/DsGHwxRecfWk1A0/+G4Cn+SfbeBoAFxe4fh2uXdP2rQYMgHPntP1btyqWr74dPvwQ5s6Fn34Cd3fOnNFi9k2b4Nlnb9/bio4jMzMTb2/v1h4GRqMRR0dHCgoKCAkJYe3atfi1+FOyHV9N309FUfSqqgbU1F9KMoQQQjTeoUPg7KytMAIQGsqH0ys+rv2K8db9vXvhlVeoPEsGVK5jvi2rlNiokmGSpbFFezVv3jxGjRqFn58fM2bMkGD5DpGSDCGEEI136BCMGVNR2wBcvQr/5fwvAsu/5drNivqLUaNqmVf5gQfgq6/AYKh0ndvKHCFb3k5qmEV78/HHH7f2EBosLCyMs2fPVmp76623CA0NbaURNZ0EzEIIIRonLw8yMmD2bGvTjRuwbRvce98TZPV4Ag5r7U5OdVxnxAiws6tS3HybSIZZiDtu586drT2EFiMlGUIIIRrn/Hlta1NSMXu2VqesKNo8yqDNOmfpWqPnnoPk5CqFzbdZlQyzBMxCiIaQgFkIIUTjXL2qbW3mifv2W2174wa4u2v7Hh7QvXsd17nnHjAvnHDb1ZJhlpIMIURDSMAshBCiUT5fXz1g7mwu8MvLq8gwOzre4YE1hGSYhRBNIAGzEEKIRtm9OReAMueK2mNLxtbfvyLD3KYCZskwCyGaQQJmIYQQjeKGlmHOKdMC5tJSuHIFHnsMPvsMHnxQ69evX2uNsA6SYRZ3MU9PT65aSqrqsXTpUqKjo+vsExsbS0ZGRksMrc2TgFkIIUTdbtwAm2VsH8BAPo5kX+0KQHY2lJXBM89oz+8NHw4nTsDDD7fSeGsiGWbRwaiqSnkr/wd8NwXMMq2cEEKIujk7Q69ekJMDt27xSzazladxyIaAALBMs+rpWXHK0KGtMtL6SYZZtISXXoK0tJa95qhRsHp1nV2ysrIIDQ0lKCgIvV7P008/TXx8PEVFRYSFhbFs2TIApk2bxrlz5zCZTPz+979n3rx5DRrCihUr2LRpE71798bd3R1/f38A1q1bx9q1aykuLmbw4MF89NFHpKWlsWvXLvbv309UVBQ7duxg79691frpbucKnneQZJiFEELUzhJRXrkCQPmlHDpTxpc8zmHzXMtZWdp24MA7P7wGkwyz6CAMBgORkZG88847ZGdnc/ToUdLS0tDr9Rw4cACAmJgY9Ho9KSkprFmzhtzc3Hqvq9fr+fTTT0lLS2P37t0kJydbj02fPp3k5GTS09Px9vZm/fr1jBkzhilTprBq1SrS0tIYNGhQjf06CskwCyGEqN1PP1XsFxdzzZCLG5CLKwmb4eWX4Xe/0w5bHvZr0yTDLFpCPZng28nDw4Pg4GBefvllEhIS8DVPzWg0GjEYDISEhLBmzRrroiHnzp3DYDDgWs8CQQcPHiQsLMyaEZ4yZYr12PHjx1myZAl5eXkYjcZaV+praL/2SAJmIYQQtUtIqNj/8Ueufq8FzEGTXInboz3YV1amHb7nnlYZYcNIhll0EA4ODoBWw7x48WLmz59f6fi+fftITEzkm2++QafT8fDDD2MymZr1nhEREcTGxuLj48PGjRvZt29fs/q1R1KSIYQQokaffQYXln5Q0XD6NOX7DwIw9Teu6HQVwXK7IRlm0UGEhoYSExOD0WgEIDs7m5ycHG7cuIGLiws6nY6TJ09y5MiRBl0vJCSE2NhYCgsLyc/PJy4uznosPz+ffv36UVJSwpYtW6ztTk5O5Ns8EFxbv45AAmYhhBA1+vb/9Nx7QU/Z4iVaw3/+w9CdKwDw8HPlb3+DYcO0Q1OnttIgG0oyzKKDmTBhArNnz2b06NGMGDGC8PBw8vPzmThxIqWlpXh7e7No0SKCg4MbdD0/Pz+eeeYZfHx8mDRpEoGBgdZjy5cvJygoiLFjx+Ll5WVtnzlzJqtWrcLX15czZ87U2q8jUNQ2/Od1QECAmpKS0trDEEKIu1Ksx4tM+OlDDPsu4POL/tC3L5w5ox0sKbEu71dSAnZ2FVnbNmnzZvjVr+DUKXjgAYxGcHKClSvhlVdae3CiPcjMzMTb27u1hyFaSE3fT0VR9KqqBtTUvy3/eBNCCNGKet88TSbefJPpDIMHW4Plec5bK9bCBrp0aePBMlTLMEtJhhCiMeShPyGEEJqbN7Xot1s3AJxNl8miD8nJ8LvBgyE9ndS+T/C1y1OtPNBmMEfIUpIh7la5ubmMHz++WvtXX31V70wadzMJmIUQQmgefRQcHWHvXujUCZeSy3zLSI4eBZ4YBMCRHhPo0aN1h9kSJMMs7laurq6ktfSiK3eBtv4hmhBCiDvh6lXQ62H/fvjgAygro1fZJS7Th4wMWLZjOOUo7O0ysX0GzJaUsmSYhRBNIBlmIYQQYHnAun9/ePVV1IMH6UwZXfr1ovwiLD8zi634UVr8IL7OrTvUliAZZiFEY0iGWQghBGRna9tNm6C0FOWTTwDo8nQYnTpBGZ3JYBg//kj7zjBXeSkZZiFEQ0jALIQQAi5f1rajR8Nf/gLAGl7A7oH7OXUK/vu/Ydw4KCqCdv1ckCxcIoRoAgmYhRCig1q4EGJjG9j58mVtYuJu3eDFF7n47naiWEL37jBoEPz1r7BvH2zdCi+8cDtHfZtIhlkIPD09uXr1aoP6Ll26lOjo6Dr7xMbGkpGR0RJDa/MkYBZCiA4qJgb+9rcGds7Jgd69tX1F4dLYGVyhN05OFV3s7OCpp6BfvxYf6p1jk1JWFMkwi/ZLVVXKW/kvvrspYJaH/oQQogMqLweTCQ4dAqNRmy2uThcvQp8+1pf5+dq2e/fbN8Y7qkqG2dIkGWbRJC+9BC09NduoUbB6dZ1dsrKyCA0NJSgoCL1ez9NPP018fDxFRUWEhYWxbNkyAKZNm8a5c+cwmUz8/ve/Z968eQ0awooVK9i0aRO9e/fG3d0df39/ANatW8fatWspLi5m8ODBfPTRR6SlpbFr1y72799PVFQUO3bsYO/evdX66XS65n1d2gjJMAshRAdkMmnbkhL45z+14HDXrlo6l5VBaiqMHGltunlT29pmmDsEm5Ryp06SYRbtj8FgIDIyknfeeYfs7GyOHj1KWloaer2eAwcOABATE4NeryclJYU1a9aQm5tb73X1ej2ffvopaWlp7N69m+TkZOux6dOnk5ycTHp6Ot7e3qxfv54xY8YwZcoUVq1aRVpaGoMGDaqxX0chGWYhhOiACgsr9i1lGX/9K0yZUrmfqsLed44xPj+fTLefU/odjBhRkWHuMAGzZJhFS6onE3w7eXh4EBwczMsvv0xCQgK+vr4AGI1GDAYDISEhrFmzhp07dwJw7tw5DAZDvav4HTx4kLCwMGtGeIrND4vjx4+zZMkS8vLyMBqNhIaG1niNhvZrjyRgFkKIDqigoGL/u++07eXLsGEDPPusVo8MYDDA568cZDwwIernnI/SgmhLhrnDlGRYSA2zaOccHBwArYZ58eLFzJ8/v9Lxffv2kZiYyDfffINOp+Phhx/GZPnIqYkiIiKIjY3Fx8eHjRs3sm/fvmb1a4+kJEMIITqa48fp/ZAXvchh0KCK5u+/h9/8BjLe+QJOnADgxg34OQfJwoPzuFv73g0ZZinJEO1ZaGgoMTExGI1GALKzs8nJyeHGjRu4uLig0+k4efIkR44cadD1QkJCiI2NpbCwkPz8fOLi4qzH8vPz6devHyUlJWzZssXa7uTkRL7lh0Ud/ToCCZiFEKKjWbOGrme/ZxafcP/9lQ/1JJcRr0yE4cPh6FFcP3yLp9jOYcZY+6iqFjArCpiTWR1HlQyzlGSI9mrChAnMnj2b0aNHM2LECMLDw8nPz2fixImUlpbi7e3NokWLCA4ObtD1/Pz8eOaZZ/Dx8WHSpEkEBgZajy1fvpygoCDGjh2Ll5eXtX3mzJmsWrUKX19fzpw5U2u/jkBR2/Cf1wEBAWqKZblWIYQQtSsogEcf1QqW9+6FP/2J6zjz6nPXWPdhRXb1GT7lU2ZpL2xqEt4f+BYLzv4R0GaYmz0bkpMhL++O38ntsX27Nifed99pfyygzRzyu99BPVPNCgFAZmYm3t7erT0M0UJq+n4qiqJXVTWgpv6SYRZCiI4gMxO+/RYiIqxpUxfyGGP3rbVLVBTca39de/H995WW7Bu39UU+/1zbT0uDxER4+uk7Nfg7SDLMQogmkIBZCCE6AkvkZzBYC5BL6MzT/5zOIrSlrufMAfce5qf5+ve3TiO3gQi6udgzcKB2yPLB3tixd2z0t5/UMAsBQG5uLqNGjar2ryFTz93NZJYMIYToCMwP/lBWBjdvUuTkyrT8j9jqHsVf8v7EHiah043C6758Si/bUW7XjXtGjoS9ezHiiE4HvXppl9DrtW09s1C1T5JhFnc5V1dX0lp60ZW7gGSYhRCiI7h1q2I/P5+ie5z4N5PIf20VAGn40q0bPNjvJvk4ceiwYp2U2YQ9Op02hZyrq7Y6IHSwgFkyzEKIZpCAWQghOgJLhhngxx+5XNAde3vo8VjFk+729tC/x01u0p0vvgAeeYR//iqe5fwZy+q1s2fDpUvavpvbnRv+HSMZZiFEE0jALIQQHYFtwJyRwfUyJ6ZPBwfnLrzKmwAoB/ZzjymfMl13/v1v+MMfYOZHv6Cwc3e6dNFOffVV6NpV25cMsxBCaCRgFkKIjsC2JCM3l5tKD2vWOBdz5Pvww5CRwT29upOeXrG6r23QeN99EBkJzs7avw5HMsxCiCaQgFkIIdqznBzYsaMiw7x2LURFsVK3zLr8dc/BNqnikyfpO6Q7mzZZpyOmrKzyJVet0mad69SRfkNIhlkIPD09uXr1aoP6Ll26lOh6JimPjY0lIyOjJYbW5tX741BRlBhFUXIURTlu09ZTUZQvFUUxmLcu5nZFUZQ1iqKcVhTlmKIofjbnzDH3NyiKMuf23I4QQtxlliyB8HD48kvo0gXmzoX/+R9SOwVYA+ao91wqndJ59M949ll4//2aL2lnB7173+ZxtxbJMIsOQlVVylv5P+C7KWBuyLRyG4H/Bf5h07YI+EpV1TcVRVlkfv0qMAl4wPwvCPg7EKQoSk/gdSAAUAG9oii7VFW93lI3IoQQd6WePbXt/v3gUhEYl5VhDZjvcTGvb/3cc9pkzKNHA1iXzX7mmTs12FZUQ4bZZqFDIRrnpZe0FX5a0qhRFXVStcjKyiI0NJSgoCD0ej1PP/008fHxFBUVERYWxrJlywCYNm0a586dw2Qy8fvf/5558+Y1aAgrVqxg06ZN9O7dG3d3d/z9/QFYt24da9eupbi4mMGDB/PRRx+RlpbGrl272L9/P1FRUezYsYO9e/dW66ez1Ia1c/VmmFVVPQBcq9I8Fdhk3t8ETLNp/4eqOQI4K4rSDwgFvlRV9Zo5SP4SmNgSNyCEEHc1J6eK/e7drbtlZdDZkhIJDIRt22DNGhg3zhpJ9+sHhw/Dpk3cPWwi5E6dJMMs2h+DwUBkZCTvvPMO2dnZHD16lLS0NPR6PQcOHAAgJiYGvV5PSkoKa9asadCiJHq9nk8//ZS0tDR2795NcnKy9dj06dNJTk4mPT0db29v1q9fz5gxY5gyZQqrVq0iLS2NQYMG1divo2jqwiV9VFW9aN6/BPQx798HnLPpd97cVlu7EEKI5rCN+H7+c+tuaWlFhhnQyjZqYE42d3ySYRYtqZ5M8O3k4eFBcHAwL7/8MgkJCfj6+gJgNBoxGAyEhISwZs0adu7cCcC5c+cwGAy41jPtzcGDBwkLC7NmhKeY52kHOH78OEuWLCEvLw+j0UhoaGiN12hov/ao2Sv9qaqqKorSYj9yFEWZB8wDGDBgQEtdVgghOibLE3v//nel6Ne2JEPYqJJhloBZtDcODlqJlaqqLF68mPnz51c6vm/fPhITE/nmm2/Q6XQ8/PDDmEymZr1nREQEsbGx+Pj4sHHjRvbt29esfu1RU5+BvmwutcC8zTG3ZwPuNv36m9tqa69GVdW1qqoGqKoa0MuyTqsQQoialZdrqdLQ0GolGRIw26glwywlGaK9Cg0NJSYmBqN5hpzs7GxycnK4ceMGLi4u6HQ6Tp48yZEjRxp0vZCQEGJjYyksLCQ/P5+4uDjrsfz8fPr160dJSQlbtmyxtjs5OZGfn19vv46gqQHzLsAy08Uc4HOb9mfNs2UEAzfMpRtfABMURXExz6gxwdwmhBCiOcrLa5z/TQLmWkiGWXQQEyZMYPbs2YwePZoRI0YQHh5Ofn4+EydOpLS0FG9vbxYtWkRwcHCDrufn58czzzyDj48PkyZNIjCwYpXQ5cuXExQUxNixY/Hy8rK2z5w5k1WrVuHr68uZM2dq7dcRKGo9Py0URfkEeBhwAy6jzXYRC2wFBgA/Ak+rqnpNURQFbUaNiUAB8GtVVVPM1/kN8CfzZVeoqrqhvsEFBASoKSkpTbgtIYS4SyxeDH/9KxQXW5vKy7VgeelSeP311htamxIfD5MnQ3IyBAQAMGiQVsWyeXMrj020C5mZmXh7e7f2MEQLqen7qSiKXlXVgJr611vDrKrqrFoOja+hrwosrOU6MUBMfe8nhBCiESzRsQ1LWXPnZj+l0gFJhlkI0QTy41QIIdqzGkoySku1rZRk2JAaZiEAyM3NZfz4ajlPvvrqq3pn0ribScAshBDtWVlZtYDZkmGWgLkGkmEWdzlXV1fSWnrRlbtAUx/6E0II0RbUUZIhAbMNyTALIZpBAmYhhGjPaijJkIC5DpJhFkI0gQTMQgjRnklJRsNIhlkI0QwSMAshRHsms2Q0jk1KWZbGFncbT09Prl692qC+S5cuJTo6us4+sbGxZGRktMTQ2jwJmIUQoj2TWTIapoYMc6dOkmEW7ZeqqpS38n/AEjALIYRoH6Qko3EkwyzasaysLB588EGeffZZhg8fzvLlywkMDGTkyJG8brNK0bRp0/D392fYsGGsXbu2wddfsWIFQ4YMYdy4cXz//ffW9nXr1hEYGIiPjw8zZsygoKCAw4cPs2vXLl555RVGjRrFmTNnauzXUcgHdkII0Z7JLBkNU0uGWQJm0SQvvQQtPTXbqFGwenW93QwGA5s2beLmzZts376do0ePoqoqU6ZM4cCBA4SEhBATE0PPnj0pLCwkMDCQGTNm1DvHsl6v59NPPyUtLY3S0lL8/Pzw9/cHYPr06cydOxeAJUuWsH79el544QWmTJnCk08+SXh4OADOzs419usIJGAWQoj2TGbJaJwqGWYpyRDtjYeHB8HBwbz88sskJCTg6+sLgNFoxGAwEBISwpo1a9i5cycA586dw2Aw1BswHzx4kLCwMHQ6HQBTpkyxHjt+/DhLliwhLy8Po9FIaGhojddoaL/2SAJmIYRoz6Qko2EkwyxaUgMywbeLg4MDoNUwL168mPnz51c6vm/fPhITE/nmm2/Q6XQ8/PDDmEymZr1nREQEsbGx+Pj4sHHjRvbt29esfu2R1DALIUR7JiUZjSMZZtFBhIaGEhMTg9FoBCA7O5ucnBxu3LiBi4sLOp2OkydPcuTIkQZdLyQkhNjYWAoLC8nPzycuLs56LD8/n379+lFSUsKWLVus7U5OTuTn59fbryOQDLMQQrRndcySIdPK2ZAMs+hgJkyYQGZmJqNHjwbA0dGRzZs3M3HiRN5//328vb158MEHCQ4ObtD1/Pz8eOaZZ/Dx8aF3794EBgZajy1fvpygoCB69epFUFCQNUieOXMmc+fOZc2aNWzfvr3Wfh2BorbhnxYBAQFqSkpKaw9DCCHarqefhu++g8xMa1NqKvj7Q2wsTJ3aimNrS778EiZMgIMHYdw4AEaPBicnSEho5bGJdiEzMxNvb+/WHoZoITV9PxVF0auqGlBTfynJEEKI9kxKMhpGMsxCiGaQD+yEEKI9k1kyGkdqmMVdLjc3l/Hjx1dr/+qrr+qdSeNuJgGzEEK0ZzJLRsNIhlkIAFxdXUlr6Tmk7wJSkiGEEO2ZlGQ0jmSYRTO05ee+RMM15fsoAbMQQrRnMktGw0iGWTSTvb09ubm5EjS3c6qqkpubi729faPOkx+nQgjRnklJRuNIhlk0Uf/+/Tl//jxXrlxp7aGIZrK3t6d///6NOkcCZiGEaM+kJKNhasgwK4pkmEXDdenShYEDB7b2MEQrkZIMIYRoz2SWjMaxiZClJEMI0VASMAshRHsmJRkNU0uGWUoyhBANIQGzEELU4eOPKy2i1/ZISUbjSIZZCNEEUsMshBC1OHkS/uu/tP0aErltQ3l5tekwZJaMGkiGWQjRDG3xx78QQrQJsbEV+99/33rjqFMNkbwEzHWQDLMQogkkYBZCCFtlZdaI88SJiubk5FYaT31qKMkoKtK2jZxmtGOTDLMQohkkYBZCtCsREbBp0226+IcfwoABMHkyoNUujx8PDg5tPGCukmE2mbRt166tMJ62TjLMQogmkIBZCNGufPYZLF16GzKD33wDc+eC0Qh791JeWERmJowYAf7+sG8fXL8O2dkt/L7NVUNJhmSYayAZZiFEM0jALIRoV0wmyMqCf/wDDh9uwQtv365FmO++C8XF5P59KwUFMHQohIfD8ePQsyf07w/x8S34vs1VQ0mGJcMsAXMNJMMshGgCCZiFEG1bQoI1Aiwrg5ISrfnXv4axY+Gnn1rofS5e1KLhp5+GQYPovO7vAHh7wwsvVA6SExJa6D1bgpRkNIxkmIUQzSABsxCi7fr+ewgNhQULgJpLDTZsaKH3unoV3Ny0iz/6KF3PnQa0gBlgyJDKXduMWkoyunRpo9PgtTbJMAshmkB+nAoh2i5LhHz0KFCROZ05s6LLrl0t9F6WgBkovG8wultXGOJ2DddbWgrbNlt75UrlU2/ehPPnW2gcjVVLSYZkl6uQDLMQohkkYBZCtF2W+ouMDFi9muIz5wAYPRr27IGFCyE9HQoLG37Jw4dhy5YaDtgEzCt3DAJgc/4U8PCA69e5556Krt9+WzkzOX8+uLtDampjbq5u5eXwr39pM9xlZdXTsYYMs9Qv18LmG6cokmEWQjSMBMxCiLaroKBi/w9/wHXaOLpRgL09TJwIjz2mVSQcO9awy5WWanXPv/xlDZnFy5etAXPmtT4ABBYd0o5lZlozto7k82z+//LtjoqUclyctt27t7E3WLsjR+DJJ+H++2HgQPjxx1o61lCSYTJJwFxNDRlmKckQQjSUBMxCiLbLNmD+29/ocuEnhnPcGgz6+2vblJSGXc52SrgffjDvqCo8+igUF2uRKfDDTbfKJx4/bg2YF/MX/pcXCJx5P2zfzo0bcOuWduy77xp+a9Woqjalndm1a9r2nJZUJze3lvOqlGTk50tJRp2qZJilJEMI0RASMAsh2i7bgNnPD4Ae3LAGzP37Q+/e9QTMlnWiqQhCAf7zH/POsWOQlARTpsBvf0tJCfyQXyVgXrWKe7pogdYk9pDaJYhrnXvDtm2VstsNzXTX6J13wMlJi+JKS61BuEXV11Y2JRlnz4KLC3z1lWSYq5EMsxCiGSRgFkK0XZaA+fhxLRIEnMmzBoOKAgEBoNfXcv5332nTRbzwAlA5YE5LM+/s2aNt//536NqVa9fgOi6olgBr9mw4fZrOJVqhdE+uUTDgQdJLh6Ge+cF6ndmztZUBbeLzxrGdty4z0zbZDMCNG7WcZ1OSkZWlvbx8WQLmWkmGWQjRBBIwCyHaLkvA7Oys/QNcuF4pGAwIgBMnKiejrU6cAEDdth2oCJgVxSZg3r0bRo2Ce+8FtGf/yrGjyNFVa3v4Ya2fuSbCheu4DnLBUHY/ZafPkp4OvXpps98VFcGpU424v+LiioE/8IC12XRIbw2YX31V29YYMO/fr9VsmNPPN29WHJKSjCokwyyEaAYJmIUQbZclmOzWzRow22aYQatjLi+vpRzCPGHy5avajzpLwDx+vDlgzsvTps144gnrKZYp40wDhsCkSeDqqjXk5mJHKd3J595hLpxhEJ1v5HIo7ho+PjBsmNbt5MlG3F9oKHTvru0XF1ubz25LsZZgLFyobWsMmP/8Z21rnp7DNmCWDHMtJMMshGgCCZiFEG2XZb44nQ50OsrtOlcLmB98UNsaDDWcbw6Yy8q0l5aA+ZFH4MIFuLH9S+3gpElVT+Hchwnw3nuVAmZn8gDoMbAnF1xHArA2ZypeD6rWhU1ef11b5CQvrwH3t2+f9v7Hj8P165ztPpL9hFCerGWYu3SxTtxRc8Ds5KRtzZGyJWC+5x4JmKuRDLMQohkkYBZCtF0FBVqg07UrKArFOudqAbOnp9blzBm06Oett6ylGGU5WhlFd7RIMjdXS1aPHm2+/MexWkQaHExSEixfXhEwu3k4aO/bs6fWcO0a72FO97q4UDbcB4Cf8zW/9/oCJyfo10+LfQ2GBkwxZ4niQZtwOS+P66ozKQRwf34a3/2nFAcHLfDt0qVywJyVZa6VtgTM5rS4JWDesAFeeaX+L+9dSTLMQogmkIBZCNF2FRRo2WVzdtDk4EpvcirV53btqi0acuYMWqC8aBFs3AhA4Tkt+nXCCMXF5ORAnz4wauAN/peF9Ev6GCZPhs6defRReO21iky1JbFsm2F+hq3avqJwr28fS5k3dwAAIABJREFUVvAnAAb/dQHk5/Pb32qBqqMjJCbWc2/mMQKwdCns309eUTdKRvjTDRMF/96v3XpxEWu6vozyZQIAp09rs9+NG0dF1rR/f0ALmLt0gVmzKkqvhZlkmIUQzSABsxCi7bp5U4s+za64PIg3mdYyBQtPT9i8Gb743U6twTzhcmn2JWuf8tzrXLoEffuCy8rFLOT/tAPTplUKmj7/XCsrtq7s16uXlmWOja3o9MgjPPQQLGEFEQ8cgp9+gj/+keXLYeVKeOghbWq3WpWXw4oVMHSoNpPHnDkAOBXn0vsXgQDsJIwLF4Bt2/id8a88nvomV65UBPTffgsF2de1F998Y/1y9ehRY2woLCTDLIRoAgmYhRBt17lz1uzp+fOwLXMYQziFQ5fiSt3ME1zQ65BNwFxQgNOJI5xDO9+YfoZLl6B/ryLYsaPi5PHj+emnipdnzlA5IO/SRXu47osvAPgDb8O99zJ5spbJ/a/3xsCvfw0ffWR9cG/8eG22DMuiI9V89ZU2afKSJTB8OLz/Ppse3cTv7dfy7PIHUEeMoDv5dOtarvUD7Cgj6/+zd97hVVRbH34nhRBaIPTeQXoXpEqzdwV7FxQVFTteuCqIXPvnvSKKYi+oSBEBCwhKkS6991ASShIgpOfs7491ZuacFAhJSF3v8+SZmT17ZvYkOef8zm/WXmsvHD7sniZ6V7SUPPRxmO05hEo61GFWFCUXqGBWFKXwcfIkTJsm9aDr1QPg5ZdhKxcQTCqsWyc27oIFgBS6a8AeOvKPqKCDB+GPPwhMTuRp3iSVQHa/N4cNG+Cy2Clw5AgAhwLqQNmyLF8ul501C7p0EQ3rx+OPw6uvArCR1s41v/kGBg5EsmycPu1UUOnfXw7L1GVOS4NXXhHX+vrrneZJiXcR1KUDQUFgPfQQALuWRjkKuRIxfoK5Tx8wUUfwhFd2zhEZKYa4cgbUYVYUJQeoYFYUpfDx/vtw442wZQvJ1ety/fWikfdRX/Y/+ij89Zf8ACkp0B+vOr3qKrF2p04lOaQcM7iOJfSA2VIY5OLdn0Dz5ox9/jRNzXZSU2HJEplcd8klEuowbVq68VgWjBxJDQ4zjwEZx9tOJgDa8RJt2kgkiVNN0Jf//lfGfdddTiqLXbtgxQro1s3bx/sloeaUdySdB1CDSJYvF8EcFgbDbjtB3bR9HK7U0jn1vn1Qv372f80likwcZstSh1lRlOyhgllRlMLHrl0ymy8oiIgqHZgxA1auhP2IkGTFCll6neKbbhJBCYi9m5wMn3/O3to9SSaE2dZVtGcdnw5bQaNDi+CWW6havwzxJpSoKJmg16uXxC1blrjHmRFFDSCTAOGaNWXpFbeWBU2ayAS9DKxbJ8uxYwHR2P37y9xGb0FCnBx1b7zhpNuowjHefSuFCRMkZruzR34Hu8u3B8Qp3b/f0dpKVvgo5IAAdZgVRckeKpgVRSlc7NsHH38sCZajo9nVUybEVaoETXrXlpLVoaGS7sIrmAcNggHtjxNHWR78byvnVCvqDyI8HAKvvRqAexbdj2UMDBpEw4bSZ+RI2LwZbrjh7EPbuzeLfM9lykhhFa9gxhjGnH6KMhuWZ+y7c6fEU3gnMz76KJw6JdEldet6+zRrJrb3HXfIer9+BGB49/aVAFzSOZqGrw/jMDVYVboHIOEYSUnqMGeJOsyKouSCoIIegKIoih+ffCLLLVugfHlOeHML//EHtG1bCuu6q6TyyPTpMHWq5F1+9lmqBEZznMr8sqsJAKdqNGFO9XupVAnGT2sOzZpIkuRataBVK/o1g0aNZK5e584wZMjZh3ZGMVq7tpOdgy1buHrH2wzgfU6eTKBMGfj6axHl5XfuhCuvdA7bt08c5o4d052ve3f5AamCUrUqD1WagvV+d+5d8DgBByO4L2wh9Q6Fcfq0XB686eaUrEnnMKtgVhQlO6jDrChK4cLO5zZdMl7YxTjCw0Xg8NNPMGIELFsmO55/Hl58kVohx0koU5lZ6+pzO18z8a5lxMRaUlHbsiS2GRwbNzgY/vMfOe+kSVmHYWSbWrVch/n33wEIJZH17/3FrFlwzz3QvXMyREX5Ke+jR7MxUa9iRbjzTgI+/ohhl++l9JzpcO+9RDe/iN27YdEi6dawIbRvn8v7KK5k4TBrSIaiKNlBBbOiKIWLmBgJufC6sLZgzpAu7aWXpNLd3XfD2LGEr57HBd0r07Yt/F71Nv7eXpk//3Tn43G1hGVw4IBzikGDRL926JAH47YFc0ICvPQSJjyc7VZz2r14HU/csA+AoGMSZ33QIzHPqalyu+nzSmfKqFFywO23S0aOgQNp1Ah275ZsHKVKiYGunAV1mBVFyQEqmBVFKVzExEjAMhKT++ST0mxXgXZ44QWpFz1yJE5nb1W+mjWlzkhyshjQgMzq69YNPvjA7zRBeRWYVru2pLB4/XWIjcW6/Xbe6/Ud5VNjGICU/fvlU8kJN+UvSRwdHS2CLVup4Bo1gsGDYelS2a5bl0aNJKTjt9+k3HeZMnl0L8URdZgVRckFKpgVRSk8xMRIDHPZsgBs3eruyjRkwrL8A4svuACQVMcgc+aaNvXuCw6Winh2aEZeU6uWOMCvvSZpLMaPp/U1jQDJoQxQ3SOC+df1NTEGjknl7uznTm7Rwl2vXp1GjeRe1693cz8rZ0EdZkVRcoAKZkVRCg/jx8vSm4oiISEbx3hzGQPOJDlv4TtefDEPx3Y27HKDCQlS9rpsWfpdU45UAgknWrLJ7d4NwPrjtThxQuKXIZshGSB1vW2qVaNRI3ezX79c30HxRh1mRVFygWbJUBSl8OBb9xlJDpEtnnlGSkj36QPAF19IbLKdOi5fuPBCd91bwa9JU4vjAeHUDY2hbVvgsZ+Ird2SqIM1OHw4Bw5z9eruemioI5jLlfO/vHIG1GFWFCUHqGBWFKXwsHatLOfOBVzBvGXLWY57/XW/zWrV5CdfqV0b/vUvUb+hoU5zYNVK3Bn1ATSdB7t2EXPnaPhC5gfa8dU5cpgRJz0oCHr3logT5Qyow6woSi5QwawoSuHg1CnYtEmyX1x2GSAhzSBZ1YoEr7ySoaliw3CIwin7Z264Cb6Q/M92JcBsC+YGDWT5zDOAxHW/+abkkVayiTrMiqLkABXMiqIUDtasEfXiE1tgO8xFRjBnhm8ajubNqdSrNQCrV7vNdurps1K1qlQ39FHYjz+eB2MsCajDrChKLtBJf4qiFA5WrJBlly5OU2wshIT4z+srclxyibt+001UrGTRt28uciZXrZqp+FOyiY+lrKWxFUXJLiqYFUUpHKxeLbP0fNxTn5TMRZdRo6SUYJUqcOedWBZ8843kigafPNHK+SWTLxkBAeowK4qSPTQkQ1GUwsGhQ/45lRGHuUiHY4AItSFD5MdLjRqwYIEkBbn44oIbWokkncOsKIqSHVQwK4pSOIiOhubN/ZqKhWDOgubNM9yucj7JwmEG0dAqnhVFORMakqEoSuYcPCgq4s8/AREVoaHw6qvn6XrR0RniL2Jiiq9gVgoeWyRrWIaiKGdDBbOiKJmzcqUsX3sNTpwg8cfZJCZKquHzQkwMhIf7NcXGFoMYZqVwkS6tXLomRVGUTFHBrChK5gQGynLuXOjWjdBBV9EQKe381FNwxx0ZCvPlnIQESEzMoI6Lc0iGks9kkVYO1GFWFOXsqGBWFCVz7KohAFu3AtCSzQC8/TZ8/TXMnJlH14qOlqWPw2yMCmblPKAOs6IoOUAFs6IomWNXDbn/fvjlFwCas41588SRCwuDdevy6FrHj8vSRzAfPw6pqVC5ch5dQynZqMOsKEouUMGsKErm2A7zBx/ApZcSX6Uul/ELFSuK0GjXDlatyqNr2TWi69YFpEr2b79JU/fueXQNRQF1mBVFyREqmBVFyZyYGChf3intvKXPQwxkHlVP7wVgwACpNRIZmcvrHD0KN94o6/XrYwxUqAC33y4utk/hP0XJOeowK4qSC1QwK4qSOelyum1odhMAVVbMAWDQINHSd9wBKSm5uM7q1e569ep+Ewn79XP0uqLkDeowK4qSA1QwK4qSOZs3Q+PGzuae4GYcoSqhW9YAcMEF8NFHMH8+jBuXi+v884+7HhDgt3nJJbk4r6L4og6zoii5QAWzoigZSUiAtWvhooucppgY2Bl4Adb2bU7b3XdD796SeS7H/PMP1KsHyckA7N/v7howIBfnVZTMUIdZUZQcoIJZUZSMrFkjKSq6dXOaYmJgf2hz2LIFdu+GRo1gzRq6dhVt7dW7sG1b9iy7JUtg714RzF26QHAwACdOyO7p06FJk7y9LaUEow6zoii5QAWzoigZWbZMll27Ok0xMbC6yqWS7+2aa2DPHnjuOa49+QVHksP4Y+Ypvhq1VWI1fOpnjxoFCxemO7/HAz17QsOGkiGjQwemToUJE0QwBwXBtdee/9tUSiA+drItmNVhVhTlbORqOo1lWSOABwADbADuBWoCU4DKwGrgTmNMsmVZIcAXQCfgOHCzMWZvbq6vKMp5YtkyEbPVqztNsbHwT8MboHwb2LBBGufNo8e8eQCMGryNCpzkDiD1xxkEjRpFWprEN48bl06U7Njhf70OHXjpWdi0CapUkewYmRiCipJzMvmH0pAMRVGyS44dZsuyagOPAZ2NMa2BQOAW4DXgHWNMEyAGuN97yP1AjLf9HW8/RVEKmunTpWyfl/h4SF28zC8cA7xJM8ID4OWXpWHMGPjkE5I7igtdj/288vgxAPavjWbQILeAXwbWrPHbjG3YgU2bJCrj2DGpkq0o54VMHGYNyVAU5WzkNiQjCAi1LCsIKAMcBvoBU737Pweu865f693Gu7+/ZamHpCgFijHwxBN+IRQPXnmAoMgDeLpmFMyVKgHXXSci+6mn4N57Cf51NgBdq+/jooaSlLk8p5g6FWbNyuK627f7bW48XhOA556T7dOnc39riuKHOsyKouSCHAtmY8xB4E1gPyKUTyAhGLHGmFRvtwNAbe96bSDCe2yqt78WvVWUguSffyQthV2aGkhYKPHLca38J/wdPgwNGiDC47rroEwZAKzK4XjKlOWJ6/dhRYlgrlIhmcqV4bHH3EsdPepz3Z07nUl+IHP/AK64Ii9vTlEyQR1mRVFyQG5CMiohrnFDoBZQFrgstwOyLGuoZVmrLMtaddTvE1ZRlDxn2jRZRkXBo49CXBzDmMgpynGkZjunm+0U+8wBdLEsAhrUJ+TQHknKDFgnT/L0Iwl+TvHMmYjyvvJK2ejVS3Y89hh79shqhw55e3uK4qAOs6IouSA3k/4GAHuMMUcBLMuaBvQAKlqWFeR1kesAB739DwJ1gQPeEI4wZPKfH8aYScAkgM6dO+vbmKKcT6ZPd9cnTICqVenPH7zKSC48HMLoVyAkBD7/XGqY+KRl9qd+fZg9G9LSoG9fWLCAfi0jke/TUK6caPMHmm2AOXNELD/1FPz2GwQEsOd+qFkTSpeWYdSvf97vXCmpqMOsKEoOyE0M836gm2VZZbyxyP2BzcAC4CZvn7uBmd71n7zbePf/YYx+r1eUAmPbNqnm17mz2/buuwBspiWvvgpTpohYvvhiWL8eypbN4lwNGohYHjgQnn4agPohkc7uoUNh3jw4fSxBGv7zH4m/CAwEy2LvXm+4B/Dww2JCK0qeog6zoii5IDcxzMuRyXtrkJRyAYgz/BzwpGVZO5EY5cneQyYDlb3tTwLP52LciqLklhkzZDl4sNsWEwPAfurx999uc+fOTshy5rRvDxUqwMSJYhUDVVIPO7tvvBHeTRlG2Ru9UVulS/sdvmePZLFTlPOOOsyKouSAXGXJMMa8aIy5wBjT2hhzpzEmyRiz2xhzoTGmiTFmkDEmyds30bvdxLt/d97cgqIoOeLPP6FVK7jrLujTBxYtcnZFUJfUVLdrnTpnOdeQIXDokMRteAVzYNRhLmuwlY31rqDLzFEM4wO3f2goW7ZIgo7kZIiIUMGsnGfUYVYUJRdopT9FKYl4PFKc5KKLpDjJwoXQsycxL/+X7xhMcrW6LFwIFStK97MKZsty4zWqVhUlEhnJ3GE/0Wr/XIJfH+fX/V9jS9OypUSA/P23RHOc9RqKkheow6woSg7IVaU/RVGKGB99JOEQXbpI+EW6WXzLugznFobz5w/QowdMnSr1Sbp3P4drBAZCtWqShy4+XtoqVICTJ50uk78NddbtlHLly+fwnhQlO6jDrChKLlCHWVFKEuPGSQhG//6yfdFFJCfDW2+Jtt20SZpbtZJl//4SueGNssg+NWtKiMaRIzKb7667/Ha/8oYbw2xXyc5yQqGi5CXqMCuKkgNUMCtKScEYcX0rVxYxC9C8OT/9JIktnn9eBHP16tIlV7RvLzHRu3aJ2zxsmN/uHv0zCuYzTipUlNySicNsN6nDrCjK2VDBrCglhdhYmWH33HOS523uXAgIcKImPv1U4oltdzlXDB0KcXFywurVoWVLXr1qqbO7WZsQZ33rVlmqYFbyBR91rCEZiqJkFxXMilKcMUYqgRw5Iu4yQN268OGHcJmkeIuKkua4OEnN3Lt3Hly3a1c37YV36enUxdkdGGSxaZNM9Nu4Udo0JEM5r5zBYdaQDEVRzoYKZkUpzhw6JCWvP/jADcNIF5AcFQWhobB6taR3+/e/8+C6lgWPPCLrt90GQKNm/nOMW7aESy5xxYo6zEq+oA6zoig5QAWzohRnTpyQ5erVsHSpCNlWrRg9GhYvll0HD4qG7thRHN9MjLic8eSTsH27uM1kXu66Qwd3XQWzcl5Rh1lRlFyggllRlixxA2mLG3Yqt1Wr4PffoWNHTOUqvPIK9OoFkyfD999Llrk8x7KgaVNns1u3jF3at3fXNSRDyRfUYVYUJQdoHmalZBMfDz17ynpsLISFFex48hpbMB86JD/PP09cnLv7gQcklHny5MwPz0sCAzO2tWsnutoYdZiV84w6zIqi5AJ1mJWSzdq17vpPPwHy4Xn99XDDDfiVhy6S2CEZNv36+dYPoXlzue18c3cHD/azmsuXhyZNRLiEhJzhOEXJK9RhVhQlB6hgVkouGzdKOTuAoCD47jsAvv0WZsyA6dMla0SRxlcdA7Rt69fUty8EB+fjeL77TlLN+dC+vbjLeRY7rSiZoQ6zoii5QAWzUnIZONBdf+IJ+O034g9E8/zzbvPBg/k/rDwlvWCuVs2vqVOn/B1OZjz5JIwfX9CjUEoM6jAripIDVDArJY+EBAnejYx02wYPhpQUHqj7CwcOwMSJEEwyB/YXcevJVsf/+Q889RRYll+URosWBTMsX7p1g+HDC3oUSrFHHWZFUXKBCmal5PHJJ+4st+HDpeKdVznW4QAg+jmZEC787+0FNcqcM2uW3OPp03JvlSpJdb833wT8TecLLiigMSpKQaEOs6IoOUCzZCglj+nTZfnOOxKKAWAMKUGl6Z36F79xCeFlRUC33jAF+BaQkOfUVP9UaIWSe+6B6Gh4/nk4ehRef91vt+0wP/EEVK6c/8NTlAJBHWZFUXKBCmalZLFoEcyfDyNGuGIZ2LLVolxqFa5iNlcxGyL3Zji0TRtZFno3Ki1NlkePSqqPZ55h4UIxnQMCYOZMqFABXnmlQEepKAWDzwvYFsyF/jWtKEqBo4JZKVmMGSPLG27wa377bRhGVep6QzLYvx+AmOCqrF0A69e7fVNTJalGoSQpSSzksWMlwXKTJoBMrPvnH+nSv7+Y61ooRClRZOIw2yEZ6jArinI2CuvHvqLkPR4PrFwJQ4Y4xUoSEyXL2ZdfwovhaRDt7fvHHwAcC6jGkCEQEeGeZs8evwJ2hYujR2VZrRp07gyIe7Zrl9tl3rwCGJeiFBbUYVYUJQfopD+l5LB9u7ivPoUznnsO+vUTY7bslRe7fb3xCruS6rBrFyQnu7s2bcqn8Z4Jjyfzct5RUbKsVs1p2rtXJvq9+KIU+1OUEskZHGYVzIqinA0VzErJYflyWfoI5s2b3d2VPnkL4uKk9J33GW0ipTOUbPY9psD44gvJ7PHll/7tx47JsmpVp2ndOllefjnUrJlP41OUwkomDrOGZCiKcjZUMCslh2XLZLabTy41O7TikkuQwOSyZeHqq2HRIpLKhhNEKu+/73+aAhPMiYkwaJCEi0yZIm3pBHPU3gRZCQ112tatE2HQunV+DVRRCiHqMCuKkgs0hlkpOSxfDhde6H5K4oZa/Phjur7du1OqdVMuK5dC0N2SqQ2kOGCBhWTs3AlTp0rd7tRUaVuzBozBYPHRRzD/oSS+AxJMaWzJvG6dfDHQSX6KgjrMiqLkCHWYlZJBQoKkuuja1a/51Clo3hzKlct4iBUcTJAnBYDFi0VUt2kjocN25rZ8xQ63sMVy9+5w/Dij797PddfBgw9CCEkAbNwR4hy2fj20a5ffg1WUQoY6zIqi5AJ1mJWSwYYNonI7dfJrPnVKojQyJTjYEac9ekhTTIxERuzdC40bn7/hZootmOfOhZ9/hmuvhUsuYdOXq/mJ+nTtCmP6JcJ4iEkoDUixv127XIdcUUo86jAripID1GFWSgY//STLdGX6Tp6E8uWzOCYoCFJS/JpatZJlgYRl2IK5XTt47z3o2RMTGEgnVgPQsSOUCxaHOfq0OMwxMXJIjRr5PlpFKVyow6woSi5QwawUf9LSpDJJixbQoIHfrrM6zOkEcwupmF0wE/9swWzXsw4NJaFRK+7iC7q1juPJJ6FMoAjm43EimBMTpWvp0vk9WEUppKjDrChKDlDBrBR/Dh6UGOYnnnA+IZOTYelSiIzMXkiGTVgY1K59ngXzvn0Zg6SNEZe8fHkoVcppjq9Qk7oc4Mc2L9GkCZRGFPKxOFHIKpgVxYs6zIqi5AIVzErxZ+9eWTZs6DQNGiRxyUeOQNu2WRyXicMMEpZxXkIyNm+W3G8NGsDHH/vvi4iQKoV9+vg1b+j3OADlj+8BICAlCQ8Wx0/I9AQVzIqSDnWYFUXJASqYleLPHhGTvuEYq1bBVVfB4cPw1FNZHJdJDDNAy5awZct5+JCdMMFV4itW+O/buROA5EdGOE2zZsFLyy/nb7pROvGENCYmkkhp/veeRWqqGOuggllR1GFWFCU3qGBWij9798qHZb16gIjIQ4ckw9wZJ8NlEpIB0KyZnOPw4Twe5/r10KWLCPsDBwDYsQMmTYKZb4lg/mJpE6f7o4/C2rVg1atLUGSENCYlkRoo8cvTpqnDrCgZUIdZUZQcoIJZKf7s2QO1akGICMndu6W5UaOzHJdFSEaVKrK0M1DkCcZI6rtOnaQ6ypIlEBnp5FfeNmcniYSw4lAd55CjR2HoUOh2cwOsfftEHSclUb5qaerVk0QatmD2KfynKCWTTBxmu0kdZkVRzoYKZqX4s3evXzjGrl2yPGse5SwEc8WKssxTwXzgAJw4IZVRnn4akpJgzBiiouD662HYwJ0cDmlIxEF5ySYkyE94OHDxxdL/r78gMRErJIThw2HRIgl7BnWYFcXBRx1rSIaiKNlFBbNSfNm4EWrWhD//9LOTsy2Ys4hhrlRJlrGxeTROEHcZZAZis2YwdChm0iT+Ff0UrZsmUT5qF9HhTZyxR0fLsnJlZCKgZcHff4twDgnhvvvEUJ8wQfqpYFZKPGdwmDUkQ1GUs6GCWSleHDwIf/wh6199JbmLX3oJRo92uuzaJank7HTGWZJFDLMtmPPUYbYFc+vWshwzhrTLr2aEeZte2z6GnTtJa9iYHTskdvr4cekWHg6ULSsZQLZskRiM0qUJD4e+fV1hrYJZUbyow6woSg5QwawUL8aNgwEDJA3G4sUys+/FF6FpUwDi4mDmTClAkonh5E9+hmSsXw9167onr1yZqInT2EETLp77LMTHE37nVYDMXbz/fqeb0KKFxGDs2OHEal98sXt6FcxKiUcdZkVRcoEKZqV4sXGj2EUPPihVSWrX9tv9008SLvzyy9k4VxYhGbamzbOQjMWL4ZtvMsxCjD1hsZIuBCfHQ7t2NHmwPzNmyJzAVaukjyOYH35YqrFs3uzU+m7e3D2XCmZF8aIOs6IoOUAFs1J8MEbyGDdoAGvWSOxFeDizZol5GxcHP/8sqeQGDszG+eyQjHSfpoGBoknPWTB7PG55a1+mTZPlQw/5NcfGwgbayMZTT4Flce21MGaM28fR2FdcIbnyZs+Gd98FoImbgc42nRWl5KIOs6IouUAFs1J8iI+XoN077nDbKlfmiSfEVV62TObF9e7tOktnJDhYlunLVCNhw/Hx5zi+zz4Ttf711/5j/v576N4dbrnFaU5NldCRr7iDg/eOgptvdvbZYc4A5cqlG+8VVzgdfA3roKBzHKuiFFfUYVYUJQeoYFaKDye81e7q1HHFbuXKTjjC9OmSYa5bt2yezz5HJmEZZcrkQDCvXi3i+847xeoGWR48CPfc43QzBoYPhzfegFoX1qXcO2OhVClnv30/3siLLClTBqpVO8cxKkpxRR1mRVFygQpmpfhgC+awMPkBPBXDncrYn34qy2wLZtu+PXkyw64cCebt2yVtXO3a8OGHkJTEya2HAIjtd4PTbdw4+OADeO45WL7cuRU/Dh1yK36fiV27YOvWcxynohRn1GFWFCUHqGBWig92UHFYmOQmBg4kViEhQZoTEsQ07tAhm+ez7dmjRzPsypFg3rZNCpNcfbU4yxUrsnXBYZIoxRNjwgHYt08y4F19Nbz6atanqlkzG2nxEM3vO/lPUUos6jAripILVDArRZv/+z94801Z93WYv/oKPv+cJaEDADfFWps255AxompVWR45kmHX0COv8OyqwdlXzadPQ0SEqNerJD0ciYkE7N9LJDX49TeL+fPh1ltl1xNPZDPOWlGUc0MdZkVRcoBOBVKKNiNGyLJFC0mDAZL3rXRpuOsuVj8tq48+CgsXQvXq53BuWzCnd5iTk7l/r7cQytNV4P0fJEJuAAAgAElEQVT3z36unTtl2bw59O8vaSuSkmi8bz7baEJkJFx2mVsnpV69cxinoihnRx1mRSk0pKXJ686eKlQUUA9LKR5cdZVU9APSyoUxYYIYzmvWiKt8ww0S4vDOO+dwTjskI73D7K1PfTSoBkycCDNnYowk5xg40NXtfmzfLsumTUUsr10LQFBaEvHX3U5IiL/LVbfuOYxTUZTs4/NCswWzOsyKkr/06OE3l71IoIJZKbokJ7vrdes6s9um/xHGo4+K0bxgAXTsKB+MI0eeYzxveLgEK3/6qcRIHDwo7Rs3AvBw1anQrh088wzz5km2uHnzpDhKBg4fdscJcMEFjLxmE03KRXHhl8OZP9//OM2brCh5TCYOs4ZkKEo+sm2bVN41huXLC3ow544KZqVoYIwIVY8HVqwQ29jOZ3z//ZIy4pVXYMQIvp5R1u/QbE/yS09goORO/ucfKQZSp47EdQwbRmyZWqxK6wADB2IiIhg5UrRwlSpSWMQOrXA4ckQ+ncNlcl9sLLzza0tuvLMM5crJt+0BA8SA/uCDHI5XUZSzk4nDrCEZipIP9O4tH5CZTKR3SE2Vwgn2bP1ChApmpWjw448iki+4ALp2FfH8wAOy78YbITCQv/v9i9si3+bn2RZVqriHXnRRLq47aBD8+ae73bcvVKjAhJv/4lh8GTYcroyVmMjm1fG89BK88IJ8iT5wIN15jhyRmGivpbVlCyQlwZVXul1KlZLIjQcfzMV4FUXJHHWYFaVgePhhqT9ghzfGxGTdd98+cZ++/z5/xnYOqGBWigbz50t5vR073DavLfTMxEa0by/F8r79Vr6g/vGHvCZ37JDUx7mid29ITIRLL4X27WHxYhJqNSYuDt79WpR5FY5x2WVudb3jx9Od4+hRvyoikZGyrFUrl2NTFOXcUIdZUfKXiRMlc5WNzwfk99+ne/3ZznJoaP6M7RxQwawUDZYskbgFm0mTAPBg8d9ZDZwUzDZt2kgMc5MmeXT9kBCYM0dmEdaqRYUK0nwcSYZcmeN+uZGPH0fUul0l8MiRTAVzjRp5ND5FUc6MOsyKUjiIjnZWb74ZPvrIZ5/9eNaeKF+IUMGsFH4mTIANG0QwewuSmBtvYk+ZluynHsmEMHQo/PXXeR5HQIDzofvAA6Kfv58vDvP/jTqOZbmCOWDJImjWDIYNg3XrZNKfN6edMbJpWW7mOkVR8gl1mBUlf7G/mdqz2dM9gv37b58NO775nCuDnX80D7NS+Hn0UVn27w/Dh8Pq1cxfU4nR8R9TCYmFKlcuf8MbwsPh8suBzaKQ+7Q6BjNnUj20NtCZmvO+kI6TJ8O0aXDqFJF9bmb0EHni9PXXYjgH6StQUfIHdZgVpWAIDpZJO23bwsqVGQTz4sU+G3YBMvsxbiFCP64z4fhxmD4d7rtPq60VCipVkol+3pCM+O4DeLo7RNe9iFc+hbkDoF8/KRcNEnKcb9iW8r33QmIiFXv3ARZS/sAWAA7U6EydyFUAfPxbPT4+5B7qTZihKEp+og6zouQv9mtu5UpZ2ilakfl9u3ZJoquGDXEnBBZCN0nlYCY88wwMGZJFPl0lfzl5Ul5Affs6TePHS5TDuHFiOhsDrVtLyuRFi2DmzHwcn616ExOhQgUCtmymSmVDeNQWPmQodSNXknjxpQBsOOFfvq8Qvh8oSvFFHWZFKRiMkQk7dkjG/PnOrogIWf7+u7fhkNdVsqvjFiJUMGeCnVD7jTcKdhwKsHu3LBs2dJrWrZMCJHfembF7z54y2S/f8K3ref31cPQoN7bcQrmkaLbQAoCfu43DtGjBkoSOjBwJjz8u3fXphaIUAOowK0r+YoyYX0lJsn30KIGB7u7QUJ+wDDskI18/yLOHfmSnw+MRd7l3b1i6NF1sTUGQmFgoZ4vmGxs2yLJVK6dpxw5o2bKAxnMmLrkEgFtKTQNgKxcAMHFFJxJXb+agpybly0tKSpAMdYqi5BPqMCtKwZGY6K7HxREW5m6mpropmp0MGmlp+Ta07KKCOR0BAVIFec4cCU997bUCHtCDD4qdeupUAQ+kgFi7Vh7jNGsGyBea3bvzMF1cXtKrFwCd9v0IwBZa0KOH5IT+5RfpUr683MqcOfD++wU1UEUpwajDrCj5i8fj/yJLTPT7kpqSIvVKADeG2XaaCxEqmLOgbFmZ9DdnDiQnF+BAfv5Zlj4uszFSjn321ITi7z6vWydJlb0BvzEx8veoXbuAx5UZdepAhQqU37mW05Qhgrq8/rpkk7MTfZQvL8vLL5f/MUVR8olMHGa7SR1mRTmPpH+BpaaSmurfdPiwd8U2BwthkQIVzGegTRv5UrRnTwENICLCfTyxdavTPGOGlGOvNOwWcZ8LVNGfR4wRwdyundNkP7bxpjQuHPz2m1QxsiwnzmIbzTEE0Lw5jBrlzmOwBbOiKAWEz4e3hmQoSj5gv8DsyfseD2mp/i+6ESdflpl/dqU/O+1VIUIF8xmwH/v7VmPONyIi4NZb3e0NGzj5wng61zvCDTdIU/dj3jQehTDBd55w+DAcO5apYPYpmlfwDBwIt98u6xdeCMCxShJCEh4ucwFtVDArSgFxBodZQzIUJR+wZ7wDTVIl9ap8aTW8aF6SeUB2rHOhcsUEFcxnoHFjWea7w/zrr1CvnpSDtnnzTSqMf4FBEW8BcPfdPv1Pn3ZWV66Ezp1lQmqRZ+1aWXpdW2PcSZiFSjD7cuut0KsXvT68g1275APZN3xEBbOiFDDqMCtK/hMYCJ06OZtXponhd801EIZPvHKhdMUEFcxnwJ7Fme/z7ezk3iBlIq+4wpkx2pE1vPoq3HGH22X/Flcwf/wxrF7tF8FRtNi3T9JIVKsGV14JQPIFbRkzRkKER42SboXwtSR07Ah//UXooKto1MhttkVzmTIFMyxFKfGow6woBYdl+X1wX+xZAMCll0J1It1+9ouxEH7Ia+mEM1CqFDQP2EGNjfuAAfl3YTth96efQpUqmDZtSZk7jyWmO+1Yx8CRPjNKgV+nnWbIAHFJ7DmCkZEZT1skuOkmiVtOSZHtSy7h+fFhvPOObA4eLJUY7QJ7RYXFiyWvd4sWBT0SRSnhqMOsKAVDqVLOanv+AcR0bkgmj/EL4Ye8OsxnwLJgq6cZ9307MH8vvGGDxMXecw8Ac9q/QGuzgVM9r6AaRyEmhrp13e7ffxbPwYOwZo07ucyZcVrU2LZNHOZJk2DLFvj1V5YulV0tW8J338G8efglPS8KNGgAEyb41zlRFCUfUYdZUQoO+8Xm/fCuxlF6soiaJ7fRLGC328+uBlgI4xdVMOeA996Dpk3JkBYlT4iPF+Xbtq3TNOGL8iTXb8aVI2QiGdu3+1WJC0k9zbPPwqxZrmNSZBxmY+DNNyU9XkKCxL9Ury7VYy6Qwh+RkRKd8fffBTxWRVGKPuowK0r+Ywvm4GDnhbeI3tQZcAG38K3bLykJA35udGFBBfOZsMMCfDhwAJ5/XqIm9u8/D9e0U2C0aQNIme5ff4Wbb4bACzvJP91nn0FUlHPI9ZecZupUyRndpQtUqVKEBPPevfDMM2IfewUy1arh8cDrr0vGtshI2V2hQoGOVFGUoow6zIpScNgvtlKlMrwWu3vcBAdxleuRYJVhzpz8HFz20Bjm9Bgjz/zLls3w3H/PHvjXv9ykFLt24TexK9ccPy7qODRUYnmBn36SIY0cCVSsI8r5gw/kx0vHC+JJni1zBe++W+YJFpnCgHY6j7Aw5xuIqVadihX976EQpmRUFKUoks5Otix1mBXlvGF/G7VFcunSfpm90lPmeASTSz9C9/r5MLZzRB3m9FiWlKN+7TU/Ufr99yKOv/1WKgDCecjP/K33scSyZU4ZuAMHoG5dqFjR2+err+DPP/0Oa1n/NHXqyHr9+pKJIVupmR94oOCVqK2Kv/0W7rwTgISg8pw6Ja8rb1rjwlj0R1GUokQmDrPdrA6zopwn7Cf19uuvTBkn61d6Pmn9Fiu/2Ar//Z9TB6MwoYI5M3r1Emv3s8+cprlzXAvitddkefRoHl/3s8+gQwe/+OUDB3DEMCCud+/ecNddTlNI6mmnOEZQkGjtbAnmyZMl3sFbWSc2VkIg8hVbMFeoAO+/D++9x9ELegEySe7nn+GJJ2BAPiYpURSlGJPOTg4IUIdZUc4bdiViWzCXK+fsOoTrhMUSxv0bR9D3wWYMHQoffZSfg8weKpgz47rrZKbmhx/ydctxACxZ6MYzV6kicetJSTk4986dbiUbLzt2wFfj9kkCZd8Ey4hg9i184fDRR+JEWxbExnLvvdJ82WXyBe4MTzwysnGjc8pLL3W/EOQLdkhG+fLyQnrkEaJj5d8yPByqVoV33pGloihKXqMOs6KcR2zBbM+wbdjQ2VXLm3950tU/0YQdgEVKirwm/YqzFRJUMGfG9deL8zl0KIGlJQ/YwX0pjB7tZmooVSoHgjkqStJrtG3rWBpxcdCsGfw2yhtm4WOl7tghcdLNmmVyrlKloGtXKUe4eTMdOsgpu3Q5h5AMG28pQzsl3fPPSwGUfMF2mH1SyERHyzI8PJ/GoChKyUFjmBUl/0jvME+aBH36AHCImjB1Kke7Xs1xxBW77DJ5PU6eXBCDPTMqmLPCmzC3XhNJbTLi4WTGjIFu3WR3SEgOBLNtA+/YAb//jjEwdiyU4xTP8RrJ5SpB69akpcHUqfDccxLH++ijZzhn27awfr1f05lCMmJjM2n0itaoKImB7t0bRo8+x3vLKSqYFUXJLzKJYw4IUIdZUc4b6QVzjRqwYAFtWUddDsCNNzJsmNv955+la2FMXKCC+Sx07yPC+eVR/inmzlkwJybC3LmyXrMmvPoqV18tqdM+bfYfWrGZU6HVICCA776DQYNg+nR49tmzTHhr21YEuI9CLlMGmkYvzzAr8YUXJLTBLtVuc2h7HP37Syrk2rWhRw84diwfXJeFC8VCBxXMiqIUCOowK8p5xJ7051s8wrLYQFunLTwcnnoKvv5anronJeWjaXcOqGA+G16nOdCTA8F8+DCnL7+JW/tGErVSUqZN7vMF+295Fv78k8jZqwC48iJRiNM7vwr4TyZ86qmzXMMO79i0yWmqEJLET0e6SSyHN0b444/htfFpDEidS9JL4zl22L2fjd+sY8QfV7F99UmqVZOMHKmpGYzrvOX336FvX5nZFxoKQUF8+aXMtZw2TfRzlSrn8fqKohRd/vkHhg93q0clJMhjuewoX530pyj5R/oYZh98H/i8+Sbcdps8IS+sFXE1D/PZsKvN2H90LxkE8xtvSIW68eNJuO0+2n35DE/t/w8PJv1Iaa7kqzE1eQr4/M/6vLr0YnYxgk6s5okvOxM69yT7ghoxZO4N/LetVMYGWLHCyS6XNXZGjXXrJIAZaBS/0d3/889EhDTh2wfjGV91Jc8efRYmQp8vL8VOTtfnwNeEkEwbNlCtWg8nhV379ufpg+T4cf+I/mrV2LvXL/EH//tfoSz0oyhKQWOMzPWIjpYnVDNnQqtWMhfjxhtFOGdFFsVLNCRDUc4TZxDMRY1cCWbLsioCHwOtAQPcB2wDvgMaAHuBwcaYGMuyLOBd4AogHrjHGLMmN9fPF+yvOilZOMyxsZIs2Cf8IfTfzxLH7dwTKHlRPuU+PPPkjXovDTiYUosUgqjPfi6/HPj8CEeoDrinadnS0b9nxp5xapeS7tmTBrFr3f1Tp1J3+nTmA0cbXA5e97pD4lL3XpB/6CX0ZGLTOCpWdFX60qViAHfokI2xZJeffoLDhyWx9e7d0KkTmze7u1u3xi+mSVEUxeHLL924rblzJdbMO3HZ/Pgj1qJFkho0K9RhVpT8w3YWz+IwFwVyK/nfBX4xxlwAtAO2AM8D840xTYH53m2Ay4Gm3p+hwMRcXjt/OJtg/uGHTCuYHKI2pTyJpCLVApcF9uRtRnCAOpSrEIiF4QVepfLEVyAqisNp1QBn8ij16mVzfL7/hLfcAkDQnh0kUYoJPCyB0F7CT+1z1u+uOjfT0/UN/NMtkoLEM3fsmGmV8Jxjf9hdeqkse/d2JiP26CElvtMVWVQUpSRjDCxeLJMrnn5a3oBt3nzTr+vpvlcRH5tMSgpM/ySaxKM+s4fUYVaU/MXOcevzoW5/QS0xgtmyrDCgNzAZwBiTbIyJBa4FPvd2+xy4zrt+LfCFEZYBFS3LKvwFj88WkrFokTRccQWULs3hbxc6faxevfjt4RkklanIgbGf8hRvYwjgyBEICPb+80ycCHv3csjIzD5bMJ81FMOXESNk6Z3416HibvZRn++42a9b4NbNjC71GskE0y7qVwAighv69Wnq2e47/85h48aMbTkmNlaE/ksvScDyo49y4oTsmjpVKhsqiqIAUhXsgQfENa5aVUSz7Vql+2a9hg6UTTtJaHgoN/eJ5Jr7qxJSrQJ8/rnbyRhO7o3GE1YJFi9Wh1lRzifewmi+6jh9teyiQm4c5obIA/5PLcv6x7Ksjy3LKgtUN8Yc9vaJBG+sAdQGInyOP+BtK9ycwWFOTjIwfz4MHiy5UKKj2VOvD43Zyd5rH4fJk7liwlWExEUzeGRjTpyAVavk2IDFiyTm4tAhiI+n08QhDBvmVqo+p/jdt9+WGOqYGIiNpVbCbppe2ogjTXsyuc6LxAZUcro+OH8wqd9NI6bTAH7gJqKDqvudKnDb5kwD7o8dO4fxnI3YWAgLg2rVJOd1YKDjMPu624qilHCMkfkOn3zi3+Yl1ZI3q2SCWU1H2iITQCzjodrfMwjEgwV4HhzGv0cbjGWRmAjDuqwi4GQsjBmjDnMeEREh01MUxQ87g5fPl1u7MnZJEsxBQEdgojGmA3AaN/wCAGOMQWKbs41lWUMty1plWdaqo3leezoHnEEw1zixTQRv//7ylw8N5dAh2E1jTo75P5xi6N7/igoVoFMn7wkuvFBmeQOMGkWXhzrx/vsShgxyynOiUSNZbtoEW7ZgNW/Og8MCeODASzzm+T/Z1749dXo2oMzgq9jx318YzA+cCg73r4wSFUWnTm4M8WOPyTIvBLMx3gwgsbEZlHFsrHxJKF0699dRFKWY8OuvkmvKp6ATQHLVWtzAjwSlStXUOVzOPbXncSSwltPnCuY46wFJCfz6ynLS0uCdtw0xx+T93EQdKR4O86pVUhTrmWdISBBD3i5ElV/Uq+d+5CmKgwpmQBziA8aY5d7tqYiAjrJDLbxLO+vvQcD3YXsdb5sfxphJxpjOxpjOVQtDPWRbMGcSktEh5g/Z6NfPabffpGrV4uzccIOUs/nXv5ymbt2kevZ9953jONu3l+XDD8s/6OWXc/fdMs6j3go6XHed071FC8nX7HnrHYmDiIyUi0dFYVnw/vvw118ynwbczB3ZxhiphOJl0yaJT65WDWL3ZhTMJ06ou6woSjp++EE+aOfNc9tWreLhaw4ynetZgcyM3hfUlI0HK/FZ2h1Ot0uD/vA71UN8gMfIJ3RLtgBgdu8p2g5zQoI8YXzmGfngePNNFs8+weTJklP/ssvku4b9VDzPmDoVtm1zNu0vHJkWxlJKNonypdZ3vlVRTZyR4+EaYyKBCMuymnub+gObgZ8AO2fY3cBM7/pPwF2W0A044RO6UXixYyPSOcylSkHLuOUSQ9G4sdMeESEuaeXK2Th32bKijNPFQDRunINvXo0aSanszZslY8aAAYSHy+nT2naU4GifVG5hYRKL3/uBZtCmjaTEa9pUhPNff8Hs2fTq5d7H+PHyfpxt3n5bKq7s3QtIuPI//8iuxMjMHeawsHO8Z0VRii/z5kkoRlqavDnMmCHvb506MX8+gEXACyMBiKvWiBo1YBFudoxSqaeddQ8WA7FFtyEQyd9s0tKKtsM8ebIk61+40Gkqs8t1N9atk6hB+703Tzh9WiprtWkDH0kmKF+hbOsjRQHcfwgfh/kMiTMKNbkd7nDga8uy1gPtgVeB/wADLcvaAQzwbgPMAXYDO4GPgIdzee384QwhGVWTD0otaR91u3u3ZHorkEcNs2eLYp80CYIkY+B778HcNdXlDbVBgzMfX7067Nsn4nrwYPB47NMA8N135zCWN96QpTfd05490LOnNEXvisET5grmlBR5Y9dCJYpSQpkzB26+2fmCDUi1JZsRI+Daa5m5vQXt20u34GB4ZcO1PBb2OW8cuo0xY2DyirbOIfFV67vrwRWozUEMFrVqQqO68kzYpHqKtsP8xx8Zmlr+90Fe5CVq+8wQWro0Q7fsYQx8/72fIj7y/QJZSUmBoUMhLY2DPs+KfYxnRXFDMnzERIlzmAGMMWu94RNtjTHXGWNijDHHjTH9jTFNjTEDjDHR3r7GGPOIMaaxMaaNMWZV3tzCeeYMIRlVUw9niL3YtcsNJ853KlfOUEc7IOAcUrS1aSP/1D16yD/5PklDZ2fu+PprfycmIsK/KqGDx+OGYxw4AMip7N9LZXOUiEQ33Oa112DrVnmqqChKCWTGDBFmbdqIi/z335i5v7j7+/Zl/37JnLlrl7yvpaTAzFkB/O/EXTz6r4o88ADU6uQmXiobIypuNw2JTSmHBQTg4fBhw6EIrwHiKeIOs+8XDC8VD23mJV6m88EZztvw2LHwyy8Zup6dTz+VLzL33AOIM/jVffP8+/z7335ZlPI0o5JS9LG1k48QsZtKUgxzySCLkIyQEKiedshNa4GIvm3b/CI0ihZ33CGltF97Tba91UQWLpTsd1u2iBNsU6+eW2gQJH57/Xr8O+3ZQ/zJVI4dEzN+5LNpVOEYe05Ldo6oKBgzRt6Tr7/+/N6eoiiFlJgYea+Ni5Nvzj17kpyU5u6/6CIeflie7gYFQfPmUlApMVGeXo0dKx++VoBF2rQZckxaGgQEsLHGACoiDqmFoWtXqFvF+5jY4zrM997rn32u0JGQgJ+VO2WKG2vhkwvU1iCfcK+jUdLS5KGhz7QS2L5dhLA3tUVCgvzqp0zxftwdOODO/p4pkZVz58IgpJKi8x3j1Vf5ZugC57R33AG33y5fbBQlM4c5k3mARQIVzGfDTtvw0EPw8stOc7XyCVQ0sSRUFMH8yScyZy4szM0sUeQICJBPoZYtZdun/N5NN8n/+zffwCuvSLgzSMizTbdu0K4deFb5FHB88UWsJo0IJpk2beDVJ48RiIdtsSKYv/9e3pyfffZ835yiKIWWnTtd22n5cvB4SEaKkzzefSUEB/PXX7L7kUfky3uXLmJcNGjg71QF9vbGfgUFwQcf0OuRdpRD4pmDSKNhvTS6tpFP7ACT5gjmL7+U7BJ//pkfN5xNPB7JptS4sTxBbNIEYmP5Z/YhPLfe5nQzNWqwOKA3R6lMEvJUNJxYLmwl9336tHwXefFFn3Nfc418Q6hSBXr04NBNj/Hmm3DrrdCgdgrH67X3e7Jqdu7i44kp1PHO1fc1B0ecGuust2oF335byL98KPmH/T/kI5jtSaglKiSjRNCwoaSMqF9fAoK9z+6ubbgegH9ON2P+fLj/fsn08NtvRdhhtqlUSZxzH8FcpYoU5nvjDRg9WpyK0iQQVjrJ6RPhzbIdvWy7uEU//wz330/o0Qh6llopmaG8Fse+pBr88Qc8/rj8au0kH4qilAAGDoTLLwfEvDy2NoI0++PI63iWSzrOThrz/Y4OGCOCLyQExo0TH+Pmm7M4d+XKMhlt0SIYMoRKF7fz2127wikqhsgndgCGlCQPaWniwqamwl13nZc7Pjc2bZIYuNmz5XNn925RGYmJvHzlCspedTEBPhlbrR07mOgZSmdWs5xuTvtvb6x11o2R6S1btiCPQn2DjZcupdGc/1GLAzRrBteV+Y3Kxj+p8uJLxxDx2yZHKJ8KdGdpl8Gtprhpk8xnX7sWRXFn+GUimNVhLm5YljyWGj5ckhF/8AEsW0areZLbeEnwxYwaJV23bPEPUSjStGzpCuZXX4UWLbijtvvY7Yor4EjpeqxKcW84OCCNUYwlaOZUcUKuvNIJ77i8whJCQ4HDkhhlZ1x1vvlG3liXLi163zQVRTlHjBEbt3NnyYDxyy+k7NrPM89AKPHEWeX9kvbHWeUZyO/Engpk/345PClJ3o5PnpQMmlnywAOSNQgkLhow3pQ/wUmnKRfg5lkrn3jUL7PD/v1y/gKlY0eJbfCxhI23HPg9S4fQjB1+3dfd/TbfcDv7qc8l/Oa0l135p1+omzEw7zePE5PstCOO8bO8zvbt0H7fNKfd/iLTa/cXTAp4CIAkgnnLPOUcX4VjdGUZDdlNffbyV1wHUmfNsZNo8PDD8OGHufh9KEUXWx37VGOz6zqoYC6u2GEKDz8MF11E4PdT+KzMMLYer8r+/fL+YxcdKRY0bizBgUOGSJ7orVu58ctrnd2pqVA+8RhN0rZjkpIxBtoFb2Ys/ybxRDJLW97P6dPw/neViaEiTUrtlwOXLcODxfITLdi4UR6rZitntaIoRZuJE+GuuzBr3JCt6Ze8z44dEEIycZR3XWbgQesj9tKQxER5zA8SMTZmTIZMnGcmLAzq18ey8/pHR1M+2BXMVRMjMqRC8853LhiOHnUfY9sxyqVK4bHkd1Of/RmqgX0a79rtSZRmIy2d46dMkXzMNnU/GwvLlrkNl12GhYjjwXxHWBjciAjm/dRlCoOd63X1SNmFg9RirGcksVQAoDF7WcZF7KYx8+hPB9byNG8wdKh8jEycKFGNJZqICHj6aeefKzUVJkyQkKLvvy/YoZ1XoqNlWaGC0/T337IsaqlkVTBnF5/JfTRsCF268GnLN9m1S0zTunWzPrRIUreuvHH7pHYKTjhFWeIAKB/ixrYlrN1GTAyUTZJHeLemfkGPqU9SrpzEGx6kNtcfmgBVq8JLLxFZpwv7T1dmzZpi5MgripI1u3djHnkUgF/K3OA0t9k9nSBSCCKNgNBgUr3xtwaY7rnG6WdPEurSJYfpJ9u2lZg5gJMnCUpxFXLNlP3Exfl3z80owoAAACAASURBVCT5RP6xKpMEUv/7H0t7POdsphfMPy/z/6Ucxvt59fvvlPq/1/nyc8mbF0wSV6wd53asXp3jX83l3+H/wwKqc4T/BI8i3DtJ8g2e43/WCNInM9hAG6rVCGIyD2QYahN2A9CDJYD/k4A8L6BSVEhNlSeub70lj2eNYeRIePRR0c///re3n8cj4Td5hDGFIANMTIwsK1VymhYtkmW6pF6FHhXM2cX3L/v007BiBdUblmHFCvmHLJaC2RdvYF9fFlC2LLzxpFtz5tSOSLZtg3Dkm2Q04c6+K68UwQzIp15AALtvk8qGffroZD9FKbbExMj7xvHj8OSTWF6ZVzdhu9OlMXucDBbV4/dyDAmbSAsuTSKhTj9jJDqud+8cjqVtWyccjLg4v+oaVzLLEeQ29nyMAsE3yxBAWBiLG9/NR3+4k2MCgJjgas72rgh53G0Xmoqgjqx4PPDcc1TZuJD69eFWvqUUbsYnzz33MWgQvBV9H8Z73oeOjXME+Y/cwHJzIWMY5Tek9aEXceQIzKN/lrcRQgpXM9Mvnd3nn8s8nxLH9OluudzNm0n7birvvy/eG0g4+alTiNXcuLHkJfeyejWsWOE/wT67tGoFHTrkfvi5wv6i6iOY7fD58PBM+hdiVDBnF2/8GOCI52eflQlrIGmOihXpBfOll0LZskwfOpdTp6BK4gFnV/U7L2HH6pOOYG7QwX0VPPooHLG8XzYeeghOnqT7+GvYvFneODUcQ1GKKaNHS8xylSpOWjKA1p4NPB8ohY1KkUIvJP1FAIYwJHg4ItgVhxUryvuIMdCiRQ7HYldNAkkZkeROVm7MLscEs7FjLAsEn8nWTJrE1rl76Hd5CFuM/4dMhXnTMhz6xRcyHyTS8poU9n0+/DDv1nmD1/BxqTt0JOi1V1iwAOIpQyrupCwLSKlYhShLnOqXeYnNuDGHd/1wDU88Ab9wOcP5P9KszINRv+IOv+1hw+Cqq2SCZYli7Fi/Tc+IJ0mMT7PregHeyZh2Od233wYkNV/nzhKOX6vWWYrCPPlkhkDx9KlgC4RT3gmh3pCMEyfcOjh16hTQmHKKMabQ/nTq1MkUKuwnHEuWOE2pqcZs2mSMx1OA4zofnDxpzM03GzN5stzzgQPGXHONMfXry80OGeL+PsA8xPvOuudUnImKMmbcOPn9mD//NKZhQ2MWLCjgm1IUJd9o3959j2jQwBgwG2lhJj+3zVQlytm3gD5+7yUGzOfcmb7J9O5tzOnTuRhPt25yojp1jGnXzjnxelqbKlX8rzVsWF79Eowxc+caExvrbH78sTHTpp2h/0UXySAqVDDGGPPRR7JZk4PueyyW9PVu2+OOjDTmvvuMebH5txl+p+l/OgWscTaDgozZTX3ZCAw0JiDAmA8/NMOGuYdcEzDL3TB+lzeL/rcmy+sEkOpshoTI8uDBPPz9Fnb27cv099KXeaZUKbfp9deN+7f3/o5//llW77pLlrNmneE6YWHS6bPPnCb7VCdPnt9bPCPh4TKIceOMMcb89ps7rvfeK8BxZQGwymShSQtcFJ/pp9AJ5i++EMF47FhBj6Rg+OAD+ZeZOVPeVIcPd/7zI6nmvgqK3bcHRVHOGV8VGhhoDJgB/GrAmF4XpXiFHyYt2Ec1eH+eY7yxLNEPAQHG9OxpTFxcLsczYkSmwuUw1U17/jH9+N1pvu46Y7ZsMaZ7d9E7Oea99/wUeHy8cUTSG29kcUz16tKhSRNjjHFEa4iV5I7bEsH8ceeJ5m4+NaVLG1Oxos85Dhw4o1j+nDsyfEn44spv3I2ZM40xxqSlOX8606e3x5jBgx3hY4zbPT7e5/cbEOB34mZsyTCEFSty8Tstajz2WKZ/gw95wK+pfXtjTO3aTkNCQsbDXnkli2t4PG6n0qWNSUkxxhSC3/fu3e4gvP/wzz3nNk2aVEDjOgNnEswaknEu3HmnzAaxA8VKGt68qVx7rYSo/Otfzq7qHHH7FbV6l4qi5DnGG+fg6Xyh8wx+B80YOhQW/R3ECSqQGlSagBTvBOLWrZ1jA0nDGEnSExEBCxZICspckUmCfAOEcYLVdGQ+A2lXSbL5REXJ/KylSyWrZo4ZPlyWU6YAklY5ORnqlo8l/pnRrPxcwi88Hrj7bnhv5AG3HF+zZqSkSIYJgHLhpZgecKNsXHklAHctfYjEm+8hMREuvtjnurVqSRqvLGZIfsUdfmEn9evD4B9ultzP69ZJURMkvOOSS6TPbbdb8N138MILznF//y1NoaHAfffh3IwPF7GM9GzfnqHJD2PwC1cosng8MHmyu3399U4O1QH8Tv/+UoERvCEZPsH0S5e6h9lZYbZuzeI6p0+764mJUqXRhwKruug7Dm+mMZ/oLL9cCkWCrJR0YfgpdA6zYkyrVq4FY4wxJ04YM2WKtFWtepZnjYqiFFtSU42JjjYJ8R5zZF+8YyMtp4tJJNgkE2QCSDXJyfKwan9gA5Nco45rN91/vxl142azk0amIbsMGHP4cB6Ob9q0M7quBsw9zZcYMKZFC2OqeR+a9e+fw+tt3ep//q5dzZstPzbgMWtpYwyY1bWvNsYY88MP0mUKg9z+ixebGTPczeBgY2bPNsbs3GlMYqJzmdRUMe+WL093/f79s7zPstZpv6Z33836NvbsMea774xJSjrL/Xo88vnw9tvG3HSTc/KvQu4zlSr5D2H06DOfapY3+mPGjLNcs7Bz+LBxngiAMevWiU0MJp4Q8/HHxsyfb8wVzDKnCfX7JdlhGL4/vXplcZ2DB/07/vijSfJ5IDF8eL7etcuHH7qDOH7ceDzuA4iqVY1JTi6gcZ0B1GFW8owrrpClPb23QgWZxVG9ukxs8M2SryhK8WfHDnEyg4IgPJzdjQbwZv3/OrsvZCVBpLGKzpSrEEhwMDz4INTtXJ3gqu7MeZo2pVb/FjRhFxFBjWjTJo/TTlWvftYuLWqIK75vHxzxPjTLcZavMWP8t5cvZ9iWxxjLKNohGRPaH5wFsbG8/joEkcJ1zMRj5xPp2pXly93Df/jB+/bbuLHfJPTAQEncdOGF6a4/c6ZUYLFLIgYEwOWXc7B+N06bMn5d+/bN+jYaNBAX1KfuROZYFmzcCCNGwI03Os03NVrN2rX+Br9fnuvRo+V/5+mnnUmKtpM6adJZrlnYsW/UrnLXvDmegZcCEEoSfQ98SefSG5jN1ZTBP+fetC/c6jn24Vn+Lx4+7L+9fr0z1w7cBB35ju9jgkqViItzH0AMG3aO+dQLASqYlXPjhRfkDfjBB922smXh4EEYOrTgxqUoSv4zaBA0ayap47w5olpG/sFrPA/ANpoyhEmkEMx8+tOokc+xLVr458pq1Ihq3kxpqann4bt3NgRz0yoxjOd5fol3s2rYoQsvvnj2UAI/Zs/2L2EaGEhpE88LjHeaAoD4YU+yYQOM53lCSMYACaXCSCXIr6CFb/GRbFG2LJQvD598Ar//Lo/H58yh7Nq/eeopyVL05JPQv38uso9kRTu3HHnI7q3Uq5XKtm2uMPcTfl9+KSE7b70lNc+nTGHGDNk1Z46TMKJoYufUTkmRzFMhIRyt7GYbafTSXVTokXkxAt+KjXb046FD+Alhh/TxK6tW+eUWX7kyQ6RM/mB/6yxTBizLb5i+EqLIkJX1XBh+NCRDURSlkJI+5ABMWkCgSSLQbKWJ8YD5mPtk/lrlaBNMkv+j4ddf9z9+4UKzZIm7+c8/eTzeU6fck3fsKJPq0o1/zf3vOesWaQYks8PGjdLcsmU2r7VsmRwQGprhGgZMUkBpZz0uvK4py0mTQqDTti3wAjNhgntI5cp5/LvIDypWzPDHvOce2WzUyNsnIiLD72Z/SCO/X1tQ0P+zd+dxNtX/A8dfZ3Zjxtj3fV+yZotkiRAiEn2jlbRJRaJIKXx9Je1FitafRAsiUSiVbWzZsi/ZlzGD2ee+f3987j3n3pkxttnwfj4e87jnnHuWzzljzPt+5v35vO0xbFefO+5wbuSnn0REZPZskRTcKRqemS1AkvCTXVZFez2aMNlNebmZ3+TkSec0VauaMZ0+Jk70fY6BgbLiL5fPpi+/zP7bt++/VCkREfnuO7F/pnIrNCVDKaVUppo40VkODIS2bfFzpRBECtXYiQX8zG0EBEBYmQI0aBJEz55ex7sHAdmKFqVpU5g713SG1quXye0NCzOvVauaahDpVHQo6B9tL5fBVC9JSnLqSHhPkZyhd981r3FxJq+kQQOfnIZAcSqlhp46wJs8TQApdqnp8SmDeeIJ875lwU03XeR1c5NmzZxld25J5cpm1Z73+tNP0xwWm+BPwbh/mcrDlOIAyckwZw60aGEGII4eTZpS5rmVeEpGVq1qahlgOtQTcKfURDv/3jZTk8qyg3mYtMcwzlKBvXxZbwIFCzp/rNi9G9q2TTVXuKfSjqc4SFISx1bt9WnL88/nwHM7aar/enKrPH+hCQ/P5nZkEg2YlVJKXZLk41GkfDLdrNSta34TL1rEM8W/4j0eZ1FIF75pMI43d3bh3DlYtw5WrDBBjy2dgNnPzwyJKFUqixq+ebNpCJiKKKnkw8kbbcA6wPwp+48/nH1ELuI63tUihg83Ob2JTpCMBWPLT/Ys0o9PTJOIYSzD+cSr5LRIOvnJV4MiRZzlX38FsFNy7BLZs2c7+1StCpiKsZ/Th4f5hD1UoBx76NMHli83mSWjRjmfR3KNyZNNSlKRItC7t7351GaTW5xcv5G9bdEi+Ia70pziT5oj+LGCpoATnJXduQSAiAiznpxsCph8+KHXwfvN7C7UqWNH1taMr3zO/++/JjsnWx0/bl7dhdA8qThFi55n/1xOA2allFIZc7l8kiBnPfUb/immxHLi8FHg58fZs/DW0Xt4kvf46fE59IwcRolKoecfLOYpk+rhVTo3y9Ss6VzHE4F4yZviBMzPMsFe3rTJ2SfdHNLUPMmawcFmujWvQXAAVtu2xHfsDkCK169hKV+ed4q+Zq97KqH6fNC4WnjnjM+ZA9HRVK4MwcTzUXwfXJu2wPr15v3wcI7XMHXPi3CS1iwDIJAUJjDUCbDdcl3APGmS6TYPCjKjM6OjiYqCUPe/p38DzSD5P/80M8D142MS/UPMsTfeCEC+BlUIDYWp9MPnM9nZs5CY6POhScR8gLB5epgrVLDLDhf4+zf88C2p+Msvqdp97pyJwL3Ex5sK3d7VGPfs8f28l550c6Q93eDun3VPXH/VVfhz04BZKaVUxu6800Rv7jmCXb8uRYBYQnhkfjcSE83gLE/va5s2F3FO7wFxixb5rmeHdAL0oIPOqKQmrMLCRAGeeAQuomy2CPaIqwceMKkg1aoRldcrSrj1VureWpjF3Eo8IfZmq1077rrbeQ6HDpke1ZYtL/qucg9PtB8UZKKwbdsoVQoasJY+fIlf7VrOP5gzZ3j8h3ZpTiHAXcxiIs8CpgO3OlsJ2LeTHTtMUNeihUlbGTDA9y8B2crzD+TQIRM5jhnDwp+EYEyU+daCqtSq5UzLnUQQR+u7R3FGRgJw77R2rFgB+aqUoF/Bb4ka6ZXytGMHU96Op0eZlQSSyJ18y751p/jmG/joIzi51T13d/nyUKUKADXPruAM4fS5YT0glOYAP3ybbP/xI/7ND5H8+U2qiDsaTkqCYTXnULBXWxZ/fpjoaBMsV6tmdovxfJ705Mi4c2s2bDBzKg8daj4vbNmC+d56cm/cPcyeyTx8Bv9eTc6X3JwbvnTQn1JK5bDERGfk0MiRkpjgkn8pIQIy169LmjFtYWGXML9qiHvw2/r1WXoL6ZoyJd0Bed5f9/JFms0rVlzgvN5z4m7ZYm/+tOhgn+2nTok0YI3E4VXpcMkS+esvZ7Vbt6u4cOpvv5mb8JRGnjdPEhNFnmdsmue8q0hjiSBKXOl8D1KwxAUyjuflD5zS0UuD28mOoZMlgiifQy40x/MliY4WGTFC5NAhe1NiosiwYV7lvb0Hk3rmWwb5ofKz9nIDVqf5d7R1zGxTOXjixPP/wAQEmJ0/+cR3ACHIK4y0V08TbhamTRN54gmf/fbU7CiH/E0FwecYLxXKJoscOyYJBNj7nKrUQH6J6Cati/4t+zH7vsCrqYs2SuvWIq5TUb4b16yRPn18N1mWyJb/W+9smDVLRExVejC3nFuhpbGVUkpdlmXLnF98VarIhtfm2Ou386NdOrlMGfN6332XcO5WrcxB+/dnWfPPK3Wxh1RfLpDVNEzz1syZFzjvV+4S0yEhPpubhW0wAaDlZ2pOi0iVKiK98SpJnZQkKSnO6urVWXTv2cHlMnXAg4PNzUydKiIiX9HbNyAuVFgK+58ynyOobp69ZZlZNkqXTvM9sWeYcH9NYpDP96dQIXPp06cz4R4mTTInLVdOZO9ekZEjZdVH6wW8ZvqYPz/dgDnFq1F5OOvTxvz5L7LUe/Hi5oCqVX3OnYi//EgHAZFwoiUZE9kmz18oUSNTzZjh/ool2H6GMRXqpLvPEYra7T5JfgGXzOQu2UJ1AZF8nJbDpW70+WCTckMdCQxMe7oFDYY7K+4PxPnymdXvvsuE700WyShg1pQMpZRS6ROBUaMQT+WEHTsoNnEoSfiTgh8LaG/nOp46ZV7vSjue6fxmzYLPPrP/ZJutSpZ00gY8vOpvp+TNR0PW0JIlPrssW5bOubwTPhcsACCpZFl70/r18OfZ2myjGtFNO9jpJy1bwsIC95DYrKUZLBYQgJ8fdDfpzZk/U0h2siwzINRdjIR1ZhBlNf7x2W1npfacSDHpMQMxBW+ssmXNQEHPBMSeUwJ+iE+O70De5lP6MmIEhBND/MmzdO1qUgRiY80/4Yzs3Al9+sDp0+m8OXeued23z0yE/eqrFHrpcUCouHsRLFpETH+TLkJYGPzvf/ahnuAqiQCa3epb1/3xxy+y1HuDBuZ1+3bnRvz9CSSF1u5/l1/d/D7+7tShraEN2JhcM70zkQfzfbCA8D0b092nGMfsdhfkNB8wgJ7MogbbqJdvN48wmeIHI7G8jrE2baRAkplP/YknnAr3xbd5/dy4p0fx5KKXL38R954bnS+Szg1f2sOslFI5Z8+oaXavlHcvn4CcCSksg72yDEDE31/k+PGcbvUl6NXrvD3M8Y2a271u3m9VYKe8+aZziriT50RAdj863mxw9wrO4G4RETl3TuzS0CX5V079c8w+NibGPaduqryLhASREyey+uazwdtvOw+ucWMRETudx9MjO7HLrz7P96NH14jExZnjz51zeqgv8JWfk3KI4rKB2vbmevXMs585cqO4/vxLZPx4kXbt7D8THDgg9l9IJk0S2bcvVfs99dHB9Ja7l1uyRNZST1x584r3xWIPnpJEr1QHATnqVyxNc70yPDL28ssZ3vNiWksMYfZ6WJhIMQ6nu2966S5J+Pn02Cd7Lafe/xkmyI90SPfc/2Nwms2niXB6xd08q3Y6Sy6EpmQopZS6oG7dzJ9/77xTZMgQiQoq4vNb8Jh/cXs5pl4LOXTIeXvYMJFVq3L6Bi5R6oIPXn/2dnkVlbiD7wVEXmS0CMin9JXp002cO76zV8pKSoq43ImfQxkns2aJ/P6783Z4eE7fcDaLi3NuvkwZERGJ9zdVSb6ni8T/e9ynfsykSU6sbHvySWeH0FBJGvK83HHjvxJJfZ+gbl+llvbyDWz0+bamG3D+8INUr+Zb3MOyRJYv97q2J0i2rPTP4fW16PY35P/+TySUs3KafE4Q2upWn8Ij1atfwvNb784DDnMHxX5+IgsXSrLln24bPIt2assF2ryY1jKbbj7bXKR/rzGBBWUfJkVmEzUlnGg5RX5xgRyglBQqZPL7X39dJC9nnPYGBYmI+KQaJSRcwb+pLKYBs1JKqYx5R78+uZiWrKeO/EhHOUIx5xdrkyYiYgr+DRsmEhubw+2/HJ5oNr0kTK9nsJxmEkGUXY0vBeTWagekc2eRd3jcOWbDBnu5CX9KRITpWAVT9K9585y+4Rxwyy3O83n7bZOfDFKSA9Krl9PD27TpeY6PixN79NlNN4mIyKlTIvc33+EzcM3764MGUy4cMIM8yntpNnfv7nXtCwTKSUGh9r8Hf5LsMXpP4/VBbMgQETG93SDy7LOX+PyGDnXONd78FSPFP+2/V++e9cpsl9b8Iq6QkPO2PRlLAkmQG8Ocip3nMB9mfqv8gCRa6fxMuL+6MVtA5Cucv9CM77fdtC1FpCEr7e3xHe6QlBQz/tUTQ+dmGjArpZTK2EcfOb8U8zk9ZAkEShn2ybDUsxtc0ui+XOrsWfMbvE46g6C8pgg4RiH5o0Ann/dXuAcEHqCkvS3x1vY+AVTqU/7yS07fcA5YuTLNs3WBgG/v7iefZHCOzz4TqV1bZOdOe1NyssiY8HHpBnRz6GSvhnL2vIFfFBFSsnCCVKokUr682VyggNd1MwiWvb9OUMBnU3igV8/6r7+KiMjzz5vVSy5R7XKJjBplgvfNm829N74pTRvCiZZy5ZxN69aJSIkSafZ7nwHyJffIi7xqnk+oSOTrv0ohjsuLj50U+fRTkwozaNB577f1zYkSEiLSkFXO/xNNWpiu48hIWUg7++eguHVEQKRrV7Nr/vyXeP/ZTANmpZS6Th0/fhFToYmI3HqrCKT5c+8CbpMlS0RurnJEXuVFKc1+mdjjD5OAey1o0UKkf38R73zUtm1FChe215OxJDFvhKQOHBrxV5o/e7tAjlAkTZwxcGBO32gO2bkzzXNLJCDN87mcWS2iT7vkcI/H05zfBXKjeyq3Fix1tlt+IrVqieD0pg7mfz6HW5b75N7TKebJk27g6Pnef85/0ry9+fNIkccft6eMS0kxE2pcdjpCdLSzPNb3w2tKUHCa68fGikjNms4GdzSdn1MS7p6FrmlT5zPIzp0iSUle10tIcI797DN7OZL6MnGi/RgllvP3YkdSL83m+vUv8/6ziQbMSil1nerZ08xwtm+f6d2aMsX9O/zUKfPba/ZskSNHTDCBGcCT/GA/+zdc/2I/iIjpjF28WGTNmlS/WK92sbHmgSxZYv7kDSKdO4usXy+uUqWc4Mjf+SDhGSj1G83TDRRW0yDN5ov60HItiopK83xOBBT1GctnB6mXY+3atIGsZck+ygq4ZGl+07V5hlD5ecZJkfh4OVW8hnh6QKP8C8ry5SIvveSc4swZMT8wng3Fi6f7fT5ISfkfQ6QgJyUkxHz2ApGHHsqsh3ceixc77fDzE1e16lK2rEiFCs5mEREZbXLuJSBA5PRpSV67QR591IyjbN/+Iq7jmfv5zBmR8ePlZLWbpDpbZPVqJ2Y/1bDteQPm+/jE53MoiDzzTFY+mCunAbNSSl0PoqNFjh41y1FRcnLyNxLg7/LuYBIQGTcmRWTAALPSv7+4/jvefvNOZsuPc5Ll9RpT5UZWXROZFxft55/Nc2jZ0qwnJ0ua6g1t2qQ7a4j31zd0T10/QqKicvTOco7Lleb5bMnf1GcCirCwK7zGvHlOToUnQASpwzpJ9g8SF8hGasoLL5jdmzVNkVhCJBk/M8jNPUuJJwf5zz/FyW/38zPpIPaHJec+uvKtfUlPj2u+fPY021knPl6kpDsVqGBBkU2b5MQJ8xkYvP6asWKF2ZCqUkh09EXOAx0fL7J1a5pjRcyPxqlTIvLjj+cNmJvecEZatjTTaefPbzbPm3elN5+1NGBWSqnrgacnLDpakgubAXotWGb/DqtaVaRyJZfssiqm+eV21v0n6uIckqJFnfFOM2bk9E1lI0+ZvYYNnW3Fivk+q6VLZW+eaukGzJ5BaKMYJf/8Y3rxIPfnbWa5VIMqtzfp4/NIy5bNhGts2+Z0sbqjs9g777EvspC20q6d2TUoSGQZNzsN2LtXREQ8E6NMmCC+BWjatbP3jcF0mT7DRPHkYYeEmJh7xgyRr7/OhHu5GLGxZhzBunU+m48fTxWwZ3n0LiL93H+RatbM53tdv77vj07+/Ll/usSMAmYtXKKUUteC48fhiCkgwPTp+J84CsAi2vEF95KXs2zfDnl3baCi7E5zeF7iOEEhjlCCPHns2hq0b59dN5AL3Hgj/Oc/8MknzrbURVUaNUIqVbFXLUCCggEIIhmA77iTYsVg+HDntNe16tV9Viu2LkutWs56jRqZcI1q1WD3bqhUya5CkueHr+2391GOzZth/nxITISp9HOO/esvAIoXN6tr1gB//21W8uaFsk4RmnDOsb7+g0ziWXCX8ChUyNRp6dUL7r47E+7lYuTJA59+mqa6TeHCzs8ukGoli0yZAqtXw/Ll0M/9XLt3Z+1aU6xk5EgYPBg2bTLP6mqlAbNSSl0LdnsFwe+8Yy8Gk8h/+IrxDCU8HFoF/mm/l0CQvXyOUJ4o+R2WZQqbpaRAnTqQP3+2tD53CAyEL7+E2rWdbZ5PDMWLm9/2oaEE1Kvle9yd3ZjO/fxLKbryPRupS758cMst5lsxfXq23UHu1LOnz6p/lUreRfGoVi0TrzV3roleAVwuO2CcRxeOHTOFJQH+j3sQT826tWsBE2sDbNyIie7AfM+9/j1sogYxEz8iLMy5ZLFimdj+q5FlQcOG5nXECJgwAb74AoCQEBg9Gl5/HUqVyuF2XiENmJVS6lpw6JCzvHOnvbibCljAE3zAqNa/8VznrQAkEMwJqygAKfjRiFWM/6MFc+ZAzZomPly8ODtvIJfq29e8Hjli9zSGtGnms4vVpg2P+H9CGQ4wh66Eh5vYwbLgySehdOnsbnQu07KlefX3N6/VqtGxI3bQ2axZ+oddlho1oE0bZ93lIrFQceZwB8nJ8NtvZnMyQSQVdncpb93qaRYAcuBfWLLErJQoAU2a2Kf7kc40vsmf3r2dS3gCbYUpNz9kiOkBv8ZowKyUUle7NWuge/d038pDvL38+N+PUWrNDwDMoTOvy7MAfMKDJFWuRfny0LkzbN4Mhw9DkSJZ3vLcr2pVp2vMFuJ15wAAIABJREFU/UAK3FLHd5+bbqJ6TT+aN7do0waqVEF5u+EG8+pJD6hWDcuCRo3MauHCmXw9rxQKAGvQU3jSJzxZSwUKQGDZEmZlzx7AxNo3sJFfzjaCs2fNe8WKsbdgA/tcUZWbEBICXbo459fv9/VBA2allLqaLVjgRB6p9ORrhrVeSbKfSb0I2bsVDhwAoHWf0nyUZxD38BVDmMijj2Zbi68ulgVt25pld8DsX9Lrb/BBQVCzJqVLw4oV8Ouv2qOcRsGC0Lw5JCVBRISdyDp9Ojz8sHkr06/nJfCBPnasLmJeb7kFLE/S8sGDIELt2jCPzpTkiH1sfHA+Bg93UpfKNjFBtnebq1bN5ParXEkDZqWUupqtW5fu5hMUYimtaftgGQLGjgbACg62/yxeuEE5+j3ixwzuIYYIBgzIthZffVIFzD5/bk5JAX9/7rkHbrsNJk6EDz7I/ibmesuXQ1QU7N9v5xiXLQtTp5o810zlPbKsSRMoU4a8eX136dMH55PN6dNQuzb1o5dSDvOBUoBk/Hhs05N8+61zXPdHCtuX8AxcLFkyk9uvciUNmJVS6mqWnOy7HhbGyW+XUZwjnKAIt96KGXTl7w/x8SbAA6hThzffNDmdy5bhM4hJpXLrrSadwLvrOCLCvM6aBZhU5/nz4dlnNYA6r/z5IV++rL+Od47HnXcCvlkawcFw111A+fLOxs2bCb67q72aSCA12Mb0dXV9Tl38BufcLVqY16t55gd18TRgVkqpq1lSku9627acuuEWUgigTBl38Faxopl2LjHRzPE0aJAJAjG/9G+5JfubfVUpUcJMPfbII862Q4fMB5Bu3XKuXSp93sn3dUy+eYUKZrVwYa84uWNH3+NiYuzFYfyXnTjJyS9Y48yC17Qx3bpBeLhv3K2uXRowK6XU1SwpyZlGC2DoUKKjzaJPZ16BAmbatNGj4c03s7WJ14TGjU105BEaaroqVe7jyU0Ge57i224zqzExXtNC16uHz4TQbrusSnwc/qy9XqcOjJNhzJ0jPvMat28P0dFpUqbVNUoDZqWUupp59zC/8ALcdBOnTpnVTM8NVepq4D0xsjt47tcP7rvP/LjUrOm1r2eeba8Pnes6vsCZM84uiYnmGO+ZMTy8P6uqa5sGzEopdTWLizND/wsXhjFjADh50rx1DU6FqtSFef9pxR3RegrjHTxospJsnoRzz/QZQOvxHXx+drZtg06dsrC96qoQkNMNUEopdQUOHzavXlPLRUWZ19DQHGiPUjkt9ZQYXkqUSLWhXDnf9QcfpNANJXnrLSdlPS5Os2+UBsxKKXV127XLvHr9vVgDZnVd80S3999/4X179IBVq8zI1/h4MzE00L+/qY4dH6+pTcrQgFkppa5mnoTlrs6UWJ5BfzpVnLouWZap1Hcxka6n5OBff8GkST5/qXnrrSxso7rqaMCslFJXM8+gP6+ptDwBs/ekDkpdVzJIy0hXvXomyVmp89BBf0opdTXzFCIJDLQ3nT1rXrOjRoRSSl0PNGBWSqmrmSdg9uIJmD3F6JRSSl0ZDZiVUupqlk7AfO6ceS1QIJvbopRS1ygNmJVS6mrmcqXZFBtrXr2q+CqllLoCGjArpdTVTANmpZTKchowK6XUBXgVAct9XK409Xnj4syrpmQopVTm0IBZKXVVio/P+kA2KQkaNjQTUNx5Jwwc6JSdzjXS6WGOjzevhQtnc1uUUuoapQGzUuqqM3OmSTeYNi2TTywC77wDrVpBZCSzZkFkpJni+Pvv4d13Yf78TL7mlRJJ08OckGBetYdZKaUyhwbMSqmrgwgkJzN5MvTubYLCZ5/NxB7f2Fi4/XZ46ilYtgzp2pU33jDFwo4cgcaNzW579mTS9TKLCPj5/leemGhetTS2UkplDg2YlVK5W6dOUKYM1K4N/fszfDjUr296faOjTYfwpVi7Fr7+Op03hg2Dn36yg0/r4EHi12y00xtWrTKvo0ZlQc92BtauhXbtnIF8aaTTw5ycbF6Dg7O2bUopdb2wJBePZmnYsKGsWbMmp5uhlMopR45AiRL2quTLR0DMKfwC/ElONrnFoaHwxhvw0EMXd8ru3eG772DpUmjZ0r3x4EEoXTrNvq/yIlEU4PQt3fi/VZXs4BlMWkjPnmbZ5YIlS+DmmzM/SK1SBXbuhOXLoXnzdHbw8zMPwpOHgXkmcXG5fLCiUkrlMpZlRYpIw/Te0x5mpVTudfCgs9y4MVZMDP2ZYlewS0oy5Z8fftjkGqerUyd44gl7dfNm8/rQQ06BDzp2dPYfONBeHMoE3mAIUzc0YspkE31auLi12CbmD/iBpDt6QEoKb78Nbdua9I1//rmC+001gC8lxQTLALt3Z3CcV0qGiHkuqTqdlVJKXQENmJVSudfx486yOyfiET4iJsbZfPiwefUElj4SEmDRInuknicAtSwTgM6a5b7G33+b/S0LmfQmE/yeByAYkwzsFx1Fu8omYn2DZ1l8tDbToroROPdbfnlnM4MHO5dctOgy73X7dggIMF3Ky5cDMGGC8/aff57nuFQ5zN9+a1Iy/P0vsx1KKaXS0IBZKZV7HTuWZlMFduPvDwsWQKFCTr7u2rXpHL9xo+lu3bsXjh1jzx7TiStiYtP584GhQ539W7Zk734/1rrqAJBEoP1WochFBJDEo3wAgAvThbt8yPcUKeKcYvRoePVViIq6iPuLjYWPP4azZ010LGIi+hYtGFr1e4YPd3bdtu08x4MdMCckwJAhZjUk5CKur5RS6qJowKyUyr288xBGjQIgjLM0aAAdmp9h/z6hdm3z9v/+B/PmpTp+5UpneelSn9U8yTFYS5fA9OkApAQGMzxkEnPnwl7KAxBIkr1/4Ndf8BMdCCERFxbHKArASymjuOnod/Z+CQmmqU8/fRH3164d9OsH4eEwdaq9WYCbdky310tzgPqrJvsceuoUbHnWfYy7O/nPP81nA5cLCha8iOsrpZS6KBowK6VyL0+qxMCB0KcPAIGk0CrkLyhenNBvPmXiRGf33l3Owm23wTffmN7a75xA9vQHX9mzY3RkPjFEMONYGwDEP4BuZdby35/qMWgQHKV42rb88QeNMGkhx0vX5yE+BsACvuEufrxlPMGBLmJiTO/1l186+dLp+vBDJ88iPNznLcGiGX8BEByQwnaq8kbso6z+4RAJCeZxVKkCKyevMweULAnAhg1mNSQEKlXK4NpKKaUuiQbMSqnca+NG89qgAcllK5KM6Ul9fmV3k46wYgV16ji7j+RVk0R8992mzN2vv9rvhSz9iZ/nxjOU8cynk89l5rd5nXm7axIUZILdE3iVyPNMv2FZhGJSICJGDOS3wHb2LgG4uP23YWy4azQNGpgskJQUeO2189zXq6/CY48565Ur24uHKIGFUIxjDOQtFt07nTyY6Tle67mBgQNNAZWAALgJd8Bdtixg0lTAVPorVuy8T1UppdQl0oBZKZX7fPEFNGjgjORr2pSH+vlx1J0Gke/cEbN98mSK5DkLQDW28SwTcVnu/9ZOnSI5T5h9yhASiCcP4xnmc6kTFKLzoqcAU/AjORnOEI4EuPOXBw6EWrXAsrCwSCKQkLu60KVHEHfwA5H+jZGgINOGjbOIjDTTv/mTxMqZe+1c5thY0+m94r9L4aWXnAZYFkmHnMGNsx77FStfPgBe5hVu/n2s/V6rwOV89JFZjjkWRxV2mJWHHwZgnbvDedw4mOybwaGUUuoKaMCslMpdjhyBBx6ATZvsTT/uqMrnn8NuvyppdverX5eB1rv0YDaBpOAnztRsAXFniSXj0W+/dHqD0qWdOdiqVYMOHSysokWgfHmoWxfmzIGhQ/HvdReB77wBhQoxbRoM/f0OasSsxBrrDmrdAf68+2aSTBC7XRWIatqB33+MIW9eMxgvZrgTAPP667iSXbSpcoChjOfekFkMfK86jBwJQEGisPbts3fvFTuNZvxBIU4w1hqBP0ISAdCzJ/HxcOKE2W/IEDPdnlJKqUwiIrn268YbbxSl1HXmgw9ETGesCEhSlzulXDmR0FCRAbzvvBcQYF7DwuxtLq/jBGRt+C1Slj1yjMISR7DPe8/W/1XasNh7k/znP17teOklkalTL67Nhw87J5k1y+c6AjKlwhjJl0+kXIkEiSdQBCSha08Rl0vefNPZdcgQ9/lOnhQpU8bnHC7LkhQsiSZc/qKJfb+bqCErV4qULWt2zZs3k78fSil1nQDWyHliUu1hVkrlLu45iClSBGbO5PPOX7Nvn0lp+Cr0ERIeeszMbFHF3dt89iwud5UOC5jMAPtUHc98zX7Ks77TCEIwlfASSleE+HiK9mrNr9xq79uokakYaHvlFTvV4YKKF3cG7t11V5qqIQ33zCQmBpoc/o5gkhDg9/6fkZxiMWaM2T0oyCvnuWBBM+90zZpmvVAhrOLFsRDycYamrITwcCxgMW1p0gQOHTK7Nm58cU1WSil18TRgVkrlHtu2meklwNSt7tmT96YE2kU4xo73J/jj9+H++6FDB/swvy5d7OVFN73ESEazMLAzQWXMbBflXh9ITANTBzvoyQEQHEyrVmb/xx6DPXtMfHpFA+Xq13eWRXBVqMT9AeZe6rOB//kP46vA+3HhRzT5+HJ2CIsWmbopIvDUU6nKahcvbtJSfv8dVq+GVauwypd33j9zhkT/EN5mEGAGGfr5mfGESimlMpdleqBzp4YNG8qaNWtyuhlKqezywAPw6admeexYdvcabk+PVr487NhhZocAzITH0dHQpo3P/G1x51xUrGRxxD0u0LLMrBX+USfgr7/g9tvteYvj4zOxwMcXX0Dfvs76e+/x3K5HGf9GAH64/58tVw7Zt48t1OQGnDYHB8Pp0xfRlvffd8p8T53K7tYP+0wf9+ST8M47mXM7Sil1vbEsK1JEGqb3nvYwK6Vyh8OH4auvnPXbbrPnFQb46COvYBlMlFm0KIwf73OaPKEWkZFmOmaAqlXd8XHhwtCli0/N6EythnfXXSbgr18fWrWCRx5h9Gt+xPjld/bZtw8LmEsXQkKcHuXevS+yLZ4qLQBNm1KxopMJEhwMI0Zkzq0opZTypQGzUur8PEPO3A4f9pm8InN98IHpCgYzpVz9+vzyi1lt1Ajatj3PcZ06wYoVPptKloSffjKdvu++m0XtTS0kBKZNMzW6lyyBgADy5IGIxtV8dnM9/gQflRtDfLzpJIcM5mtOrW5dZ9mdnuH5YDBypM69rJRSWUUDZqVU+kSgRQto3drk5EafoVYt08n5009ZcL35800Srr8/LFwIfn52IbwePS5wbJMmMHo0fPutvcmy4N57Mwi0s4n1yitmrjqAt9/G77132b7Ln969Tb2R4cOhdOmLPJn3XHF58wImQ6NLF3jmmcxtt1JKKYfmMCul0rdpk5MCULs2snMnZeK2cxAT3bVoATNm2FWZr0xKiskpSEkxhULefhswWRQnT5oBeY0aZcJ1clJmJUwvXgz//mvSP5RSSmUazWFWSl26bduc5b//xoqLYwCmfFyNGmbyhnHjMula27ebYDkgwGdut5gY83rDDZl0nZyUWQnTbdtqsKyUUtlMA2alVPr++CPNphG8RjsWsnWrWX/3XXjwQTh48Aquk5wM3bqZ5WLFICCAqCjYutU9u4U/5MlzBedXSimlrpAGzEqp9K1enWaTBbzLk/Z6YKCZ2KJNG0hMvMzrzJplepgBypdHBGrVcmp2FCx4medVSimlMokGzEqptFJS4O+/zfLEibBlC3/d0B+Ayuzk58qPsYxb6Jv0Md8mdqLt9vf44w+YNy/dODstETNh8NSp8L//OZsrVWLrVjMbh0eFCpl4X0oppdRluOJBf5Zl+QNrgIMi0tmyrArADKAQEAn0FZFEy7KCgc+AG4GTQC8R2ZvRuXXQn1I5ZN48M/UCwM6dUKkS3W+PZ9qCYkQQk2b3OEIozHGCSGIDddkSWI9NY+cweLBJqzh92kyZbJs7F+64I815WhTbzvKjVXy2vf++qcanlFJKZaWsHvQ3CNjqtT4emCQilYEo4GH39oeBKPf2Se79lFI5bft26N8fDh0ySclLlsBzz5l8C7C7eLftDWEE6dddzkM8Q5hA69BVlOUAHZLmMuq5swwbBs2amenTfHqef/89zTmG5XmTAyFOsFyjBhw5osGyUkqpnHdFPcyWZZUGPgXGAM8CXYDjQHERSbYs6ybgZRFpb1nWQvfyX5ZlBQBHgCKSQQO0h1mpLBYXZ+YIPnAAihfHricNxFp5CSGe7VuS+eUXU3Y5gCRiC5Uh8ORRs1OTJrByJQDJ+LGJWtTDpHL04yM+pp99vlKlIPKPeIpNedWUv041UrAUBziEMyHxSy/BK69k1Y0rpZRSvrKyh/lNYCjgcq8XAk6LSLJ7/V+glHu5FHAAwP1+tHv/1I19xLKsNZZlrTl+/PgVNk8plaFXXjHBMvgEywChco7fpTk1aphgGSCZQBImfWBWhgyBFSvYNmM9AP64qOsOlgEas9LnfIkHj/Hna7/C2LFw9Kjve4F5OURpgoKcbZ06ZcL9KaWUUpngsgNmy7I6A8dEJDIT24OITBGRhiLSsEiRIpl5aqWUt7VrYcIEZ37g0FAIC7PfPkco7fgZsIvKERgIYX3vhKVLYcwYAKrdXZe1YS04l7coltfpG+XbjoWLgbzNLHpwjGLcOdUdBbtc9n5SvjwdS20EoJ/TIU2DBpl6t0oppdRlu5Ie5ubAHZZl7cUM8msDvAXkd6dcAJQGPH93PQiUAXC/H4EZ/KeUym67dpm8ZRFTgQ4gNhY5dw6AExSkZ8O9JBEMgHuzPdUbLVvi6Q62LGjw+r2EnTvmc4laeXbzGO/zNoPowbecxGt+OJcLF3682i2S7Z+v4te9FQFTArtCBTPeMCAApZRSKlfIlNLYlmW1Aoa4Z8n4BpgtIjMsy/oQ2Cgi71uW9QRQW0QetSyrN9BdRO7O6Lyaw6xUJuvQwczZtnGjvSmh9/18srsVpSsG0WXGvQD0YBbf0sPeJ08eeOIJePRRqFTpPOeOjjZFSGJioGJF8PfnUIFalDyxMd3dj1GEYhyjQQPT2Q1w6hTkz2/ieD+d9FIppVQ2yiiHOSv6cJ4HZliW9RqwDvjYvf1j4HPLsnYCp4DeWXBtpdT5HD0KCxf6RqIFCvBZm+k8PgNYBbcTwUuMZm3RjrzQz0yRnJwMzzxjZ2CcX0SEfU4CAyEp6bzBchzBHKE44ATL+fKZQ8H0WiullFK5RaYEzCKyFFjqXt4NNE5nn3igZ2ZcT6lM9e+/sGaNU575WhIXZ+Z1e/BBO4XCO3+Y9u2J9BqFMJ9OzKcTPVuaAHnkSJg8GQYMuIRr+vmZUn3795suY4BCheCkk4GVQAjHMBMzW5bpUa5W7TLvUSmllMpi+kdPpfr2hTvvhEWL7E0bNkD37k5671Vr505Yvx4GDXImNA4KwhWSh7jvfuLg2E/5+mtn9/BwM+7vtdfMekiIOdQzLvCi9evnBMtly8LBg4ws+bH9dn6iOVejkc8h9epd4jWUUkqpbKIBs7q+iJg5gGNiTKGOBg3MjA8A//kPJCQApgjdd9/BqlVpT3HuHCxenH1NviL795tXrzJ7KWP/S/74I4Te2Z4mLYI4c8ZsL1LEPJaYGKha9Qqv+/jj5nkCfP45BAezu9VD9ttSqRJd145i5kzzLQGvAYVKKaVULqMBs7q+rF4NDzxg8m1LlYJ165z3TpxgYVh3OreNZ/9+yEMsO2ekHXT6+OPQrp2Jt3Ot+HiTevHoo2b9mDODxeFGd3CGfAAcP+5kaPxsZpDLnPxhyzKB8vbtcMstgJkBw377xx8hJMRnW+fOmXBdpZRSKgtowKyuL//8c963XED75Pm8+stNBJDEDqrw0AeNkHOxxMWZfX75BT77zCwfPpz1zb0szz5rprWoUMHkZ3tJCgln7AxnmovERAgONjNfZHpKhJ8fVHFKXXfpAnO7T+Ps86/aCct+fqYX//33oXLlTL6+UkoplUk0YFbXF+8eZS/jGWr/MNRnPa8zmFKYLuRHOh+iShUzycSjjzpj5zwF8nKVuDgTfYaEmO5jt6NdHmZI3g+4Mf53PvjA95Dy5eHjj8lygYHQZfYDhP13hM/2Ro2c9GqllFIqN9KAWV1fFi50lps25XSpWvxEe0YFjCUBpy7zIN6xl/9eepyDB00P7M6dZpo1gC1bsqvR6RAxcymnnkd95UqThx0fj90tDpSa+wETzz3K39T12f2zz2DTJlOHRCmllFLp04BZXT8iI32j3N9/p0LMBjryExPf9Odo3orpHlYQM9vDkSOml9RTJtozni7buFxmYuSZM+HFF6FuXShRwlTscycin9p+AgDx84MOHUguUoK8nKFQ0UD7NEFB5mvbNjNBiL9/Nt+HUkopdZXR4rPq+jFpkqm3nJwM1auTYgVw2j1DxJNPQhy3M4RtZkNQkEnwBQp5VXAPDoYZM6BTJzh4MPUFstiKFfD8877bjh6FqVM53rUfOws1Ye4zBxkLxEaUJO+CBfz6M8S2h1j3mL/QUNi928yIoZX0lFJKqYujvzLV9WHtWvjqK9NFDLBhQ+rxcCSOHIPr1ddg3Dg7WAYoQJS9fO6cU9DOqw5H9li+3LwGOakjkj8/ANO7zKJZM2gWa6a6+DL4AQBOmA5nGjc2PcknTkCxYhosK6WUUpdCf22qa58I3H23eY2LMwPigoLYsMHZpXdvGP5KCH4jXjR1oAOdFIYHu0b5nKprV7McE5PJ7YyNNRXyZs5M/x4++cQsewXz1unTADTnD/z9oTAmQl5wqingBMzbt0OdOmbyDKWUUkpdGg2Y1bVv3z7Ytcssv/girF/P3r3w8svOLkOHes0/HBxsZpgoUQKA+hWi7f02bDBpDWB6mzPVokUmx7pXL3j3XXvzuXNwLnKrMyVenToAnB0wmGNBpQBoyGr8UxIoFmzaui2xAt995wTM8fEwZUomt1cppZS6TmjArK59f/5pXi3LBMzVqtG3r+/4vwoVUh0TEQHudAfOnGH2bJg40cSq48ebzbGxmdS+xESYPx+mTnW2vfQSrFvH99+bUtUvN/rR7Eogv//3D5g8mQ5rXuPtpMcBCCKZL274L+UKnQXgX0rTvTtMm2ZO16cPNGyYSe1VSimlrjM66E9dW/btgy+/hLNn4cEHTbfqnDnmvQYNIE8e4uNNDO1ymTGA7do5sbGPsDDzGhND9+7O5lKmU9dTRfvKdeqUttZ2VBQ0aEC5Eh0JD5jNf1wzwAXf0p17bg9j06ZH+OsxiJfbeI0XAeh5eirExyHAWcIJDHTqlpQvn0ltVUoppa5DGjCra8u4cTB5srMM9rxprju6khAH3bqZYLlwYZOy8PTT5zlXeLh5PXvWZ3Px4ubVK5X40iQlmUjdkwMSGZlmF5d/AMkpFvUPL2AzVSiJKSs4jQcAM4jP5YKN1CHWLy+hrnM+Vf1CQixSUpzzeYJ8pZRSSl06TclQ15aNGyFfPt9tKSkI8G50X5Ytg5/NRBKcOAGDB5se5nR5znPsmM9mT8DsHZBetCVLTHf2hx+adZfL6ap2DzSUH+fza75uBJGEAGU4SLyVBwGW0Qp/fycdJH+RIFJOxZh865AQ90Us4uNNXO5RuvRltFUppZRSgAbM6loiYqaPSzV9xdJbXqIVSxn9aXnmzXO2P/ccvP6612C/1EJDzZtbtvhU1PN0PF9ywLxmDXTsaKLdCRPMIL7vv3ei36QkqFSJOYkdaBf1DUOYgKdpeeUcxylMAiF06uScsnlzCI/wg7Zt7cYlFypK27bQr5+zX7Fil9hWpZRSStk0YFbXjgMH0iYWWxZD417hN1py8iR8+63Z3KYNjB17gfPlyWPmPI6Lg3XrvE/JSOtVjqYUgj17fA5xz/KWvtGjnW7fPXugenWk7314QvGUJjex4/vNvPCiCZMnMpiv6UmSO3NqRXh7wBT787jlFvfC+PGmvvWZMwRWr8yiRXD//c5+BQpc4F6VUkopdV4aMKur25Ilpvrd9987QW14uJ20KyEhbNvm7H7YpAIzbpxJI85QnjzOTqNGOZVKRHhBXqMQp0zEGhcHwOefm8DUe/YNW1ycmQnDXcLaw4o9hwX8TFsCVv5J1drBbNniuaxFb2by4RtxMHs2LddMZNcuqFYNPvvMHN+tm/tEJUrAggUwaxb8978A1KzpXCd1lopSSimlLp4O+lNXtx49zIwS3sqVY01wcxoenMzykLacSfW2ZUHt2hdx7pAQJ+9i3jwzSnDxYihYkBDcI/7+/RdX8RIkHoli3DjTM1yrluno9irIZ8pae84VEmImRvYyged81pOTYdgwqFsXevYMAP/uRADuIoP07WtqsQQHp7qxHj3s1YIFnbc8E34opZRS6tJpD7O6evzzjynocdNN0KwZLFiAeIJlr0Tk2FYd+TiyLgBLo+r6nMLPD2688SIr3uXJYwLbGjWcbW3bmunpvM8ZE02bfGvYutXZtnIlprxecrLZ8Msv5rVYMWKnf+1z/HKas5jbfLZVqmR6wXv3tif5SMMnWD6Pjh3dbdSfdKWUUuqy6a9RlfudOAEPPQTVq8PAgaa3dvVquP12e1AcIsQRQiKB3L1xJNN5gFcZwQSeIyTELtpHQAB89NFFXtdT0s97EmbP5YBknEi2b/LHPu8vmHLA5E4MG2Y2LFxoXps0ocJTd1CPdTx14x+UYy8t+J0RI8wU0nv2mGD5gw8uso0X8MMPF8irVkoppdQFaUqGyv3uv9/k/3rz9NwCuynPOF7gbZ7ibZ7ix9/MNBYv8SoAFUvCY4/BzJkwYADUq3eR123VyrymE2FvD61Hq9j5HKYkAA8wnSMBZfmTZixObkXM3GVmx4kTcW3fgd/69QB8uaoKx47BMeqxwT39cqlSZsYOT57xzp0X2b6LEBhoihYqpZRS6vJpwJyOEydMsbjlrIZzAAAUzElEQVTevXU6rlzh99+d5dKlfQp0AGykLlPpz9f0IpZQe/srr5hA1JN+MWTIJV63aVP49FN4/32TBPzII6aISY8evDW5Lme/sGgd/ztLkluQhwReSX4R8fenOpu5PfpL+zR+c+fYywuPpE2eHjdOB+UppZRSuZmmZKRjzBhT/a1CBTM5Qo5yueDo0Us6pFUrePvtrGlOtjt2DM6cMcsTJ0LDhgCMZLS9y2y6U6QI/GdAPlLcnwHDwsz4t4vKVc7IffeZFJBdu8xsHK++CvXqEZHfIjYW1iXf4OwbEoKVksIcutKehemebimtfNYjIuCuu66wjUoppZTKUhowpyIC06aZ5eRkM3XukiU52KCXXjKl5TwTCGPauHIlbNhAmmnKYmJg2TIYNCib25lVFi0yr8HByNPP8MsdbzEw5CNeYwR/hzYhuWBREnv2Zf16eOIJ57C//zazVWSVMmXMo2/aPj+uMHclE/fMF9X4B3/37MqbqcHrDAbgECU4QDmf8zz0UCYE9UoppZTKUhowpyM83EzJ1aSJWZ86NQcaceiQGdjmnlOXnj1x/bSQyZOhcmWTLfBmvWlmCoUTJ+zDcjS4zyzx8aZHF+xPL64KFenazaLtQ2V5N74fYHF05jIC9u/m65kWJUtC2bLmkNBQKF8+a5vYrx/s3g0//QR+lSuleV+ARALpUGozJ5+fwK0s5hZ+Izzc99/Tk09mbTuVUkopdeU0YE5FBOrXh1OnYP9+s231at999uyBN96w61VkvlGjzEiwxo2duXtdLvw6dqDyo7faxeUewASTu+8YZFdu/vBD83recs+53aFDpmu4cmWoWtWeju2rbfWZO9d31/pNgyFvXns9IsJUwVuzJuubGRRkUnYAM6IwLMxEz2XKAGAB/0dvGjayGPWyxa/cyi4qU7o0PPywyfL44QeoWDHr26qUUkqpK6MBcyqe+WpLljR/cu/AfO7Z/ZrPPvffD4MHw1tvZVEjvvgiTcQbF1YYgFv5lVKhZu7hQpie5VJ/fUOvXqaK3c8/m/0vZo7eXEMEhg+Hp54y3fq7d5vtO3aYt4H3eRwwNT/A1BApVCjtqZ57znfa5GzxyCMmz7p9e7j3Xnvz2wzC5TJt9nw/PNX3mjSBO+7I5nYqpZRS6rJowJyO1avh1jbC+vWwgE68kjKS+HjTcxkZ6Uza4AlOM9U775iA0d1lLJbFd4F3k+esk3bxY0h33qo9lVqYShlBJLHwm9M89piT0uzpmM7QiRNmhODmzZl8E5do9GiTevLOO2lmwACYQW/+ojlgepDh/MU8clzXrgBIpUq0H9aASZPMZk+KSNOmOdMspZRSSl0+DZhTS0xkf/46vB/Vm0JjB9ubBw2CRo3sSRoAM8Auky7Jpk3AgQOml9Vj8GDmv7aO7kkzGMML9uY6J5fy1N/97aIdFtCPqfzwgwkkixb1mab4/KZMMSMEe/Xy2Tx9upkpJFscOgQvv+y7LTSUk1YhkgjgH6ryEJ8AZoDcgw+aXS55irjs0rgx1K6N9dhjjB1n2SkXnukJs733WymllFJXTOdhTs2yCNyzncBtf8OPzuaPpqQAvt2al5zDfPYsLF8OzZubkYWYjuROnWDxYjjU8jlKBAZCUpLZ/6GHGPeI+Rv+CMawmkZ8R3cs9wwMHgLcxWzeYAgNGjg51yIXyGVesMC8bt0Kp0+TlDc/gwY5VeYeeMCkUmep9Kp0dO1KiVlf2Y8BoFs3+NhdTC8pKRf3MPv5wcaNaTZ7nqP7266UUkqpq4j2MKcWEJBuFYmCnLKXS5UyAdslB8wjRkDHjqYLuE0bTm89TLt2sHix8CEDKLHsa86IKbyRNG8hP+6pycqV5tDbb4f+87rh+npmmvnSJCiYumwAxGeAYmys7+U3bzZVnu12e0YPuly81ehzXn7ZtyTz559f4v1djr1702zaGtbQDpafeca8Pvyw835AwNU3qPG99+D11+Hmm3O6JUoppZS6ZCKSa79uvPFGyRFTpoiASJs25hWkZ42/PYtSsqSIZYmUKHH+U/z2m8iXEw7KspoDpF2hSJk0MUVcERH2+QRkZrEnBERuZZEIiMu9fT+lpX49l72rZYnExaW6wN9/i8yZI1KhgkihQiIgo/NNkN+WpkhYmMgoXpLdD442+yQny+7dIhGclid4R74btESGDHZJMv52W7ZSTcBc85lnzOYOHbLwGR87JtK3r319V0SEDOv7r0z16y/BxAmIlC5tdt2+XcTlysK2KKWUUuq6B6yR88SkOR4UZ/SVYwGzyyVy8qSIiOwr3kgEpEvQT3YAmzeveY2IcO+/f79I06YivXqJ3HabRBetJD2ZIZHUFwFJIFDGMNwOis8SKi6QwxSV3nwpk+kvLvd7dzND8nDOvlZoqEj16hm0ddAgkYAAJxD/6ivpXmurT2AuIL9YbeQlXrbXZ9AzzT5rqSc1w/ZJ27ZmU4ECIrNnm+X9+zPx+e7fL1K2rM+1l3Z5PXVzpEmTTLymUkoppVQGMgqYNSUjPZZlKpcAIUVM0mmZgEMcOGAG6HkmlUhIwFSiK1vWTKz79dfw88/kO7aLmfSmDiaXNYgkXmAcAhyjKA/zMRZQnGP8H/fyCB9hYUo8/5y/F3GYtIy6dU1aRb16GbT1jjt8R/h9/z0PJ32IACn44akD2EZ+ZTjjiMXMy9aLb+xD4jBzntVnPbNTurF4sdkeFWXKSwN88smlP8bzeu89Z5Jrt6kBA9Lsls6EGUoppZRS2U4D5gvIXzYCgCd7HqV0aQgMdCrKJSdJ2hkewB6Sl0iQz3YL+JiHmc/t6e7/HBNo3drZvn27GXj3yisZNLBFC9/1mTO5fftbWIA/Ls7hFPY4FlKOUOLTnOIszki06nHrKJIvIc0+V1pB8ORJU8pbBGdePo/27TkZH+ZzrQcfhM8+u7JrKqWUUkplBg2YLyCoaH4AahQ8Zm+zLDMZQgvXMvjzT7PRzw/uuYdN/5tvT/cWijMqcOcN3RjCBEbwGj0f8h1UaAFR5GcvFe1y3GCmmps2zRS8O6/AQLjttjSbBXi3wkTCOWdvKxu/Pd1T5PP3HR145Fw4b/A0Xbo4hfSWLXMm1QDTqb1kSQbzPS9fDjfcAMePA3DjjaanvHhxOLV+r7NfWBh8/jmHDpnV4GBo2dL0aLdpk8F9K6WUUkplEw2YL6SwqbDHqVM+m/394WH5CEJN+oTrnv/w55Nf8cS8jrzMKPYEVYUzZzh79BwJ706hyJKZTGQIpcv48c47znlcTz4F9eszglcB6NkTWreGf/65hLLJ330H69aZ2T3cpQp3UIWBe55hQMh0ltLS2Tc4GM6dg6VLWXnfu6ws04PgFCdgloIFsVKSeIa3iJz7L+eceJtOneDbb83yvfeagLZPn/O0qX9/k7vy1lvELY/kxL6zABw7BmGx5sOH5M3LwmYvU7hGETZuNB9Eype/+mbAUEoppdQ17nzJzbnhK8cG/XmbONGMQMuXz8ye4Z6uITRU5DT5zEA+f3/pXnm9z6wWffumPdXevV6zXbzxhsjIkfZ7HTqYY69oNohFi0Ty5BEBOfrKB7Jjh0ilSiLgkqWjl4l07iwyYYLvMadOiRw8KLJ6tUjt2mbGjVatREA2BtQ3t06UPMp7AiKBgSLffOMMzPPzc9q8b5/IDz+I/D7zkLj8/HxG8H1DDwGRcKJFQJLwl+XL04w7lC5druD+lVJKKaUuExkM+tPCJRfS0t07GxMDjzxiupYffJC50pkIYkjGn8VjIvl2WF3y5IEKFWDbNqcUsrdy5bxWPBMMu82ebXpfr6h3tW1bU7t79myKPv8wRQNNJ+/27Ra1at0CfrekPaZAAfNVsiQMH266jpcuBaB28jrWroXgBs2oyVZ68zUhSfE07Wkmh76Hr4hx5SMysjMNG5o5niMjhdG8z832cEOjDb/Qpw8Eb/wHNkIM4XaZa29PP30F96+UUkoplQUsE1DnTg0bNpQ1a9bkbCP27jVRsLeaNWHLFgBe40VG8lqaw9atu8DsFrlVUhIcPGjyQURMjsg33/jsUoKDHKEEnklW5t84ktvXjCYiKI4FSW1oxop0T33wrkGUWjcPdu3id5rTLni5mWnEy7lzdpaLUkoppVS2sSwrUkQapvee5jBfSPnyMGcOjBnjbNu1CwF2UJlROFNYPPkkfPghzJ9/lQbLYAYRli8PlSub9VTBMkBrlnBXvV32erPIt2l7qzAh6SmfYLkvn/JYnun2eqlZb8Euc9z3dCMhwfSob9liXkuU0GBZKaWUUrmPpmRcjC5doHFjePFFs56QgCswhCeS3uO+B/xZtcoEfS+8YIK+a8JNN8GOHSaSdf8VIiUgGP/kBG5mOR3z/YxgZviIIJq7fn2MR5gKQGxQBL0SPyehXReeGgOnGj9DQaJ8Tv9pQH+KFjRlxmvUgHvu8Z1OWimllFIqt9Ae5otVrBgEuedVrl4d/+1b2VbmNlJSoHZtyJ/fTJl2zfDkWHtSdt58E1d4BAI8zodU+O0ze/o8C3iUyfahZ6d9Q3CPLkybBg0awKGi9d07miMOlG/ByeQIjh83M28AfPGFqfuilFJKKZXbaMB8KSZNMukK8+dD+fJUrGgG6339tQmar6np0OrVM/nb+/bB6NHw2GOkjPmvHSTbHx7SUfS2+syaZXqP/f3hhhfuMG/06wcxMawePR+A++4z4wzhGnt2SimllLqm6KC/K/D00/DBBzBwIAwYAFWq5HSLstihQ6bMoadaiWXRWP7i4RorGLDVa3oLl8s3Ak5Jga++MmW8IyJwucw80zVqZG/zlVJKKaXOJ6NBfxowX4G4OPNVsGBOtyQbRUaamtXly0PXriSUqoi/n7D5hS9ZtSWM/o8FQOfOOd1KpZRSSqlLogGzUkoppZRSGdBp5ZRSSimllLpMGjArpZRSSimVAQ2YlVJKKaWUyoAGzEoppZRSSmVAA2allFJKKaUyoAGzUkoppZRSGdCAWSmllFJKqQxowKyUUkoppVQGNGBWSimllFIqAxowK6WUUkoplQENmJVSSimllMqABsxKKaWUUkplQANmpZRSSimlMqABs1JKKaWUUhnQgFkppZRSSqkMaMCslFJKKaVUBjRgVkoppZRSKgMaMCullFJKKZUBS0Ryug3nZVnWcWBfNlyqMHAiG65zPdFnmvn0mWY+faaZT59p5tNnmvn0mWa+a+GZlhORIum9kasD5uxiWdYaEWmY0+24lugzzXz6TDOfPtPMp8808+kzzXz6TDPftf5MNSVDKaWUUkqpDGjArJRSSimlVAY0YDam5HQDrkH6TDOfPtPMp8808+kzzXz6TDOfPtPMd00/U81hVkoppZRSKgPaw6yUUkoppVQGrsqA2bKsMpZlLbEsa4tlWZstyxrk3l7QsqxFlmXtcL8WcG+vblnWX5ZlJViWNeRC5znPNTtYlvWPZVk7Lcsa5rX9Sfc2sSyrcFbed1bJZc/zd8uy1ru/DlmW9f3/t3c/IVaVcRjHvw+mFKmk/TFxAhOKiCgTMiGREDKzMBctDCKpRQRBREQYQdTCRS7CZQtbCP1dVCS2KLOENpmZWoaROgyYWUOUmIvoj78W5715us6cRufce94z83zgcM+ce+Z973n83fu+99x7xl4eey/VmOuFkj6XtD+180JFn+tSu4ckrStt3yDpqKRTvTzmXsslU0kzSnW6T9LPkjb1+vh7oa5MS+1NkbRX0raKPl2nfcjUdTpyppKGJH2dMvmiok+P+/3Jsx3jfkS0bgHmAovS+gzgO+B6YCOwPm1fD7yY1q8AbgE2AE/9Xzsj9DcFOAIsAKYB+zv7ATcD84Eh4LKms2l7nl37vQ082HQ+GeQqYHpanwrsApaM0N9sYDDdzkrrs9J9S9LjOdV0LhMl06799gDLms6nyUxL7T0JvA5sG6U/12kfM3Wdnp0pYxiv8bjftzy79st23G/lGeaIOB4RX6b134CDwDzgXmBL2m0LsCbtMxwRu4E/x9hOt8XA4YgYjIg/gDdTX0TE3ogYqvcI+yunPDskzQSWA3m+0xyDGnONiOiccZualpEuPrgT2B4Rv0TEr8B2YGVq47OIOF7n8TUhp0w7JF1LMZh8Ov4j7L+6MgWQNADcDWyu6NJ1WuhLpqV2XKfnxuN+n/LsyH3cb+WEuUzSfIp3e7uAOaUX2x+BOefZTrd5wNHSz98z8kSw9TLKcw2wIyJOjrXPnI031/SR7D5gmGJgnNR1CllluhZ4K9LpkTar4fm/CXgaOF2xj+u00O9MXadnBPChpD2SHhlln0lTpxnlmfW43+oJs6TpFKfvn+gOOL0ojOmFoaqdySSzPO8H3jjP381KHblGxN8RsRAYABZLuqEnD7YlMst0LROgVsebqaR7gOGI2NO7R9kumWXqOj1jaUQsAu4CHpO0rP5H2g6Z5Zn1uN/aCbOkqRT/yK9FxDtp80+S5qb751KcOTrndtKX4TtfQH8UOAZcVfq1gbRtwsgpz3QRxWLg/fEfWbPqyrUjIk4AnwArJd1aynU1k6BOIa9MJd0EXND2SWJNmd4GrJY0RPFx63JJr7pOm8/UdfpfEXEs3Q4D71K8Yfa4X2gkzzaM+62cMEsS8ApwMCJeKt21Fehccb0OeO982omIoxGxMC0vA7uBayRdLWkaxTv1rfUdUbMyzPM+iotbfh/vsTWpxlwvl3RJWr8IuAP4NiJ2lXLdCnwArJA0S8WVzSvStgkjw0yzPiMyFnVlGhHPRMRARMyneE5/HBEPuE6zyNR1eqadiyXN6KxTZHXA4/6/msoz/3E/Mrjy8FwXYCnFxwRfAfvSsgq4FNgBHAI+Aman/a+k+L7MSeBEWp85Wjuj9LmK4irSI8Czpe2Pp/b+An4ANjedT5vzTPftBFY2nUtGud4I7E3tHACeq+jzYeBwWh4qbd+Y2judbp9vOp+2Z5ruGwSuazqXHDLtavN2RvmLDq7T/mbqOj3rub+A4i807Ae+oWv86erT434f8kz37STzcd//05+ZmZmZWYVWfiXDzMzMzKxfPGE2MzMzM6vgCbOZmZmZWQVPmM3MzMzMKnjCbGZmZmZWwRNmMzMzM7MKnjCbmZmZmVXwhNnMzMzMrMI/4nSj+pGxLksAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRcdyWWSvA36"
      },
      "source": [
        "new_testset.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLI_Z_MvvMnz"
      },
      "source": [
        "X_test=[]\n",
        "Y_test=[]\n",
        "timesteps = 7\n",
        "for i in range(timesteps, 20):\n",
        "    X_test.append(train_data_numpy[i-timesteps:i])\n",
        "    Y_test.append(train_data_numpy[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZchWsMqjvaLf"
      },
      "source": [
        "X_test=np.array(X_test)\n",
        "Y_test=np.array(Y_test)\n",
        "Y_test=Y_test.reshape(Y_test.shape[0],-1,Y_test.shape[1])\n",
        "X_test=torch.Tensor(X_test)\n",
        "Y_test=torch.Tensor(Y_test)\n",
        "X_test=X_test.cuda()\n",
        "Y_test=Y_test.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeCkKgz_wdRz",
        "outputId": "d220b1d1-acc2-4b63-833c-250789a240fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        }
      },
      "source": [
        "generate_data_test=[]\n",
        "for i in range(len(X_test)):\n",
        "    x = X_test[i]\n",
        "    x = torch.unsqueeze(x, dim=0)\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cuda()\n",
        "\n",
        "    y = torch.squeeze(rnn(x))\n",
        "    generate_data_test.append(torch.squeeze(y.cpu()).detach().numpy()[6][0] )\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(testset.index[7:], generate_data_test, 'b', label='generate_test')\n",
        "plt.plot(testset.index[7:], np.array(new_testset['Open'][7:].copy().tolist()), 'r', label='real_data')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOHC36Oafqo2",
        "outputId": "5261f91c-236f-4ab6-cc92-4afb832b08aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "testset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-01-03</th>\n",
              "      <td>778.81</td>\n",
              "      <td>789.63</td>\n",
              "      <td>775.80</td>\n",
              "      <td>786.14</td>\n",
              "      <td>1657300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-04</th>\n",
              "      <td>788.36</td>\n",
              "      <td>791.34</td>\n",
              "      <td>783.16</td>\n",
              "      <td>786.90</td>\n",
              "      <td>1073000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-05</th>\n",
              "      <td>786.08</td>\n",
              "      <td>794.48</td>\n",
              "      <td>785.02</td>\n",
              "      <td>794.02</td>\n",
              "      <td>1335200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-06</th>\n",
              "      <td>795.26</td>\n",
              "      <td>807.90</td>\n",
              "      <td>792.20</td>\n",
              "      <td>806.15</td>\n",
              "      <td>1640200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-09</th>\n",
              "      <td>806.40</td>\n",
              "      <td>809.97</td>\n",
              "      <td>802.83</td>\n",
              "      <td>806.65</td>\n",
              "      <td>1272400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-10</th>\n",
              "      <td>807.86</td>\n",
              "      <td>809.13</td>\n",
              "      <td>803.51</td>\n",
              "      <td>804.79</td>\n",
              "      <td>1176800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-11</th>\n",
              "      <td>805.00</td>\n",
              "      <td>808.15</td>\n",
              "      <td>801.37</td>\n",
              "      <td>807.91</td>\n",
              "      <td>1065900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-12</th>\n",
              "      <td>807.14</td>\n",
              "      <td>807.39</td>\n",
              "      <td>799.17</td>\n",
              "      <td>806.36</td>\n",
              "      <td>1353100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-13</th>\n",
              "      <td>807.48</td>\n",
              "      <td>811.22</td>\n",
              "      <td>806.69</td>\n",
              "      <td>807.88</td>\n",
              "      <td>1099200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-17</th>\n",
              "      <td>807.08</td>\n",
              "      <td>807.14</td>\n",
              "      <td>800.37</td>\n",
              "      <td>804.61</td>\n",
              "      <td>1362100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-18</th>\n",
              "      <td>805.81</td>\n",
              "      <td>806.21</td>\n",
              "      <td>800.99</td>\n",
              "      <td>806.07</td>\n",
              "      <td>1294400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-19</th>\n",
              "      <td>805.12</td>\n",
              "      <td>809.48</td>\n",
              "      <td>801.80</td>\n",
              "      <td>802.17</td>\n",
              "      <td>919300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-20</th>\n",
              "      <td>806.91</td>\n",
              "      <td>806.91</td>\n",
              "      <td>801.69</td>\n",
              "      <td>805.02</td>\n",
              "      <td>1670000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-23</th>\n",
              "      <td>807.25</td>\n",
              "      <td>820.87</td>\n",
              "      <td>803.74</td>\n",
              "      <td>819.31</td>\n",
              "      <td>1963600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-24</th>\n",
              "      <td>822.30</td>\n",
              "      <td>825.90</td>\n",
              "      <td>817.82</td>\n",
              "      <td>823.87</td>\n",
              "      <td>1474000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-25</th>\n",
              "      <td>829.62</td>\n",
              "      <td>835.77</td>\n",
              "      <td>825.06</td>\n",
              "      <td>835.67</td>\n",
              "      <td>1494500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-26</th>\n",
              "      <td>837.81</td>\n",
              "      <td>838.00</td>\n",
              "      <td>827.01</td>\n",
              "      <td>832.15</td>\n",
              "      <td>2973900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-27</th>\n",
              "      <td>834.71</td>\n",
              "      <td>841.95</td>\n",
              "      <td>820.44</td>\n",
              "      <td>823.31</td>\n",
              "      <td>2965800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-30</th>\n",
              "      <td>814.66</td>\n",
              "      <td>815.84</td>\n",
              "      <td>799.80</td>\n",
              "      <td>802.32</td>\n",
              "      <td>3246600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-31</th>\n",
              "      <td>796.86</td>\n",
              "      <td>801.25</td>\n",
              "      <td>790.52</td>\n",
              "      <td>796.79</td>\n",
              "      <td>2160600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Open    High     Low   Close   Volume\n",
              "2017-01-03  778.81  789.63  775.80  786.14  1657300\n",
              "2017-01-04  788.36  791.34  783.16  786.90  1073000\n",
              "2017-01-05  786.08  794.48  785.02  794.02  1335200\n",
              "2017-01-06  795.26  807.90  792.20  806.15  1640200\n",
              "2017-01-09  806.40  809.97  802.83  806.65  1272400\n",
              "2017-01-10  807.86  809.13  803.51  804.79  1176800\n",
              "2017-01-11  805.00  808.15  801.37  807.91  1065900\n",
              "2017-01-12  807.14  807.39  799.17  806.36  1353100\n",
              "2017-01-13  807.48  811.22  806.69  807.88  1099200\n",
              "2017-01-17  807.08  807.14  800.37  804.61  1362100\n",
              "2017-01-18  805.81  806.21  800.99  806.07  1294400\n",
              "2017-01-19  805.12  809.48  801.80  802.17   919300\n",
              "2017-01-20  806.91  806.91  801.69  805.02  1670000\n",
              "2017-01-23  807.25  820.87  803.74  819.31  1963600\n",
              "2017-01-24  822.30  825.90  817.82  823.87  1474000\n",
              "2017-01-25  829.62  835.77  825.06  835.67  1494500\n",
              "2017-01-26  837.81  838.00  827.01  832.15  2973900\n",
              "2017-01-27  834.71  841.95  820.44  823.31  2965800\n",
              "2017-01-30  814.66  815.84  799.80  802.32  3246600\n",
              "2017-01-31  796.86  801.25  790.52  796.79  2160600"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6lYpYF-yF2u",
        "outputId": "0101dd45-3233-4650-ecf1-91ae4caf9ba6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "generate_data_val=[]\n",
        "for i in range(len(X_val)):\n",
        "    x = X_val[i]\n",
        "    x = torch.unsqueeze(x, dim=0)\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cuda()\n",
        "\n",
        "    y = torch.squeeze(rnn(x))\n",
        "    generate_data_val.append(torch.squeeze(y.cpu()).detach().numpy()[6][0])\n",
        "plt.plot(df.index[907:], generate_data_val, 'b', label='generate_val')\n",
        "plt.plot(df.index[907:], np.array(df1['Open'][907:].copy().tolist()), 'r', label='real_data')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}